[["index.html", "R Essentials: a Practical Step-by-Step Guide to Data Cleaning, Stunning Visuals, and Analytic Insights Delighted to have you!Welcome to: What to expect Packages and R version", " R Essentials: a Practical Step-by-Step Guide to Data Cleaning, Stunning Visuals, and Analytic Insights Gillian Debra 2025-03-16 Delighted to have you!Welcome to: This guide will tackle data preprocessing (the various steps to obtain a “clean and ready” dataset using e.g., dplyr), data visualization (using ggplot2), and data analysis (linear regression, analysis of variance, and structural equation modelling), using R and RStudio. If you already had a go with these activities, you may have noticed that they involve several steps, some of which may have taken you by surprise. You received your data file, you inspect it… only to find that it is far from usable. The data should be reordered, variables transformed, certain entries removed. There’s a lot to look up or to figure out. After some time, the data file is finally ready to go. However, you realize that this is only the beginning, the fun has yet to start. With a functional Wi-Fi connection, most of these steps are documented online. Thing is, this abundance of information is scattered far and wide across the internet. Additionally, you may be unaware of certain possibilities, leaving you without “quality of life improving” tips and tricks. Drawing from my own experience in R, I want to help you by providing explanations, demonstrations, and by sharing some tips and tricks. I also want to raise your awareness of the many possibilities that R offers. If you are aware that something is possible, you can then explore it further on your own. Before we begin, I want to briefly note two points. First, R allows for different personal styles. Different code can lead to the same result, some ways are more intuitive for me while others might be more intuitive for you. Second, while this guide is made with “beginner” R users in mind, it occasionally contains information for more advanced users. Keep in mind that I will explain relatively basic code in the early parts. In later sections, I will progressively reduce explanations of the “basics” to focus on the topic at hand. What to expect This guide will tackle the following aspects. Loading different types of data files. We’ll start with a brief reminder of R packages and how to set a working directory before demonstrating how to open a variety of files such as .csv and .sas. Sharing and communicating your work with others. This section will seem to jump ahead slightly, but the sooner you learn how R projects notably simplify sharing your data files and R scripts, the better. Additionally, I will provide a short introduction to making dynamic (online) reports using R Markdown to enhance transparency and reproducibility. Data preprocessing (dplyr) and descriptive calculations. Learn perform various computations and to shape and clean your data, using traditional methods and the more modern dplyr approach. Data visualization (ggplot2). Learn to plot and show your data - one of the most powerful skills, in my humble opinion. To do so, I will introduce you to a flexible toolbox provided by the ggplot2 package, helping you unleash your inner artist. Reliability, confirmatory and exploratory factor analysis. Learn to have an indication of how “closely” questionnaire items relate related to one another (Cronbach’s alpha and McDonald’s Omega). In addition, I’ll show you techniques that are used to evaluate whether your data “can be explained” (or deemed to be) by underlying constructs. Basic regression analysis (lm, glm, lmer) and model assumptions. Learn to model straight lines, from the basic linear regression to the mixed-effects (multilevel) linear regression. In addition, we will check several assumptions of (linear) regression including the distribution of residuals and homoscedasticity. ANOVA and MANOVA. This section focuses on linear regression with categorical variables and comparisons of group means. Learn to create various contrasts (beyond treatment and sum contrasts) to test your own questions and hypotheses. Mediation analysis. Sometimes we want to know why a relation or effect occurs. In this section you will learn how to fit structural equation models to test mediation and moderated mediation models using lavaan. In addition, I’ll show you how to visualize the results of a mediation analysis by plotting path diagrams. Missing data and multiple imputation. Data can go missing due to a variety of reasons and should not be ignored. In this last part, I provide some approaches on how to deal with missing values and how to inspect patterns of missingness. Lastly, I provide a step-by-step guide on multiple imputation using the mice package. Packages and R version All demonstrations in this guide were done using R version 4.3.2 and RStudio version 2023.12.0+369. The following packages were used throughout this guide: Package Version car 3.1-2 corrtable 0.1.1 dplyr 1.1.2 e1071 1.7-13 effectsize 0.8.6 gganimate 1.0.8 ggcorrplot 0.1.4.1 ggimage 0.3.3 ggplot2 3.4.4 ggpubr 0.6.0 haven 2.5.3 introdataviz 0.0.0.9003 knitr 1.43 lavaan 0.6-16 lubridate 1.9.2 MASS 7.3-60 mice 3.16.0 pacman 0.5.1 psych 2.3.6 readxl 1.4.3 rockchalk 1.8.15.7 semTools 0.5-6 stringr 1.5.0 transformr 0.1.4 "],["loading-datafiles.html", "Chapter 1 Loading datafiles 1.1 Working directory 1.2 Loading different types of data files 1.3 Loading multiple files (same extension) at once", " Chapter 1 Loading datafiles In this short section, we will start with something seemingly small but fundamental If you cannot load data files, you cannot preprocess, visualize, and analyze their contents. Fortunately, R provides several ways to load (and create) data files. For certain types of data files such as those with .sav extension (from SPSS), you will need to load R packages. As a quick reminder: # The classic way to install a package: install.packages(&quot;car&quot;) # or whatever package you want # Alternatively look at your R-studio screen. Look at the left window -&gt; packages -&gt; install # This &quot;package window&quot; can also be used to deactivate/activate/remove/update packages # Let&#39;s load some packages (using code lines). The old way is to do: library(car) library(ggplot2) library(dplyr) #... too exhausting # An alternative is the pacman package library(pacman) p_load(car,ggplot2,dplyr,effectsize) # bonus: it will auto-install any packages if this was not done previously 1.1 Working directory Before we open a data file, we need to tell R where to find it. In RStudio you can import a data set by point and click (top right window under “Environment”). Of course this may become tedious if you want to load multiple data sets. Alternatively, by R code, we could use the working directory. Personally I would highly recommend to use R projects (see chapter 2) but for the sake of illustration I will provide a quick recap of the working directory. # Getting/Setting the working directory (henceforth abbreviated as &quot;WD&quot;) # Without R projects and manually: getwd() # Which will output the WD of the location where R the script was activated (i.e., where you opened it) # The WD can also be set. For example to a variable I arbitrarily name &quot;myWD&quot; myWD = &quot;C:/Users/Gillian&quot; # Now suppose I want to load a data file (a .csv extended file) located on my desktop put in a folder called &quot;folder&quot;. # I could do something like this: myWD = &quot;C:/Users/Gillian/desktop/folder&quot; library(stringr) # Will use this package to glue strings together, and to replace them (just to demonstrate that this is possible) mydatafile = read.csv(str_glue(mycurrentWD,&quot;/thedatafile.csv&quot;)) # Suppose there is a second dataset in a subfolder mydatafile = read.csv(str_glue(mycurrentWD,&quot;/subfolder/thedatafile.csv&quot;)) # Suppose there is a dataset in another folder on the desktop and I refuse to set another WD mydatafile = read.csv(str_replace(mycurrentWD,&quot;folder&quot;,&quot;anotherfolder/thedatafile.csv&quot;)) 1.2 Loading different types of data files # Example 1: open a file without telling R the file extension Mydata = read.table(file.choose(), header = TRUE, sep=&quot;,&quot;, dec=&quot;.&quot;) # -&gt; With this code I say: let me manually find a folder on my computer, the first row in that file contains the columnnames, data are separated by a &quot;,&quot; and a decimal is noted as a &quot;.&quot; # -&gt; the above could work to open a csv file. # Example 2 csv file: Mydata = read.csv(&quot;datafile.csv&quot;, na.string=c(&quot;&quot;,&quot;NA&quot;,&quot;NaN&quot;)) # -&gt; Optionally I told R that empty cells, cells with &quot;NA&quot;, and cells with &quot;NaN&quot; should be read as NA in your datafile (not administered, empty) # Example 3 spss file (.sav) mydata = haven::read_sav(&quot;datafile.sav&quot;) # -&gt; You will need the package &quot;haven&quot; # -&gt; Here I used a function of the above package without loading it (only works if you have installed the package) # -&gt; Of course, feel free to load packages (library(haven)) # Example 4: xlsx file (i.e., your typical excel file) Library(readxl) Mydata=read_excel(&quot;datafile.xlsx&quot;) 1.3 Loading multiple files (same extension) at once Suppose we have multiple data files such as after conducting a study. We have one data file per participant or test subject. You want to merge all these individual data files into one big data set. In this case, you could make a folder and put all your data files in there. Then you could open R and tell it to list all files (i.e., put every file below one another) that have the same file extension. The example below was specifically made for csv-files. temp_data=list.files(&quot;folder_with_files/&quot;, pattern=&quot;*.csv&quot;, full.names=TRUE) # Now we can &quot;glue&quot; everything together: Mydata = do.call(rbind, lapply(temp_data, function(x) read.csv(x, stringsAsFactors = FALSE))) "],["communicate-with-others-a-quick-glance-at-r-projects-and-r-markdown.html", "Chapter 2 Communicate with others: a quick glance at R projects and R markdown 2.1 R projects 2.2 R Markdown (brief) A heading (level two)", " Chapter 2 Communicate with others: a quick glance at R projects and R markdown 2.1 R projects We finally loaded a data file and, after we got through the next parts, processed and analyzed it. One day someone (likely your colleague) who works on the same or similar project wants to make some corrections to your code. They ask you to share the data file and R code. Inspect the following R code line. What would be inconvenient for your colleague? myfile = read.csv(&quot;Users/Gillian/folder/datafile.csv&quot;, na.string=c(&quot;&quot;,&quot;NA&quot;,&quot;NaN&quot;)) Unless you’re close with that person and they can always use your computer, they likely have a different file location and working directory on their computer. Thus, they will need to adjust the R code. Sometime later, they send their version of the R script. On your computer, you will have to adjust the R code. Your computer crashes, you get a new one, you will need to adjust the R code. Not efficient. Fortunately, we can create a folder on our computer, let’s call it “My_Project”, and put a R project in it. Whenever you click on the R project, the working directory will automatically update to the location of this project. You can now move all your data files and scripts to this folder (or subfolders) and send it to your colleague. In turn, they can put this folder wherever they want without having to adjust the R code. Open R Studio and click on the blue cube (top right corner). This will prompt a window to a New or Existing directory. With a “New Directory”, you create a new folder with an R project inside. So if I want to have a project in a folder “My_Project” on my desktop (with “etc.” = the location of the desktop): With an “Existing Directory”, the R project will be placed in the desired location without creating extra (sub)folders. So after I create the folder “My_Project” on my desktop: Navigate to the R project folder. I also made an new R script and put it in the folder Click on the R project and you see the following window. Note the “Files” window that now contains the project and R script. This window will automatically update if you put any new files or folders, and the working directory will always equal the location of the project. In the Files window (bottom left corner) click on your R script and add your data files. Instead of calling the data file using “C:/…”, you can now simply call its name, provided the file is in the same location as the project. If you put all your data files in separate folders, you simply need to include them: 2.2 R Markdown (brief) You know how to exchange your data files and other materials using R projects. However, often you want share your work without requiring your audience to run your code from scratch. To address this need, R Markdown makes it easy to create and share reports (pdf, Word, HTML, or Powerpoint format) that can contain your steps in the analysis, your plots, and so on. Thus, it is a handy tool that promotes transparency and reproducibility. Beyond reports, R Markdown has extra capabilities. For example, this very own guide was created by combining multiple R Markdown documents (using R bookdown). Now, explaining this file format for making dynamic documents in full detail would be too much. Instead, I will very briefly go over some basics and give you “a behind the scenes” look. In R Studio go the empty paper symbol (New File) (top right corner) and click on R Markdown. The following window pops up. From here you can define a document title, author, and date. This info can be changed later or it can be left empty. Concerning the Output format, you can choose PDF or Word documents but I personally prefer HTML as it supports plots (see the part about data visualization). The following default screen appears. What is all this? Let’s start from the top By default you see something like this: I believe this is more or less self-explanatory. My advice would be to change the date to: “2025-03-16” so that the date automatically updates to the time you knit the document. Now with HTML documents we can add some extra customization options including a table of content, and more. Behold the following example. We introduce a table of contents, we make it “float” so that it stays present when scrolling down, and we include up to 4-level headings. Next, you may be wondering why we included something about a yeti and haddock highlights. The “yeti”, refers to one of many themes (others include: “darkly”, “cerulean”, “journal”, “spacelab”,and more) that determine the appearance of our HTML document. Haddock refers to the highlight style that sets the appearance of the R syntax (others include:“tango”,“espresso”,“zenburn”,“kate”, and more). Next, we have the figure settings. Here we say that we want all figures (plots) generated inside the Markdown document to have a height and width of 3 inches. Finally, with number_section: TRUE, we want to automatically add numbered sections. Now, what would any type of R document be without R code. To include R code (or code chunks as they are often referred to), you can use the key combination Alt + Ctrl + I. On to the text, outside of R code chunks, you have already noticed that I can put text in bold and in italics? I can also include hyper links where you just have to click on a word or a sentence (click here) to be brought on the web page. I can also add bullet points bullet point one first sub point second sub point first sub sub point second sub sub point third sub sub point Or I can do something like this first second (and so on) I can add multiple headings to my document A heading (level two) Another heading (level three) I add images like this (see below). This is what I see. And there you have it, the above provides a good basis to start creating your R Markdown documents but do realize that I only scratched the surface. If you finish your R Markdown document, the next and potentially final step is to Knit the document by pressing the combination: Ctrl + Shift + K. "],["data-cleaning-and-descriptive-statistics.html", "Chapter 3 Data cleaning and descriptive statistics 3.1 Access different parts of a dataset 3.2 Mathematical calculations in R 3.3 Correlation matrices and visualisation 3.4 Dplyr, a more modern approach to shape data (!).", " Chapter 3 Data cleaning and descriptive statistics Opening data files is one thing, preprocessing (“cleaning”) them is something else entirely. You may need to restructure your data, change variable values, compute new ones, merge datasets, and more. Fortunately, the data is fluid. It can be changed. reshaped. remade. This all requires some understanding of how to access different parts of your dataset, how to perform mathematical calculations in R, how to recode variables, and so on. After a brief overview, I will introduce you to dplyr package - a personal favorite of mine - which simplifies preprocessing/cleaning. In this part I will be covering: How to access the different parts of dataset in R. This includes how to navigate and extract certain rows or columns from a dataset. How to do mathematical computations in R including the standard deviation and the kurtosis. In addition, I will show how to get multiple descriptive statistics of multiple variables in one code line. I will also show how to easily make a correlation matrix. Important functions of the dplyr package. I will explain and demonstrate a variety of functions that I commonly use in my data processing toolbox such as mutate and left_join. Ending with a demonstration of data preprocessing/cleaning a raw dataset from scratch. 3.1 Access different parts of a dataset Let’s start with the demonstration or a refresher of the basics. For the examples below, I will generate my own dataset using the data.frame() function. As the values of this dataset will be generated at random, I will set a seed so that in each run, the generated data will be identical. set.seed(123) # Note, the number can be anything (but the results will be different ) mydata = data.frame( participant = c(1:5), # generate the values 1 to 5 (1,2,3,4,5) scores = runif(5, 0,100), # generate 5 values (from a uniform distribution) between 0 and 100 gender = c(&quot;male&quot;,&quot;female&quot;,&quot;male&quot;,&quot;female&quot;,&quot;female&quot;) ) 3.1.1 Viewing the data Now that I have made a dataset, I’ll briefly show you how to view your data as RStudio allows different ways to do so. With code: mydata head(mydata) # shows the first 10 entries of a dataset View(mydata) # Prompts the dataset in a separate window # TEMPORARILY filter values, TEMPORARILY rearrange values, see changes in real-time, ... Next to writing code, you can also click on your datasets (under Environment, top right corner). The dataset will be shown in a separate window (similar to the View() code). As shown in the image above, the “view window” allows to quickly track any changes. If you want you can use the view window outside of R**. Also practical, you can check specific entries in your dataset using the filter option and (temporarily) rearrange the data in an ascending or descending order to quickly check who or what has the highest or lowest score. 3.1.2 Several examples A dataset consists of rows and columns. In referring to these, R will first look at the rows and then at the columns. For example, dataset[1,2] will output the value at the cross point of the first row and second column (28.75775). Let’s learn through example. # If I want to see the first row of all columns: mydata[1,] # Again rows are on the left of the comma (columns on the right) # If I want to see rows 1 to 3 of all columns: mydata[1:3,] # If I want to see rows 1,2, and 5 of all columns: mydata[c(1:2, 5),] # Here I have to use a &quot;c&quot; vector # Same goes for the columns. I can mention the column number (in my example 1= participant, 2= scores, 3=gender) # I want to see column 1 and 3 but by calling them by name instead their column number mydata[,c(&quot;participant&quot;, &quot;gender&quot;)] # If you want to see one column. In R you can also ask it like this: mydata$gender # If I want to see the value at column 2, row 3: mydata[3,2] # If I want to change the above value and add the value of row 5 and column 2 mydata[3,2] = mydata[3,2] + mydata[5,2] # Suppose I want to look at certain values. For example, I want to look at scores but only from females: mydata$scores[mydata$gender==&quot;female&quot;] # Within the column scores, search for scores that &quot;row-wise&quot; corresponds with &quot;female&quot; # Suppose I want to look at scores from females who have a score of at least 85 # In other words: look to scores with condition &quot;female&quot; who have a &quot;score&gt;=85&quot; mydata$scores[mydata$gender==&quot;female&quot; &amp; mydata$scores&gt;=85] # I want to look at scores from females and from participant 1 (a male) # &quot;female&quot; OR &quot;participant == 1&quot; mydata$scores[mydata$gender==&quot;female&quot; | mydata$participant==1] # One final example, I want to look at scores from females with a score of above 90, males with a score below 30, and participant 3 # &quot;female AND score&gt;90&quot;, &quot;male AND score&lt;30&quot; mydata$scores[c(mydata$gender==&quot;female&quot; &amp; mydata$scores&gt;90) | c(mydata$gender==&quot;male&quot; &amp; mydata$scores&lt;30) | mydata$participant ==3 ] Insight in the above was required to compute novel variables and filter data. However, as I mentioned earlier, packages such as dplyr ease the majority of this work. There remains one obstacle before I move on to dplyr. While preprocessing/cleaning your data, you might have to transform your variables or compute new ones. Thus, knowledge concerning math calculations is essential. I will briefly show how to do different (basic) math calculations and how to make a correlation matrix in R. Although I have to note that there are different ways to do so. 3.2 Mathematical calculations in R # Defining a new dataset (with the same name) set.seed(0209) mydata = data.frame(v1 = runif(10,1,10), v2 = runif(10,1,10), v3 = runif(10,1,10)) 3.2.1 Sum # Take the sum across all values of column v1 sum(mydata$v1) # Take the sum row-wise (from v1 to v2) of each row rowSums(mydata) # Note, as expected, you get 10 outcomes. # Do the same but OUTPUT only the results of rows 1 to 3, 5, and 9 to 10 rowSums(mydata)[c(1:3,5,9:10)] # Do the same but now CALCULATE and OUTPUT the &quot;row wise sum&quot; of v1 and 3 rowSums(mydata[,c(&quot;v1&quot;,&quot;v3&quot;)])[c(1:3,5,9:10)] # Compute the sum of values in column v2 and then do the same in column v3 colSums(mydata[,c(&quot;v2&quot;,&quot;v3&quot;)]) # Do the same but use only rows 5 and 7 colSums(mydata[c(5,7), c(&quot;v2&quot;,&quot;v3&quot;)] ) # Quick note, I don&#39;t need to stay on the same line. To my knowledge, you can start a &quot;new line&quot; with &quot;,&quot; &quot;+/- etc.&quot; and brackets so that you provide R some expected &quot;continuation&quot; of the code. 3.2.2 Average # Take the mean of column v1 mean(mydata$v1) # take the mean rowwise rowMeans(mydata) # Take the mean column wise but only using odd rows from v1 and v2 colMeans(mydata) 3.2.3 Variance, standard deviation, and correlations # variance var(mydata$v1) # standard deviation sd(mydata$v1) # (bivariate) correlations cor(mydata$v1, mydata$v3) cor(mydata$v1, mydata$v3, method=&quot;spearman&quot;) cor(mydata$v1, mydata$v3, method=&quot;kendall&quot;) 3.2.4 Other calculations and the describe function(!) # range range(mydata$v1) # min/max (value) min(mydata$v1) max(mydata$v1) # Centering and standardizing scale(mydata$v1, center=TRUE, scale=FALSE) # will mean center variables scale(mydata$v1, center=TRUE, scale=TRUE) # will standardize variables # skewness and kurtosis (using the e1071 package) e1071::skewness(mydata$v1) e1071::kurtosis(mydata$v1) Or use the describe function from the psych package to have a quick overview library(psych) describe(mydata) describe(mydata[,c(&quot;v1&quot;,&quot;v3&quot;)]) # If you want to use it for specific variables 3.2.5 What if there are missing values? As it turns out, taking the sum, mean, correlation, among other things, will output NA if I would apply the above methods. Therefore, I will both illustrate and correct this issue: mini_data = data.frame(a = round(runif(30,1,10)), b = round(runif(30,1,10)) ) mini_data[1:2,1]=NA mini_data[2:3,2]=NA sum(mini_data$a) # NA rowSums(mini_data) # Only the last value is not NA cor(mini_data$a, mini_data$b) # NA # To let the above functions ignore NA&#39;s (but proceed with caution, do the outcomes make sense in your individual specific case?) sum(mini_data$a, na.rm=TRUE) # remove the NA (&quot;ignore it&quot;) rowSums(mini_data, na.rm=TRUE) # With correlations we have to use different code: cor(mini_data$a, mini_data$b, use = &quot;complete.obs&quot;) # Will apply listwise deletion (delete rows containing NA&#39;s) cor(mini_data$a, mini_data$b, use = &quot;pairwise.complete.obs&quot;) # Will apply pairwise deletion (delete cases/cells containing NA&#39;s) # In this example the result will remain the same irrespective from listwise or pairwise deletion 3.3 Correlation matrices and visualisation It is often appreciated to show the correlations between all relevant variables in your dataset (personalliy, I frequently spot them in papers from social and behavioral disciplines). For this purpose I will create a new dataset in which variables are “taken from” a multivariate normal distribution and in which the correlations between variables are prespecified. For transparency I show how I made this dataset (in case you’re interested). # OPTIONAL, just if your interested in how to create a correlated dataset library(MASS) # Using the MASS package I will create a dataset based on a multivariate normal distribution. To create one I need the following ingredients: the mean of the variables I want to generate and their variance covariance matrix # the mean (determined by me) set.seed(123) a_mean = 6 b_mean = 8 c_mean = 3 d_mean = 5 # the standard deviation for the variance covariance matrix (determined by me) a_sd = 0.8 b_sd = 3 c_sd = 0.4 d_sd = 1.5 # Correlations between variables (determined by me) cor_ab = -0.2 cor_ac = 0.3 cor_ad = 0.5 cor_bc = 0.7 cor_bd = -0.25 cor_cd = 0.15 # Variance-covariance matrix sigma=matrix( c(a_sd^2, a_sd*b_sd*cor_ab, a_sd*c_sd*cor_ac, a_sd*d_sd*cor_ad, b_sd*a_sd*cor_ab, b_sd^2, b_sd*c_sd*cor_bc, b_sd*d_sd*cor_bd, c_sd*a_sd*cor_ac, c_sd*b_sd*cor_bc, c_sd^2, c_sd*d_sd*cor_cd, d_sd*a_sd*cor_ad, d_sd*b_sd*cor_bd, d_sd*c_sd*cor_cd, d_sd^2), 4,4 ) # The above variance-covariance matrix can also be made using the lazyCov() function from the rockchalk package library(rockchalk) sigma = lazyCov(Rho = c(cor_ab, cor_ac, cor_ad, cor_bc, cor_bd, cor_cd), Sd = c(a_sd, b_sd, c_sd, d_sd) ) # Our correlations can be retrieved from our &quot;sigma&quot; object: round(cov2cor(sigma),2) #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1.0 -0.20 0.30 0.50 #&gt; [2,] -0.2 1.00 0.70 -0.25 #&gt; [3,] 0.3 0.70 1.00 0.15 #&gt; [4,] 0.5 -0.25 0.15 1.00 # Now we can create the dataset, I want 100 values per variable n = 100 corr_data = as.data.frame( mvrnorm(n=n, mu=c(a_mean,b_mean,c_mean,d_mean), Sigma = sigma, empirical = TRUE) ) names(corr_data) = c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;) Now, there are plethora of packages available to visualize correlations. I’ll point you to a relatively recent one corrtable. This package makes it easy to create and save a correlation matrix provided with frequentist p-value significance indications (“the significance stars”). In this dataset, I want to have the correlations between all variables. However, if you want to do this with a smaller subset of variables, then extract those variables first in a separate dataset and use this smaller dataset to create your correlation matrix. new_correlation_matrix = corrtable::correlation_matrix(corr_data) # your correlations accompanied with the significance stars new_correlation_matrix[upper.tri(new_correlation_matrix)]=&quot;&quot; # Remove the upper part, so we get the typical triangle shape new_correlation_matrix = as.data.frame(new_correlation_matrix) # Or we can do the above in one step and save it to a csv file (to copy-paste it in Words) corrtable::save_correlation_matrix(df=corr_data,file=&quot;data_files/corr_matrix.csv&quot;,digits=3,use=&quot;lower&quot;) Note that you only get the significance stars. If you want the specific p-values from the above matrix you can use e.g., the cor_pmat() function from the ggcorrplot package. library(ggcorrplot) mycorr_pvalues = round(cor_pmat(corr_data),3) 3.4 Dplyr, a more modern approach to shape data (!). We finally arrive at the dplyr package. This package contains a variety of functions to ease data preprocessing/cleaning. For the purpose of this guide, I will go through some functions that I frequently use on my adventures in data analysis. If you are hungry for more, I highly recommend to have a look at the following cheat sheet (click here). In the various examples below, I will use a fabricated dataset. Next to the dplyr package, I will use the stringr package (simplifying operations with strings/“text” variables) and the readxl package (to load this fabricated dataset) pacman::p_load(dplyr,stringr,readxl) mydata = read_xlsx(&quot;data_files/mydata.xlsx&quot;) 3.4.1 Dplyr: Filter Let’s start with filter which “filters” entries (row-wise). The question here is what data (row-wise) you want to keep or what data you want to exclude. Off-note, in dplyr, we can do something called “pipelining” noted by adding %&gt;% in the code. Pipelining allows to do various calculations and functions on the same code line. So instead of do this do that do this… You can keep all of your code on the same line, chaining your functions/calculations together (as shown in later examples and my demonstration at the end). # Suppose you want to only SHOW/KEEP rows that have a &quot;1&quot; in the variable &quot;cond&quot; (filtering out the rest) mydata %&gt;% filter(cond==1) # Notice the &quot;%&gt;%&quot; to start pipelining. Here I tell R in the dataset &quot;mydata&quot; to filter... # Show rows containing condition 1 and from participant 5 mydata %&gt;% filter(cond==1, participant==5) # Keep rows from participants 1 to 5 mydata %&gt;% filter(participant %in% c(1:5)) # I recommend using %in% instead of only c(1:5) # As the name suggests %in% implies &quot;in range of ...&quot;. Here, in (range of) values of 1 to 5. # Keep all rows EXCEPT THOSE CONTAINING participant 4 mydata %&gt;% filter(!participant ==4) # Notice the &quot;!&quot; this mean NOT or in this context DO NOT KEEP/KICK OUT ... # Keep rows from participants 1, 3, and 5 EXCEPT THOSE containing a rt of 0.245 mydata %&gt;% filter(participant %in% c(1,3,5), !rt==&quot;0.245&quot; ) # Note that rt is written as a character (hence &quot;&quot;) # Exclude rows containing NA&#39;s (Not Administered, empty cells) in q_2 mydata %&gt;% filter(!is.na(q_2)) # Keep rows that contain the characters: &quot;sagree&quot; (to get &quot;disagree&quot; &amp; &quot;heavily disagree&quot;) in q_1. mydata %&gt;% filter(str_detect(q_1,&quot;sagree&quot;)) # For this you can use the stringr package 3.4.2 Dplyr: Select Similar to filter, but here we will select or exclude columns/variables so we go “column-wise” instead of row-wise. # Select (KEEP) columns q_1 and r_2 mydata %&gt;% select(q_1, r_2 ) # Select the columns whose name has/contains a &quot;q&quot; character (we can&#39;t use stringr here) mydata %&gt;% select(contains(&quot;q&quot;)) # DO NOT select (EXCLUDE) column names ending with &quot;ge&quot;, beginning with an &quot;r&quot;, and containing the characters &quot;ond&quot;. # Exlude rows containing participant 3 mydata %&gt;% select(-ends_with(&quot;ge&quot;), -starts_with(&quot;r&quot;), -contains(&quot;ond&quot;)) %&gt;% filter(!participant ==3) # Note the beauty of PIPELINING &#39;%&gt;%&#39;, I chain SELECT and FILTER on the same code line # Note that I use &quot;-&quot; in SELECT instead of the &quot;!&quot; that I use in FILTER # Why? From personal experience, using &quot;!&quot; in SELECT does not always yield the intended results # Alternatively, we could also use only one &quot;-&quot; in the above code: mydata %&gt;% select(-c( ends_with(&quot;ge&quot;), starts_with(&quot;r&quot;), contains(&quot;ond&quot;) )) %&gt;% filter(!participant ==3) # Note that I add some unnecessary &quot;spaces&quot; in the c() part. This is just for clarity, you can add as much of these spaces as you want without repercussion 3.4.3 Dplyr: Mutate Now that we can include or exclude certain columns and rows, we want to start transforming our variables. in dplyr language we will mutate them. The beauty with mutate, is that we can do multiple mutations in one go. # Suppose I want to create a novel variable containing the natural logarithm of the variable &quot;rt&quot;. I do NOT want to overwrite the original &quot;rt&quot; variable # Since rt is a character (and not a number) I will FIRST transform/mutate it from character to numeric and THEN compute the natural logarithm mydata %&gt;% mutate(rt_log = as.numeric(rt), rt_log = log(rt) ) # I will now permanently (because mydata = mydata %&gt;% ...) set rt to be a number (all code up til now were temporarily) mydata = mydata %&gt;% mutate(rt = as.numeric(rt)) # You can also transform all variables into a specific type, here transform all numeric variables to &quot;factor&quot; mydata %&gt;% mutate_if(is.numeric,as.factor) # For the next example, suppose you questionnaire items (noted by a &quot;q&quot; in the variable name). Suppose you want to add 2 to each of these items (but temporarily). Here we can use &quot;across&quot; as will be explained below mydata %&gt;% mutate(across(starts_with(&quot;q&quot;), ~. + 2 ) ) # In this context, &quot;across&quot; can be translated as &quot;in each&quot;. For each variable starting with &quot;q&quot;, we add &quot;~&quot; to indicate that the following function should be applied. Next, The dot represents the variable itself (which starts with &quot;q&quot;). In summary. Across (in each) variable starting with &quot;q&quot;, do the following: add 2 to each variable (starting with &quot;q&quot;). # Important detail, across tends to &quot;apply the following&quot;, one variable at a time, and not simultaneously. So pay attention with certain function (such as rowMeans), as explained below. Watch out if you use across in combination with another function that uses variables simultaneously (such as rowMeans or rowSums)! Again, across works with variables “one at a time”. Luckily, this can be easily fixed by knowing where to place across. # Lets take the row-wise average score of all variables starting with &quot;q&quot; (ignoring empty variables) # HOWEVER, doing it wrong first (so expect an error) mydata %&gt;% mutate(average_score=across(starts_with(&quot;q&quot;)), ~ rowMeans(., na.rm=TRUE)) # Why this error in the case of rowMeans? Well, rowMeans is function that works with all variables SIMULTANEOUSLY (since it is an average). This can be fixed by putting &quot;across&quot; within rowMeans. So pay attention were you put across and all will be fine. mydata = mydata %&gt;% mutate(average_score=rowMeans(across(starts_with(&quot;q&quot;)),na.rm=TRUE)) A final example # Similar example but a &quot;bit more advanced&quot;. Say we want to have a sum score across variables starting with &quot;q&quot;. However, now we want to set this sum score to NA (empty/not administered) if at least ONE value is NA/missing (row-wise). mydata %&gt;% mutate(na_count=rowSums(is.na(across(starts_with(&quot;q&quot;)))), average_score=ifelse(na_count&gt;=1, NA, average_score) ) 3.4.3.1 (Re)coding (questionnaire) items In surveys there is often a need to (re)code responses (like agree and totally disagree), giving these a numerical value. Fortunately, the mutate function is a versatile in this regard as well. One way is to use the versatile ifelse function within mutate (but see a more straightforward approach afterwards). In short, you first need to specify a condition (e.g., if the item is scored as “agree”), then you specify what should be done if so (the item should be given a value of…), and finally you indicate what value should be given if not. Lets give an “easy” example # If the variable q_1 is scored as &quot;heavily disagree&quot;, give it value a 1, else give it a value of 0 mydata %&gt;% mutate(q_1 = ifelse(q_1==&quot;heavily disagree&quot;, 1, 0) ) Looks straightforward right? However, suppose you have five response categories: mydata %&gt;% mutate(q_1 = ifelse( q_1==&quot;heavily disagree&quot;, 1, ifelse( q_1== &quot;disagree&quot;, 2, ifelse( q_1==&quot;neutral&quot;, 3, ifelse( q_1==&quot;agree&quot;, 4, 5 ) ) ) ) ) The above follows the same logic (if…, do …, else do…) but a lot of brackets… and unnecessary in this case. Not to speak ill about about the ifelse() function, it is very flexible in numerous situations. Fortunately in this context, there exist a more straightforward alternative: recode. With recode I can give text (“agree”) a numerical value, and a value (2) a textual value. Let us redo the above example and change text to numbers, and back. # Take again the example with five response categories. # Lets recode text to numbers: mydata = mydata %&gt;% mutate( q_1 = recode(q_1, &#39;heavily disagree&#39; = 1, &#39;disagree&#39; = 2, &#39;neutral&#39; = 3, &#39;agree&#39; = 4, &#39;heavily agree&#39; = 5 )) # Lets change it back from numbers to text mydata %&gt;% mutate( q_1 = recode(q_1, &#39;1&#39; = &quot;heavily disagree&quot;, &#39;2&#39; = &quot;disagree&quot;, &#39;3&#39; = &quot;neutral&quot;, &#39;4&#39; = &quot;agree&quot;, &#39;5&#39; = &quot;heavily agree&quot; )) # Lets recode from text to numbers in all variables starting with &quot;q&quot;. # I will apply these changes permanently to my dataset (mydata = mydata %&gt;% ...) mydata = mydata %&gt;% mutate(across(starts_with(&quot;q&quot;), ~ recode(., &#39;heavily disagree&#39; = 1, &#39;disagree&#39; = 2, &#39;neutral&#39; = 3, &#39;agree&#39; = 4, &#39;heavily agree&#39; = 5) )) 3.4.4 Dplyr: Arrange Arrange data in ascending or descending order based on value or alphabetical order. # Arrange reaction time (rt) from lowest value ascending to highest mydata %&gt;% arrange(rt) # Arrange rt from highest value descending to lowest mydata %&gt;% arrange(desc(rt)) # Arrange on both age and rt mydata %&gt;% arrange(age,rt) 3.4.5 Dplyr: Group_by and Summarize Suppose we want to compute something like the sum or mean, per “cluster variable” such as per each participant, per each condition, per each plant or animal species… We can do so by grouping and then summarizing it, which will create a tinier dataset including the desired computation(s). First we will need to define our “grouping/cluster” variables. Then we need to indicate what we want to do per this cluster variable. As an illustration, suppose you want to have an average per participant. tempdata = data.frame( participant = rep(c(1:3), each = 3), score = c(1,2,3, 4,5,6, 7,8,9) ) group_by(participant) %&gt;% summarize(average_per_participant = mean(score) ) You may “visualize” the above procedure as: Note that you can group by multiple variables (e.g., participants within countries). Below, some examples with group by and summarize. # Suppose we want to compute the mean and sd for rt per participant. mydata %&gt;% group_by(participant) %&gt;% summarize(aggr_rt=mean(rt), sd_rt=sd(rt)) # Again, note that the output is a seperate dataset # If you want, we can &quot;glue&quot; our freshly computed mean and standard deviation to each corresponding participant # Below, a sneak peak on how to do the above (WITHOUT SAVING THE &quot;NEW SMALLER DATASET&quot; FIRST). mydata %&gt;% group_by(participant) %&gt;% summarize(aggr_rt=mean(rt), sd_rt=sd(rt)) %&gt;% left_join(mydata) 3.4.6 Dplyr: Join As I demonstrated above, the obtained smaller dataset can be glued to the original one if desired. This glueing of datasets often can come in handy. For example, suppose you ran an experiment and you obtain two separate data files: one from the experiment itself and one on the demographics from the participant. Say you want to test how the main effect of some variable is moderated by some demographic, you will need to merge both data files into one dataset. There exist different types of join() such as inner_join, right_join. Personally, I mainly use left_join() since it is straightforward. The idea is that I put two datasets into this function so e.g., left_join(dataset_1, dataset_2). What it will do is to find variables that both datasets share, variables that share the same name and the same type (numeric or character). After identifying these shared so called “key variables”, it will then glue all “non-shared” variables. In our example with left_join(), it will glue data_set 2 to data_set 1 (so the second argument “dataset_2” to the left of argument “dataset_1). A visual example: My advice, after joining, always check whether the amount of variables in your original dataset stayed the same. Sometimes it could be that (unbeknownst to you) there are duplicates or empty values (coded as NA) in your variables. This may lead to left_join returning more values. So check for these if you would encounter this problem. What also could help is to make sure that your second dataset (the one to glued on the main one) is as small as possible (while withholding necessary information) and here group_by and summarize could help. Below, a small example of left_join, the last one before we move to a “live” demonstration of data preprocessing. # Say we want to &quot;join/glue&quot; the aggregated rt and sd from the previous code exaple to the main dataset data_aggr = mydata %&gt;% group_by(participant) %&gt;% summarize(aggr_rt=mean(rt), sd_rt=sd(rt)) left_join(mydata, data_aggr) 3.4.7 Practical example data shaping/cleaning If this was your first encounter with dplyr, all these different functions might be overwhelming. I understand. For me personally, what I find most appealing about dplyr is that by “chaining” different functions (pipelining) and by allowing to do multiple things at once, it made data preproccesing and my R scripts in general, less chaotic for myself. Yes, %&gt;% this and %&gt;% that… can lead to behemoths of code that may seem intimidating at first. As with everything new, the beginning can feel akward. Perhaps by further example, things become clearer. To conclude this section, I will demonstrate a complete example from scratch, integrating what we have learned about dplyr so far (plus some new elements added along the way). I made two raw datasets: a main dataset (mydata_prac.xlsx) and a smaller one (mydata_prac_add.csv). In a nutshell I will have to do the following preprocessing steps: 1. Check my data 2. Recode/rename/transform/create variables 3. Join datasets 4. Create the preprocessed dataset ############################ # Preprocessing example # ############################ library(pacman) p_load(dplyr,stringr,corrtable,readxl, haven) options(scipen=999) set.seed(54321) ## # Load data file and inspect data ## mydata = read_xlsx(&quot;data_files/mydata_prac.xlsx&quot;) View(mydata) # -&gt; So far so good. However, one participant shows no data so here I decide to delete this one (filter it out) mydata = mydata %&gt;% filter(!participant==11) # -&gt; Ok so now I should have 10 partipants, let&#39;s quickly check. length(unique(mydata$participant)) ## # Recode variables ## # Items &quot;q&quot; need to be recoded to values (heavily disagree = 1 ; heavily agree = 5) mydata = mydata %&gt;% mutate(across(starts_with(&quot;q_&quot;), ~ recode(.,&#39;heavily disagree&#39; = 1, &#39;disagree&#39; = 2, &#39;neutral&#39; = 3, &#39;agree&#39; = 4, &#39;heavily agree&#39; = 5) )) # Turns out question q_1 and q_2 need to be REVERSED code mydata = mydata %&gt;% mutate(across(starts_with(&quot;q_&quot;) &amp; !ends_with(&quot;3&quot;), ~ abs(.-6))) # -&gt; note !ends_with = does NOT end with; abs take the absolute value otherwise I end up with negative values. View(mydata) # Check your data ### # Rename and transform variables ### # First off the &quot;age&quot; variable was misnamed and should be called &quot;date_of_birth&quot; # Similarly, &quot;q_3&quot; is actually &quot;r_4&quot; # rt is not a character but should be coded as a numerical value. Participant should be coded as a factor mydata = mydata %&gt;% rename(birth_date=age, r_4 = q_3) %&gt;% mutate(rt = as.numeric(rt), participant = as.factor(participant)) # -&gt; Note that the dataset reads as r_4; r_1; r_2; r_3 # -&gt; This may trigger someone so lets quickly change the order # The current column number 7 (&quot;r_4&quot;) needs to placed last: mydata = mydata[,c(1:6,8:10,7)] # from &quot;participant&quot; to &quot;q_2&quot;, from &quot;r_1&quot; to &quot;r_3&quot;, &quot;r_4&quot; ### # Compute new variables ### # Moving on, I will create a variable &quot;participant_age&quot; which is the difference in time between the variable &quot;birth_date&quot; and a certain date (say the first of April 2023) # I will use the packages anytime and lubridate pacman::p_load(anytime,lubridate) mydata = mydata %&gt;% mutate(age =time_length( interval( as.Date(anytime(birth_date)), as.Date(&quot;2023-04-01&quot;) # Note, has to be in YYYY/MM/DD format. ), unit =&quot;year&quot; # To get the difference in years ) ) # Next, I wish to compute the row-wise averages of the &quot;q_&quot; variables as well ass the &quot;r_&quot; variables. BUT I don&#39;t want to have the average (set to an empty value) if only one item was filled in (row-wise) # Essentially, we have to base whether or not to average based on the sum of NA (a.k.a. empty values) per row # In other technical words: if our the condition is met (the sum of NA&#39;s per row is 1 or lower), compute the mean (ignore NA&#39;s) mydata = mydata %&gt;% mutate( q_scale = ifelse(rowSums(across(starts_with(&quot;q_&quot;), ~is.na(.)))&lt;=1, rowMeans(across(starts_with(&quot;q_&quot;)),na.rm=TRUE),NA ), r_scale = ifelse(rowSums(across(starts_with(&quot;r_&quot;), ~is.na(.)))&lt;=1, rowMeans(across(starts_with(&quot;r_&quot;)),na.rm=TRUE),NA ) ) # Again be mindfull of where to put across. # Check your data View(mydata) # It worked... but some ages are set to zero. Transform &#39;em to NA mydata = mydata %&gt;% mutate(age = ifelse( age==0,NA, age)) # It is a good thing we frequently check our data :-) # Next, PER PARTICIPANT, I want the average rt and the average of the freshly created &quot;q_scale&quot; and the &quot;r_scale&quot; (ignore NA). # In addition, I want to grand-mean center the average rt per participant. # For the first part, we can use the group_by() and summarize() combo. We could join the created miniature dataset with the main one to grand-mean center. # I could glue datasets using left_join()... but in this example, I will &quot;pipeline&quot; all functions without &quot;breaking the chain&quot; # To keep the chain intact, I will have to use right_join() # You see, after &quot;summarizing&quot; the dataset, a miniature dataset will be created. From that moment, if you pipeline further, you will be using that miniature dataset. Now, with right_join(), you tell R: &quot;use the first dataset (i.e., the miniature dataset) to glue it to the second one (i.e., our original dataset), which is exactly what we want to preserve the chain mydata = mydata %&gt;% mutate(rt = as.numeric(rt)) %&gt;% group_by(participant) %&gt;% summarize( rt_average_part = mean(rt,na.rm = TRUE), q_average_part = mean(q_scale,na.rm = TRUE), r_average_part = mean(r_scale,na.rm = TRUE) ) %&gt;% right_join(mydata) %&gt;% mutate(rt_grand_mean = scale(mydata$rt, center = TRUE, scale = FALSE) ) # Check. your. data. View(mydata) ### # Merge datasets ### # Let&#39;s wrap it up. Glue the &quot;mydata_prac_add.csv&quot; to the main one and recode gender into words (1 = female, 2 = male) mydata = left_join(mydata, read.csv(&quot;data_files/mydata_prac_add.csv&quot;) %&gt;% mutate(gender = ifelse(gender == 1,&quot;female&quot;,&quot;male&quot;)) ) # You know what to do View(mydata) # Everything looks fine, save the &quot;clean&quot; dataset in a &quot;clean&quot; datafile. write.csv(mydata,&quot;data_files/mydata_prac_finished.csv&quot;) # Done. "],["data-visualisation.html", "Chapter 4 Data visualisation 4.1 The language of ggplot 4.2 Plotting various graphs. 4.3 How to customize your plots 4.4 Interactive plots", " Chapter 4 Data visualisation Humans are highly visual creatures, exploit that. Visual depictions of the distribution and trends in your data are not only appealing to the eye, at times these may tell us more than a bunch of p-values and the like. Data visualization is a strong point of R and covered in a variety of packages. My package of choice? ggplot2. The ggplot package allows (copious customization possibilities), it can provide aesthetically-pleasing plots, and you can plot various things including graphs that you can interact with in real-time”. As you might suspect, this will be a very visual chapter. I will go over: The basics of ggplot2. I’ll go over aspects such as the aes() part (aesthetics) and what arguments to put where. Simple demonstration on how to plot a variety of graphs and some nice “additions” to them. I will restrain myself to histograms (with normal distribution indication), density plots, box plots, (split) violin plots, bar graphs, pie charts, and scatter plots. I will not heed much attention to aesthetics and professional look of the plots, that part will be covered next. More in depth customization to make them professional and beautiful. Demonstrating how to adjust some important elements of the canvas and the graphs on it (the text elements, colors, sizes, how to hide things, …) and how to combine plots cowplot package. This will be illustrated in two “more advanced” examples in which I also address some issues that you might not expect. Interactive graphs. These will be ideal for R Markdown reports (HTML output style), when you have multiple cluster units (like participants or species of something), or when you want to look at change over time. 4.1 The language of ggplot Similar to the pipes and specific functions of dplyr, ggplot has it own language. Lets quickly familiarize. Everything starts with the following code: your dataset %&gt;% ggplot(aes() ) or without pipelining ggplot(data= your dataset, aes()). The ggplot() part will draw an empty canvas where you will add elements like lines, bars, points, etc. In the aes() part, which stands for aesthetics, you will define characteristics of the things you want to draw, like the values on the x-axis, values on the y-axis, color, text, and so on. However, the characteristics put within aes() will always be based of your dataset. In other words, if you need something (values or text) from your data for your plot, put it in aes(), if not, put it outside. Say you have the following data: pacman::p_load(dplyr, ggplot2) set.seed(1) example_dataset = data.frame( group = factor(c(1,2,3)), x = round(runif(3,1,10),0), y = round(runif(3,1,10),0) ) You want plot the cross points where y crosses x. Of course the values on the x-axis and those on the y-axis need to be taken from our dataset itself. So we need to put these in aes(), not outside of it. example_dataset %&gt;% ggplot(aes(x=x, y=y)) + geom_point() We could actually define two aesthetics, one within ggplot() and/or one within geom_point(). The difference is that the aes() within ggplot() will apply the defined characteristics to all your drawings (lets’ call it the global aesthetics). example_dataset %&gt;% ggplot() + geom_point(aes(x=x, y=y)) The aes() within geom_point() will apply stuff only to that drawing (call it the local aesthetics), only to the points in this case. Its good to know that the global aesthetics can be overridden in the local aesthetics (as demonstrated later on) so that your lines, bars, and point can be colored differently. Moving on. Assume we want to color every point red. We can do this without “referring” to our dataset, and therefore we put it outside aes() example_dataset %&gt;% ggplot(aes(x=x, y=y)) + geom_point(color=&quot;red&quot;) However, what if we want to give the points a different color based on the group variable from our dataset? Indeed, you will need to put within an aesthetic. example_dataset %&gt;% ggplot(aes(x=x, y=y, color=group)) + geom_point() To end this part, let me prove that that a local aesthetic can override the global one. All points will be blue: example_dataset %&gt;% ggplot(aes(x=x, y=y, color=group)) + geom_point(color=&quot;blue&quot;) 4.2 Plotting various graphs. Now the fun part, ggplot allows to plot various graphs with many customization options. Before I demonstrate some important customization possibilities, I will show R code that will output a variety of common graphs (e.g., histograms and bar graphs) as well as some less common ones. For this purpose I will use a custom dataset provided by R about three species of flowers. mydata = iris 4.2.1 Histogram Starting of with the classics. # Suppose we want to have black contours and a white fill color mydata %&gt;% ggplot(aes(x=Sepal.Length)) + geom_histogram(fill = &quot;white&quot;, color =&quot;black&quot;,bins=20) 4.2.2 Density plots # Blue fill color (i.e., the area covering by the density plot) and making it transparent mydata %&gt;% ggplot(aes(x=Sepal.Length )) + geom_density(fill =&quot;blue3&quot;, alpha =0.50) You can also put a density plot over a histogram. Note that a histogram shows the counts of values within a given range while a density plot shows the proportion. As a consequence, they do not share the same y-axis, and we will need to fix this using the following argument (y = ..density..). # Histogram: black contour, blue fill, a bit of transparency # Density plot: yellow fill, a high degree of transparency # In addition: # A vertical black line indicating the mean of Sepal length # A vertical red line indicating the median of sepal Length mydata %&gt;% ggplot(aes(x=Sepal.Length )) + geom_histogram(aes(y =..density..),fill=&quot;blue&quot;, color = &quot;black&quot;, alpha = 0.7) + geom_density( fill = &quot;yellow&quot;, alpha = 0.3) + geom_vline(aes(xintercept = mean(Sepal.Length)), color=&quot;black&quot;) + geom_vline(aes(xintercept = median(Sepal.Length)), color=&quot;red&quot;) 4.2.2.1 Adding a normal distribution indication In software such as SPSS you get a reference to a normal distribution when plotting histograms. This is not by default in R so we will have to it ourselves. We can use stat_function() to draw a line that would depict how your variable would look like if your variable was normally distributed. Now, a normal distribution is made by two parameters: the mean and the standard deviation. Therefore, we must tell stat_function to use the mean and standard deviation of our variable. Moreover, we need to use our previous trick with y =..density.. # Histogram: black contours, white fill color # A Line showing the normal distribution colored in blue # Additionally: # A black slightly transparent line indicating the mean # Two dashed slightly transparent red lines indicating +/- 1SD from the mean # Adjust the titles: plot title = &quot;Histogram; x-axis= &quot;Sepal width&quot;; y-axis= &quot;Frequency&quot; mydata %&gt;% ggplot(aes(x=Sepal.Width)) + geom_histogram(aes(y=..density..),fill = &quot;white&quot;, color =&quot;black&quot;) + stat_function(fun=dnorm, args=list(mean=mean(mydata$Sepal.Width), sd=sd(mydata$Sepal.Width)),color=&quot;blue&quot;) + geom_vline(aes(xintercept = mean(Sepal.Width)), alpha = 0.8, color =&quot;black&quot;) + geom_vline(aes(xintercept = mean(Sepal.Width) - sd(Sepal.Width) ), linetype =&quot;dotted&quot;, alpha = 0.8, color =&quot;black&quot;) + geom_vline(aes(xintercept = mean(Sepal.Width) + sd(Sepal.Width) ), linetype =&quot;dotted&quot;, alpha = 0.8, color =&quot;black&quot;) + ggtitle(&quot;Histogram&quot;) + xlab(&quot;Sepal width&quot;) + ylab(&quot;Frequency&quot;) 4.2.3 Violin plots A personal favorite of mine, music to my ears. The violin may be a bit more exotic so allow me to explain. It can be described as a mirrored distribution plot. A box plot is put on top of it, so it will resemble a violin. The major strength of the violin is that, as a hybrid, it combines the advantages of the box plot (median/mean, quartiles,…) and the density plot (more detailed distribution). Suppose we want to compare the petal length across flower species: # Violin plot: fill color per species; the mean is added in the box plots (using stat_summary) mydata %&gt;% ggplot(aes(y=Sepal.Length, x = Species, fill = Species)) + geom_violin() + geom_boxplot(color=&quot;black&quot;, alpha = 0.75) + stat_summary(func = mean) 4.2.4 Split violin plots One unfortunate part of violin plots is the redundancy of mirrored distributions. One half is enough. Luckily, packages like introdataviz made it easy to differentiate the right/left side (or top/down part as shown below). Suppose we can categorize flowers in young and old. Per species we could make one side of the violin reflecting young flowers, the other reflecting old ones. I must note that we cannot (yet) install introdataviz the “basic way”. We will have to use the devtools package: # devtools::install_github(&quot;psyteachr/introdataviz&quot;) library(introdataviz) ### Split-violin plot # Give a fill color per age group # Getting tired of the standard colors that R will use? Tell R the colors you want (we will do it manually) # Plot a transparent split-violin plot # Here I will R to NOT trim the end points (just an aesthetical consideration) # Plot a transparent box plot # Add a dot to represent the mean # Use position dodge to adjust where this dot will be placed # Flip the plot (90 degrees to the left) mydata %&gt;% mutate(age = rep(c(&quot;young&quot;,&quot;old&quot;),times=75)) %&gt;% ggplot(aes(y = Sepal.Length, x = Species, fill = age)) + scale_fill_manual(values = c(&quot;royalblue&quot;, &quot;gold1&quot;)) + geom_split_violin(alpha = 0.5, trim = FALSE) + geom_boxplot(color=&quot;black&quot;, alpha = 0.75, width = 0.2) + stat_summary(fun = mean, position = position_dodge(0.15), color=&quot;black&quot;) + coord_flip() 4.2.5 Bar graphs # Bar plot (counting occurennces of a given letter) # Plot a white transparent bar plot COUNTING the amount of letters in the dataset data.frame(letters = c(rep(c(&quot;a&quot;), each=10), rep(c(&quot;b&quot;), each=7), rep(c(&quot;c&quot;), each=15), rep(c(&quot;d&quot;), each=3), rep(c(&quot;e&quot;), each=7))) %&gt;% ggplot(aes(x=letters)) + geom_bar(color=&quot;black&quot;, fill=&quot;white&quot;, alpha = 0.75) # Bar plot (showing the VALUES of the letter counts) # Plot a white transparent bar plot &quot;identifying the values&quot; and showing them # Note, instead of geom_bar(stat = &quot;identity&quot;) we could use geom_col data.frame(letters = c(rep(c(&quot;a&quot;), each=10), rep(c(&quot;b&quot;), each=7), rep(c(&quot;c&quot;), each=15), rep(c(&quot;d&quot;), each=3), rep(c(&quot;e&quot;), each=7))) %&gt;% group_by(letters) %&gt;% count() %&gt;% ggplot(aes(x = letters, y = n)) + geom_bar(stat = &quot;identity&quot;, color=&quot;black&quot;, fill=&quot;white&quot;, alpha = 0.75) 4.2.6 Piecharts To my knowledge, there is no function like geom_pie within ggplot. Instead, pie-charts will actually take the form of a bar graphs (the identity type) and trough the coord_polar function these bars take a circular shape. # Pie-chart # Plot a bar plot (&quot;identity&quot;) with a fill color per letter # Transform to a circle (coord_polar) # add percentages as text on the pie-chart # To do so, in the our dataset we can add a variable depicting the percentage # To the percentage value I will add a &quot;%&quot; sign # Use the &quot;Dark2&quot; palette to adjust fill color data.frame(letters = c(rep(c(&quot;a&quot;), each=10), rep(c(&quot;b&quot;), each=7), rep(c(&quot;c&quot;), each=15), rep(c(&quot;d&quot;), each=3), rep(c(&quot;e&quot;), each=7))) %&gt;% group_by(letters) %&gt;% count() %&gt;% mutate(letters=as.factor(letters)) %&gt;% ungroup() %&gt;% mutate(percentage = as.character( round(n/sum(n)*100,1) ), percentage = paste0(percentage,&quot;%&quot;)) %&gt;% ggplot(aes(y=n, x=&quot;&quot;, fill = letters)) + geom_bar(stat=&quot;identity&quot;, color=&quot;black&quot;, alpha = 0.75) + coord_polar(&quot;y&quot;) + geom_text(aes(label = percentage), color = &quot;white&quot;,size = 2, position = position_stack(vjust = 0.5)) + theme_void() + scale_fill_brewer(palette=&quot;Dark2&quot;) 4.2.7 Scatter plots Suppose we want to visually inspect if lower or higher values of petal length go with higher or lower values of the sepal length. In the example below,I will split the plot (containing all flower species on the same canvas) in three separate ones using facet_wrap() to ease looking at each species separately. # Scatter plot: colored per species and separated by species mydata %&gt;% ggplot(aes(y=Petal.Length, x = Sepal.Length, color = Species)) + geom_point() + facet_wrap(~Species) + ggtitle(&quot;Scatter plot&quot;) + xlab(&quot;Sepal length&quot;) + ylab(&quot;Petal length&quot;) 4.2.7.1 Adding a regression line Say we want to quickly inspect whether we can “forcefully” draw a diagonal linear line between petal and sepal length (for a first indication of a linear relation). To do this we can employ the geom_smooth() function which will regress such a line and provide a spread around it (95% confidence interval by default but changeable). Lets add one in the plot above. Note that we will have a line per species separately. mydata %&gt;% ggplot(aes(y=Petal.Length, x = Sepal.Length, color = Species)) + geom_point() + facet_wrap(~Species) + geom_smooth(method=&quot;lm&quot;, se=TRUE, color=&quot;black&quot;, fill=&quot;grey20&quot;) + ggtitle(&quot;Scatter plot&quot;) + xlab(&quot;Sepal length&quot;) + ylab(&quot;Petal length&quot;) I can prove that geom_smooth() did a simple linear behind the scenes. I will just need to quickly extract some parameters from the linear regression (spoilers for the next chapter). For simplicity lets focus on the species “Setosa”. Ta-da: mydata_setosa = mydata %&gt;% filter(Species==&quot;setosa&quot;) # Keeping one species mylm = lm(Petal.Length ~ Sepal.Length,data=mydata_setosa) # linear regression mydata_setosa %&gt;% mutate( the_line = predict(mylm), # Extract the predicted values (forming the line) CI_lower = as.data.frame(predict(mylm,interval=&quot;confidence&quot;))$lwr, # Lower bound CI_upper = as.data.frame(predict(mylm,interval=&quot;confidence&quot;))$upr # Upper bound ) %&gt;% ggplot(aes(x = Sepal.Length)) + geom_point(aes(y = Petal.Length), color=&quot;firebrick&quot;) + geom_line(aes(y = the_line), color=&quot;black&quot;) + geom_ribbon(aes(ymin = CI_lower, ymax = CI_upper), fill=&quot;grey20&quot;,alpha=0.25) + ggtitle(&quot;Scatter plot&quot;) + xlab(&quot;Sepal length&quot;) + ylab(&quot;Petal length&quot;) Now, geom_smooth can also be used to quickly inspect other patterns in the data. For example, lets use the default “LOESS” argument which will not draw a straight line but rather will “loosely” follow the data (i.e., using local averaging which “rolls” out the mean across the x-axis). The result should resemble pasta al dente. Below I will also hide the 95% confidence interval mydata %&gt;% ggplot(aes(y=Petal.Length, x = Sepal.Length, color = Species)) + geom_point() + facet_wrap(~Species) + geom_smooth( se=FALSE, color=&quot;black&quot;, fill=&quot;grey20&quot;) + ggtitle(&quot;Scatter plot&quot;) + xlab(&quot;Sepal length&quot;) + ylab(&quot;Petal length&quot;) 4.3 How to customize your plots Up-till now I have not paid much attention to the appearance of the plots. Just as the data is “fluid”, plots are as well. Ggplot allows full customization of the canvas and everything on it. 4.3.1 changing the theme, colors, fonts, combining plots, and adding p-value indications You can change each individual element of the theme click here for an overview. If you clicked, you have noticed that there is a lot that you can change. To avoid a lengthy discussion, I will about the default themes and give two relevant demonstrations in which. The ggplot2 package has several built-in themes. By default ggplot goes with theme_grey, the one you saw in each plot. Other built-in themes include theme_bw(), theme_linedraw(), theme_light(), theme_dark(), theme_minimal(), theme_classic(), and theme_void(). Click here to have look Personally,I mainly choose theme_classic() as I will do in the following examples. 4.3.2 Example 1 Suppose you collected a number of flower species and measured their petal length. You want a plot that shows whether the differences in the average petal length between species are statistically significant (based on p-values). In addition, you are subjected to certain rules: 1. No color, only grayscale 2. There must a confidence interval. Here lets pretend(!) that the upper and lower bounds of our confidence intervals are half a standard deviation above and below the average petal length. 3. Times New Roman font style with font size 12 for the titles of the x- and y-axis, font size 14 for the title presented in bold, font size 10 for the text of the marks. 4. The legend ought to be placed on the bottom I will use the stat_pvalue_manual() function from the ggpubr package as it is user-friendly for the “novice” R user. Crucially, the stat_pvalue_manual functions demands a couple of things from you**: a first group, a second group, and the position of y-axis where you want to put the p-values. All will be clarified soon. First things first, we need to calculate the average petal length per species as well as our confidence intervals (i.e., upper and lower bounds). group_by() and summarize() from dplyr can help here. As you recall, this creates a mini dataset. To this miniature dataset I will have to add five things: group 1, group 2, the obtained p-value (arbitrary for current demonstration purposes), statistical significance signs, and the position on the y-axis where I want to put the p-values. What is group 1 and group 2? Well in our example we compare 3 species with one another, so species A with B, A with C, and B with C. In group 1 we add the left side of all comparisons (A with B, A with C, and B with C). In group 2 we add the right side of the comparison (A with B, A with C, B with C). If you would compute this and view your dataset you will see that you have each combination beneath one another. Let’s Move on, about the p-value, I will arbitrarily choose values for each combinations: &lt;.001 (A with B), .040 (A with C), and .231 (B and C). Keeping these values in mind, the sign of our p-values will be respectively three stars (p&lt;.001), one star (p =.040), and “not significant” (p = .231). Finally, about y-position, I could put them in a position corresponding with the petal length averages. However, this would look unclear so I will add a value to these averages so that the position will shift from slicing the top of the bar to “hoovering” a bit above it. mydata_example1 = mydata %&gt;% group_by(Species) %&gt;% summarise(average_Petal_Length=mean(Petal.Length), CI_lwr = average_Petal_Length - (0.5*sd(Petal.Length)), CI_upr = average_Petal_Length + (0.5*sd(Petal.Length)) ) %&gt;% mutate( p = c(&quot;(&lt;.001)&quot;,&quot;(.040)&quot;,&quot;(.231)&quot;), # Adding the ARBITRARY p-values p_notation = c(&quot;***&quot;,&quot;*&quot;,&quot;ns&quot;), # The &quot;signs&quot; of the above p-values group1 = c(&quot;setosa&quot;,&quot;setosa&quot;,&quot;versicolor&quot;), group2 = c(&quot;versicolor&quot;,&quot;virginica&quot;,&quot;virginica&quot;), y.position = average_Petal_Length + 4 # For the y-position, adding it with 4 so it &quot;hoovers&quot; above the bars.The x-position will correspond with Species ) Create the bars with the p-values on top (ignoring rules such as grayscale). library(ggpubr) mydata_example1 %&gt;% ggplot(aes(x=Species,y=average_Petal_Length,fill=Species)) + theme_classic() + geom_bar(stat=&quot;identity&quot;) + geom_errorbar(aes(ymin = CI_lwr, ymax = CI_upr), width=0.2) + stat_pvalue_manual(mydata_example1, label = &quot;{p} {p_notation}&quot;, size =3) This looks acceptable. However, here we encounter something unexpected, notice the bars float above the x-axis? To bring the bars down, I specify the range of y-axis using coord_cartesian() and use the scale_y_continuous() function. mydata_example1 %&gt;% ggplot(aes(x=Species,y=average_Petal_Length,fill=Species)) + theme_classic() + geom_bar(stat=&quot;identity&quot;) + geom_errorbar(aes(ymin = CI_lwr, ymax = CI_upr), width=0.2) + stat_pvalue_manual(mydata_example1, label = &quot;{p} {p_notation}&quot;, size =3) + coord_cartesian(ylim=c(0,10)) + # new scale_y_continuous(expand = expansion(mult = c(0, 0))) # new Good, lets’ address our given rules. To get grayscale we can use the scale_fill_manual function to need to determine the colors ourselves. Font family, size, and style (or face) can be changed in the theme(). Titles are by default left-aligned but this can easily fixed. Similar to the font, the position of the legend (even its existence) can be changed in the theme(). Since we have a legend, we don’t need a title on the x-axis plus there is no need for mentioning the word “species” in the legend itself. The end product: mydata_example1 %&gt;% ggplot(aes(x=Species,y=average_Petal_Length,fill=Species)) + theme_classic() + geom_bar(stat=&quot;identity&quot;) + geom_errorbar(aes(ymin = CI_lwr, ymax = CI_upr), width=0.2) + stat_pvalue_manual(mydata_example1, label = &quot;{p} {p_notation}&quot;, size =3) + coord_cartesian(ylim=c(0,10)) + scale_y_continuous(expand = expansion(mult = c(0, 0.0))) + scale_fill_manual(values=c(&quot;grey65&quot;,&quot;grey45&quot;, &quot;grey25&quot;)) + xlab(&quot;&quot;) + ylab(&quot;Average petal length&quot;) + ggtitle(&quot;Comparing species based on petal length&quot;) + theme( axis.title.y = element_text(size=12, family = &quot;Times New Roman&quot;), plot.title = element_text(size = 14, family = &quot;Times New Roman&quot;, face=&quot;bold&quot;,hjust = 0.5), # h(orizontal)just ranges from 0 (left) to 1 (right) so 0.5 is the middle axis.text.y = element_text(size = 10), # text on the &quot;ticks&quot; on the y-axis axis.text.x = element_text(size = 10), # text on the &quot;ticks&quot; on the x-axis legend.position = &quot;bottom&quot;, legend.title = element_blank() # otherwise the word &quot;Species&quot; will appear in the legend ) 4.3.3 Example 2 Staying with the flowers. Suppose you want to have scatter plot using sepal width (x-axis) and sepal length (y-axis). Above this scatter plot you want a histogram of the predictor (the variable on the x-axis); on the right side you want a histogram of the outcome (variable on the y-axis). This time, you get the following rules. 1. No legend 2. Use the following colors: hotpink2, darkgoldenrod3, and darkorchid1 yes these are colors in R, click here 3. The name of the species should be put somewhere random on the scatter plot without using aes(). 4. The histogram of the outcome should be rotated 90° to the right. 5. Histograms should be small in height, have 20 bins, and have a “normal distribution showing line”. Good, so we will need to make 3 graphs: one scatter plot, two histograms. We can use the package cowplot to “glue” the plots together into one object. The same package can also be used to reduce the height of the histograms. We are already familiar with scatter plots. The novelty is the removal of the legend but this can be easily done in the theme(). For the text objects (the species), we can use geom_text and place it somewhere. I will name the plot “scatter”. scatter = mydata %&gt;% ggplot(aes(y = Sepal.Length, x = Sepal.Width, color = Species)) + theme_classic() + scale_color_manual(values = c(&quot;hotpink2&quot;,&quot;darkgoldenrod3&quot;,&quot;darkorchid1&quot;)) + geom_point() + geom_text(label=&quot;Setosa&quot;, x=2.1, y=7.5,color=&quot;hotpink&quot;) + geom_text(label=&quot;Versicolor&quot;, x=2.1, y=7.2,color=&quot;darkgoldenrod3&quot;) + geom_text(label=&quot;Virginica&quot;, x=2.1, y=6.9,color=&quot;darkorchid1&quot;) + ylab(&quot;Sepal length&quot;) + xlab(&quot;Sepal width&quot;) + theme( legend.position = &quot;none&quot; # Removes the legend ) We are also familiar histograms with a “normal distribution line”. I will remove the x-and y-axis, text, and tick marks (those small vertical stripes on an axis). This can be done in the theme() as shown in the code for histogram “hist_x” but we can also use theme_void() as shown in the code for histogram “hist_y”. Very important, since we need to rotate one of our histograms using the coord_flip() function, my tip is to put coord_flip() right after ggplot(). The histogram could change to some extend If you put coord_flip later in the code! hist_x = mydata %&gt;% ggplot(aes(x = Sepal.Width)) + geom_histogram(aes(y=..density..),fill=&quot;grey40&quot;,color=&quot;grey10&quot;, bins = 20) + stat_function(fun=dnorm, args=list(mean=mean(mydata$Sepal.Width), sd=sd(mydata$Sepal.Width)),color=&quot;hotpink&quot;, linetype = &quot;dotdash&quot;,size=2) + xlab(&quot;&quot;) + ylab(&quot;&quot;) + theme_classic() + theme( axis.line.x = element_blank(), axis.ticks.x = element_blank(), axis.text.x = element_blank(), axis.line.y = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_blank() ) hist_y = mydata %&gt;% ggplot(aes(x = Sepal.Length)) + coord_flip() + geom_histogram(aes(y=..density..),fill=&quot;grey40&quot;,color=&quot;grey10&quot;, bins = 20) + stat_function(fun=dnorm, args=list(mean=mean(mydata$Sepal.Length), sd=sd(mydata$Sepal.Length)),color=&quot;hotpink&quot;, linetype = &quot;dotdash&quot;,size=2) + xlab(&quot;&quot;) + ylab(&quot;&quot;) + theme_void() Alright, lets use the cowplot package to glue the plots together and make it so that the histograms are shorter, putting the scatter plot is in the spotlight. Now our current example where you want to put one plot in the spot light and putting multiple smaller plots above,below,left, or right, is a tricky one. If we did not have to adjust the size of the histograms, we could have done something simple like this: library(cowplot) plot_grid( hist_x,NULL,scatter,hist_y, align=&quot;hv&quot; ) Where align = “hv” ensures that the axes are lined up appropriately both horizontally and vertically. If we would have to add only one histogram (e.g., the y-axis one), we could have done something like this: plot_grid( scatter,hist_y, rel_widths = c(1,0.3), # Here the width of the histogram is multiplied by factor 0.3 align=&quot;h&quot; # horizontally ) But enough of that, lets reduce the size of the histograms and align them to the scatter plot. In this case, we need to first align our histograms using the align_plots() function from the cowplot package. Then we can use these aligned histograms alongside our scatter plot # Create the aligned histograms (aligned to the scatter plot). aligned_hist_x = align_plots(hist_x, scatter, align = &quot;v&quot;)[[1]] # [[1]] will extract the aligned version of our original hist_x aligned_hist_y = align_plots(hist_y, scatter, align = &quot;h&quot;)[[1]] # Arrange plots plot_grid( aligned_hist_x , NULL , scatter , aligned_hist_y , ncol = 2 , nrow = 2 , rel_heights = c(0.3, 1) # hist_x reduced in size, NULL (nothing) with no size adjustments , rel_widths = c(1, 0.3) # scatter plot with no size adjustments sized, hist_y reduced in size ) 4.4 Interactive plots Wrapping up this section, and you have many “cluster units” (participants, test subjects, etc.). You want to plot the change in some measurement per cluster unit, over time. We could quickly visualize this: mydata = data.frame( participant = factor(rep(c(1:50),times = 5)), score = runif(250,1,10), time = rep(c(1:5), each = 50) ) mydata %&gt;% ggplot(aes(y=score, x=time, color=participant)) + geom_point() + geom_line() Have fun figuring out the change per participants… Luckily, the ggplotly() function from the plotly package package offers to interact with our plots. If you use plotly(), and you have a lot of cluster units, make sure to remove the legend (you will have access to it anyways) otherwise it will be only the legend that you will see. Look at the plot below. This is a plot that you can interact with on this very own webpage. Hover over any of the dots and you should see a mini window showing the time, score, and participant. In R Markdown and R Studio, you would also see the list of participants on the left side. Unfortunately you cannot see that part on this webpage so I will have to describe it. If I would click once on participant 1 in the participant list (that you cannot see), it would temporarily remove that participant in the plot. If I would click once on them again, they will reappear. If I Click twice rapidly in succession on participant 20, I would only see the dots and lines of this participant. If I would click twice again, everything changes to normal. In short, you can “show” and “hide” participants at will. There are some others you can do but this is beyond the scope of this guide. library(plotly) ggplotly( mydata %&gt;% ggplot(aes(y=score, x=time, color=participant)) + geom_point() + geom_line() + theme( legend.position = &quot;none&quot; ) ) The last plot of this chapter. Before we can rest our eyes for a bit, Since we have multiple measurements per cluster unit, we could consider to plot a simple regression line per participant (again just for demonstration purposes). Here we could consider to plot the simple regression slope per participant alongside the overall slope ggplotly( mydata %&gt;% ggplot(aes(y=score, x=time, color=participant)) + geom_smooth(method=&quot;lm&quot;, se = FALSE) + # per participant geom_smooth(aes(group=0), method=&quot;lm&quot;, se = FALSE, color=&quot;black&quot;) + # Overall (I like to call this &quot;ungrouping&quot;) geom_smooth(aes(group=0), se = FALSE, color=&quot;black&quot;, linetype=&quot;dashed&quot;,alpha=0.75) + theme( legend.position = &quot;none&quot; ) ) In ending, I want to note that I will show more different plots in later parts such as when discussing linear regressions (e.g., johnson-neyman intervals) and mediation (path diagrams). "],["reliability-indicators-confirmatory-factor-analysis-and-exploratory-factor-analysis.html", "Chapter 5 Reliability indicators, Confirmatory Factor Analysis, and Exploratory Factor Analysis 5.1 Cronbach’s alpha 5.2 Omega 5.3 Exploratory factor analysis 5.4 Referred article", " Chapter 5 Reliability indicators, Confirmatory Factor Analysis, and Exploratory Factor Analysis By now you have some useful skills under your belt: data preprocessing/cleaning, calculations, data visualization… For the remaining parts, I shift the focus more into data analysis, and this mainly from a “investigation/research/testing” view. This particular part may interest you, in particular if you are affiliated with fields such as Psychology. In those fields, you may be confronted with questionnaires that are presumed to measure something… often an abstract construct. Questionnaires often include multiple items that are presumed to represent the constructs the questionnaire are presumed to measure. For example, items like “happy”, “energized”, “relaxed”, “calm”, could be presumed to reflect the construct “positive affect”. The question arises, do you have some indications for the consistency of these questionnaires? Do the questionnaire items reliably tap onto the assumed construct? Can our questionnaire items be summarized in factors (“more latent constructs”) and in how may? In this part I’ll go over reliability, confirmatory factor analysis, and exploratory factor analysis. Specifically: Starting with a classic (but debated) indicator of reliability, Cronbach’s alpha.. We will compute this indicator using the psych package and I’ll show you how to compute it by hand. We explore McDonald’s Omega reliability indicator as an alternative to Cronbach’s alpha. During this part I will use the lavaan package and the semTools package to introduce you to Confirmatory Factor Analysis, factor loadings (checking tau equivalence), CFA fit indicators and how we could improve them. How to conduct an Exploratory Factor Analysis to unveil the underlying structure of the data. We will briefly discuss some considerations concerning this kind of analysis including multicollinearity (variance inflation factors), multivariate normallity (Mardia’s skewness and kurtosis), and univariate/multivariate outliers (mahalanobis distance). I’ll also briefly discuss some methods that gives us suggestions regarding the number of factors we could restrain (scree plots, parallel analysis, Minimal Average Potential). We end with a small demonstration using the fa() function from the psych package. 5.1 Cronbach’s alpha Starting off with one of the most popular indicators of internal consistency (reliability). Over the years, this indicator was not spared of criticism click here for an example. Nevertheless, I will show you how to compute it. I will use the psych package and a free online dataset, click here. This dataset contains items that “belong” to the same presumed construct. library(pacman) pacman::p_load(psych, dplyr, haven) mydata = read_sav(&quot;data_files/pone.0199750.s001.sav&quot;) alpha(mydata) #&gt; #&gt; Reliability analysis #&gt; Call: alpha(x = mydata) #&gt; #&gt; raw_alpha std.alpha G6(smc) average_r S/N ase mean sd #&gt; 0.28 0.88 0.89 0.24 7.4 0.022 1.8 0.79 #&gt; median_r #&gt; 0.26 #&gt; #&gt; 95% confidence boundaries #&gt; lower alpha upper #&gt; Feldt 0.21 0.28 0.34 #&gt; Duhachek 0.23 0.28 0.32 #&gt; #&gt; Reliability if an item is dropped: #&gt; raw_alpha std.alpha G6(smc) average_r #&gt; age 0.89 0.89 0.89 0.25 #&gt; sex 0.27 0.88 0.89 0.25 #&gt; BDI1 0.26 0.87 0.88 0.23 #&gt; BDI2 0.26 0.87 0.89 0.23 #&gt; BDI3 0.26 0.87 0.88 0.23 #&gt; BDI4 0.25 0.87 0.88 0.23 #&gt; BDI5 0.27 0.88 0.89 0.24 #&gt; BDI6 0.27 0.88 0.89 0.24 #&gt; BDI7 0.26 0.87 0.88 0.23 #&gt; BDI8 0.26 0.87 0.89 0.23 #&gt; BDI9 0.27 0.88 0.89 0.24 #&gt; BDI10 0.25 0.87 0.88 0.23 #&gt; BDI11 0.26 0.88 0.89 0.23 #&gt; BDI12 0.26 0.87 0.89 0.23 #&gt; BDI13 0.26 0.87 0.89 0.23 #&gt; BDI14 0.26 0.88 0.89 0.23 #&gt; BDI15 0.25 0.87 0.88 0.23 #&gt; BDI16 0.27 0.88 0.89 0.24 #&gt; BDI17 0.26 0.87 0.89 0.23 #&gt; BDI18 0.26 0.88 0.89 0.24 #&gt; BDI19 0.26 0.87 0.89 0.23 #&gt; BDI20 0.25 0.87 0.88 0.23 #&gt; BDI21 0.25 0.88 0.89 0.24 #&gt; clinicalandgeneral 0.27 0.88 0.89 0.25 #&gt; S/N alpha se var.r med.r #&gt; age 7.8 0.0049 0.0091 0.26 #&gt; sex 7.6 0.0215 0.0111 0.26 #&gt; BDI1 6.8 0.0214 0.0121 0.25 #&gt; BDI2 7.0 0.0215 0.0126 0.25 #&gt; BDI3 6.9 0.0213 0.0122 0.25 #&gt; BDI4 6.9 0.0216 0.0125 0.25 #&gt; BDI5 7.1 0.0212 0.0125 0.26 #&gt; BDI6 7.1 0.0212 0.0125 0.26 #&gt; BDI7 6.8 0.0212 0.0116 0.25 #&gt; BDI8 7.0 0.0211 0.0124 0.25 #&gt; BDI9 7.2 0.0215 0.0128 0.26 #&gt; BDI10 6.9 0.0215 0.0128 0.25 #&gt; BDI11 7.1 0.0214 0.0131 0.26 #&gt; BDI12 7.0 0.0212 0.0124 0.25 #&gt; BDI13 7.0 0.0212 0.0126 0.25 #&gt; BDI14 7.0 0.0215 0.0126 0.25 #&gt; BDI15 6.9 0.0218 0.0125 0.25 #&gt; BDI16 7.3 0.0213 0.0126 0.26 #&gt; BDI17 7.0 0.0214 0.0129 0.25 #&gt; BDI18 7.2 0.0213 0.0131 0.26 #&gt; BDI19 7.0 0.0211 0.0127 0.25 #&gt; BDI20 7.0 0.0218 0.0126 0.25 #&gt; BDI21 7.1 0.0221 0.0131 0.26 #&gt; clinicalandgeneral 7.5 0.0216 0.0118 0.26 #&gt; #&gt; Item statistics #&gt; n raw.r std.r r.cor r.drop mean #&gt; age 1038 0.884 0.17 0.11 0.097 31.44 #&gt; sex 1040 0.102 0.24 0.18 0.073 0.55 #&gt; BDI1 1040 0.352 0.65 0.64 0.319 0.35 #&gt; BDI2 1040 0.319 0.59 0.56 0.288 0.30 #&gt; BDI3 1040 0.266 0.59 0.57 0.236 0.29 #&gt; BDI4 1040 0.409 0.62 0.60 0.376 0.47 #&gt; BDI5 1040 0.205 0.53 0.50 0.170 0.47 #&gt; BDI6 1040 0.220 0.51 0.48 0.181 0.33 #&gt; BDI7 1040 0.295 0.67 0.66 0.259 0.39 #&gt; BDI8 1040 0.246 0.57 0.55 0.205 0.66 #&gt; BDI9 1040 0.182 0.45 0.41 0.159 0.13 #&gt; BDI10 1040 0.381 0.60 0.58 0.343 0.50 #&gt; BDI11 1040 0.307 0.53 0.50 0.266 0.56 #&gt; BDI12 1040 0.281 0.59 0.56 0.247 0.59 #&gt; BDI13 1040 0.282 0.57 0.55 0.244 0.49 #&gt; BDI14 1040 0.320 0.55 0.53 0.288 0.28 #&gt; BDI15 1040 0.462 0.59 0.57 0.429 0.68 #&gt; BDI16 1040 0.192 0.42 0.38 0.146 0.92 #&gt; BDI17 1040 0.298 0.56 0.53 0.261 0.44 #&gt; BDI18 1040 0.257 0.47 0.43 0.210 0.79 #&gt; BDI19 1040 0.254 0.57 0.55 0.209 0.71 #&gt; BDI20 1040 0.458 0.58 0.56 0.426 0.69 #&gt; BDI21 1040 0.497 0.50 0.47 0.466 0.38 #&gt; clinicalandgeneral 1040 0.096 0.30 0.25 0.081 1.08 #&gt; sd #&gt; age 15.81 #&gt; sex 0.50 #&gt; BDI1 0.69 #&gt; BDI2 0.64 #&gt; BDI3 0.60 #&gt; BDI4 0.73 #&gt; BDI5 0.64 #&gt; BDI6 0.74 #&gt; BDI7 0.71 #&gt; BDI8 0.78 #&gt; BDI9 0.43 #&gt; BDI10 0.90 #&gt; BDI11 0.81 #&gt; BDI12 0.74 #&gt; BDI13 0.82 #&gt; BDI14 0.66 #&gt; BDI15 0.74 #&gt; BDI16 0.82 #&gt; BDI17 0.72 #&gt; BDI18 0.89 #&gt; BDI19 0.85 #&gt; BDI20 0.76 #&gt; BDI21 0.78 #&gt; clinicalandgeneral 0.28 #&gt; #&gt; Non missing response frequency for each item #&gt; 0 1 2 3 miss #&gt; sex 0.45 0.55 0.00 0.00 0 #&gt; BDI1 0.76 0.15 0.07 0.02 0 #&gt; BDI2 0.78 0.17 0.02 0.03 0 #&gt; BDI3 0.78 0.18 0.03 0.02 0 #&gt; BDI4 0.65 0.27 0.06 0.03 0 #&gt; BDI5 0.59 0.37 0.02 0.02 0 #&gt; BDI6 0.79 0.14 0.02 0.05 0 #&gt; BDI7 0.72 0.19 0.07 0.02 0 #&gt; BDI8 0.49 0.40 0.07 0.04 0 #&gt; BDI9 0.89 0.09 0.01 0.01 0 #&gt; BDI10 0.72 0.12 0.10 0.06 0 #&gt; BDI11 0.60 0.30 0.05 0.05 0 #&gt; BDI12 0.53 0.37 0.07 0.03 0 #&gt; BDI13 0.68 0.21 0.07 0.05 0 #&gt; BDI14 0.82 0.10 0.06 0.02 0 #&gt; BDI15 0.46 0.43 0.09 0.03 0 #&gt; BDI16 0.34 0.43 0.19 0.04 0 #&gt; BDI17 0.67 0.25 0.05 0.03 0 #&gt; BDI18 0.46 0.36 0.12 0.06 0 #&gt; BDI19 0.52 0.30 0.15 0.04 0 #&gt; BDI20 0.47 0.41 0.09 0.03 0 #&gt; BDI21 0.76 0.15 0.04 0.05 0 #&gt; clinicalandgeneral 0.00 0.92 0.08 0.00 0 Note that you receive a lot of output. The (raw) alpha is printed at the top of the output. You could also skip most of it and ask for what you want. For example: alpha(mydata)[[&quot;total&quot;]][[&quot;raw_alpha&quot;]] #&gt; [1] 0.2751774 alpha(mydata)[[&quot;total&quot;]][[&quot;std.alpha&quot;]] #&gt; [1] 0.8808157 The above looks a bit complicated with all those brackets. here is a trick to it. Suppose I ran the following code: alpha_output = alpha(mydata) Now I click on the freshly created alpha_output object (Environment window, top right). It looks something like this: Click on total, then click on raw_alpha. On the bottom you should see: “alpha_output[[”total”]][[”raw_alpha”]]”. Nnow you can copy-paste the [[“total”]][[“raw_alpha”]] part into R. “This store as an object and click” strategy works in variety of contexts such as with regression models. 5.1.1 Alpha by hand If desired we could also compute the Cronbach’s alpha ourselves without fancy packages. Just for fun and educational purposes, I will demonstrate how. # Formula Cronbach&#39;s alpha: (N)/(N-1) * ( (VARIANCE SUM SCORE ACROSS ITEMS - VARIANCE SUM SCORE PER ITEM)/VARIANCE SUM SCORE ACROSS ITEMS ) # ingredients: N = ncol(mydata) # N variance_sum_across_items = var( rowSums(mydata) ) # VARIANCE SUM SCORE ACROSS ITEMS variance_sum_per_item = sum( diag(var(mydata, na.rm = TRUE)) ) # VARIANCE SUM SCORE PER ITEM # Put the above in the formula: (N)/(N-1) * ( (variance_sum_across_items - variance_sum_per_item)/variance_sum_across_items ) #&gt; [1] NA 5.2 Omega As I told you earlier, there is a bit of commotion surrounding whether or not it is appropriate to use Cronbach’s alpha. It has been put forward that alpha holds several strong assumptions, some that likely do not apply to the majority of data. A classic example, Cronbach’s alpha assumes (essential) tau equivalence, meaning that all items contribute equally to the construct being measured (i.e., similar factor loadings, a similar “weight” so to speak).. What can we use instead? One popular alternative, McDonald’s omega, relaxes the assumption of (essential) tau equivalence. A variety of packages allow to compute this indicator, even the psych package. For simplicity, I will use the lavaan and semtools packages to use the reliability() function which can compute both Cronbach’s alpha and McDonald’s omega. To do so, we will have to walk into Confirmatory Factor Analysis territory. First things first, the psych package may interfere with certain functions from semTools so I will unload it. Yes, packages in R may sometimes interfere with one another. If this occurs, you typically receive a message in the Console (bottom left) telling you that some function is masked from a given package. detach(&quot;package:psych&quot;, unload = TRUE) To use the reliability() function from semTools, we have to “define” a model, tell R to run a CFA using the “defined” model. When “defining” a model for a CFA or structural equation model, we need to tell R which variables we want to include, what variables are so called manifest or latent (see later parts), how variables relate to one another (regressions), and so on. In context of CFA, I will need to tell that there is a (latent) construct/factor (the thing that the questionnaire is presumed to tap into) and that all my items, the (manifest) observed item scores/factors, reflect/load on this latent construct/factor. To define our model: p_load(lavaan, semTools) mymodel = &#39; # Here I defined a latent factor (which I arbitrarily named &quot;DEP&quot;) and told R that the following manifest factors (here the variables from my dataset) load on this latent factor. # The names of the manifest factors must match those in your dataset ! # the &quot;=~&quot; sign denotes a &quot;latent construct reflected by manifest factors relation&quot; DEP =~ BDI1 + BDI2 + BDI3 + BDI4 + BDI5 + BDI6 + BDI7 + BDI8 + BDI9 + BDI10 + BDI11 + BDI12 + BDI13 + BDI14 + BDI15 + BDI16 + BDI17 + BDI18 + BDI19 + BDI20 + BDI21 &#39; Now I will have to enter this model in a CFA (using the lavaan package). In the code below I specify certain options. I enable standardized output (discussed later on), use listwise deletion of empty values, and use the Maximum Likelihood with Robust Estimations method. If all goes well, we can compute the omega by putting this CFA object in the reliability() function. Importantly, for now I will temporarily ignore the output of my CFA object (i.e., it’s summary). However, I highly recommend to always look at the output first (as I do later on) mycfa=cfa(mymodel, data = mydata, std.lv=TRUE, missing = &quot;direct&quot;, estimator = &quot;MLR&quot;) reliability(mycfa) #&gt; DEP #&gt; alpha 0.8889281 #&gt; omega 0.8899104 #&gt; omega2 0.8899104 #&gt; omega3 0.8872431 #&gt; avevar 0.2844963 Glance over the output of reliability(), you see alpha, omega, omega2, omega3, and avevar. Avevar is the average variance extracted, not a reliability measure but an indication of how much variance is attributable to the common factor. Notice that we have three omega’s. In short, The first one (omega) “controls” for other (latent) factors, the second one (omega2) does not, the third one (omega3) has a denuminator that equals the sum of all elements in the variance-covariance matrix of item scores. Omega and omega2 likely differ when you define more than one latent factor and there is multi-dimensionality in the manifest factors/items(e.g., items loading notably on more latent factors). You can also request a spread (e.g., confidence intervals) surrounding our omega using the MBESS package. In the example below I will use a bootstrap of 5 (purely for demonstration purposes) but in practice consider to go for a higher number (e.g., 500 or more). library(MBESS) ci.reliability(data = mydata, type = &quot;omega&quot;, interval.type = &quot;perc&quot;, B=5, conf.level = 0.95) #&gt; $est #&gt; [1] 0.2725901 #&gt; #&gt; $se #&gt; [1] 0.0191108 #&gt; #&gt; $ci.lower #&gt; [1] 0.267691 #&gt; #&gt; $ci.upper #&gt; [1] 0.3061892 #&gt; #&gt; $conf.level #&gt; [1] 0.95 #&gt; #&gt; $type #&gt; [1] &quot;omega&quot; #&gt; #&gt; $interval.type #&gt; [1] &quot;percentile bootstrap&quot; # applying a bootstrap of 5 bootstrap runs (consider 500+) and a percental CI type It is very important to note that we used a fairly simple model with continuous variables and only one latent construct (i.e., a unidimensional model). Other types of models exist such as those including categorical variables, multiple factors, and hierarchical factor models. Covering all of these would be exhaustive for the purposis of this guide. However, I will direct to the work Flora, 2020 who provides a detailed background and examples in R. The reference to this article is provided at the end of this part. 5.2.1 Fit indices and how to potentially improve model fit As I said before, I ignored the output from my CFA object, but you really should not. For starters, we should check the model fit. The model fit refers to how closely your data matches the model you defined (the relations of the model). important to realize, a “good fitting model” does not prove that your defined model is a “good”, “realistic” or proven model. To see how well your model fits your data we start by requesting model fit indicators. Sometimes you may want to improve these fit indicators. After all, they could affect the value of your omega. While we’re at it we can also inspect the factor loadings. This way we can also check cronbach’s alpha assumption of (essential) tau equivalence. summary(mycfa, fit.measures=TRUE, standardized=TRUE) #&gt; lavaan 0.6.16 ended normally after 27 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 63 #&gt; #&gt; Number of observations 1040 #&gt; Number of missing patterns 1 #&gt; #&gt; Model Test User Model: #&gt; Standard Scaled #&gt; Test Statistic 969.539 590.608 #&gt; Degrees of freedom 189 189 #&gt; P-value (Chi-square) 0.000 0.000 #&gt; Scaling correction factor 1.642 #&gt; Yuan-Bentler correction (Mplus variant) #&gt; #&gt; Model Test Baseline Model: #&gt; #&gt; Test statistic 6110.882 3716.183 #&gt; Degrees of freedom 210 210 #&gt; P-value 0.000 0.000 #&gt; Scaling correction factor 1.644 #&gt; #&gt; User Model versus Baseline Model: #&gt; #&gt; Comparative Fit Index (CFI) 0.868 0.885 #&gt; Tucker-Lewis Index (TLI) 0.853 0.873 #&gt; #&gt; Robust Comparative Fit Index (CFI) 0.886 #&gt; Robust Tucker-Lewis Index (TLI) 0.873 #&gt; #&gt; Loglikelihood and Information Criteria: #&gt; #&gt; Loglikelihood user model (H0) -21461.330 -21461.330 #&gt; Scaling correction factor 1.772 #&gt; for the MLR correction #&gt; Loglikelihood unrestricted model (H1) NA NA #&gt; Scaling correction factor 1.674 #&gt; for the MLR correction #&gt; #&gt; Akaike (AIC) 43048.660 43048.660 #&gt; Bayesian (BIC) 43360.319 43360.319 #&gt; Sample-size adjusted Bayesian (SABIC) 43160.223 43160.223 #&gt; #&gt; Root Mean Square Error of Approximation: #&gt; #&gt; RMSEA 0.063 0.045 #&gt; 90 Percent confidence interval - lower 0.059 0.042 #&gt; 90 Percent confidence interval - upper 0.067 0.048 #&gt; P-value H_0: RMSEA &lt;= 0.050 0.000 0.993 #&gt; P-value H_0: RMSEA &gt;= 0.080 0.000 0.000 #&gt; #&gt; Robust RMSEA 0.058 #&gt; 90 Percent confidence interval - lower 0.053 #&gt; 90 Percent confidence interval - upper 0.063 #&gt; P-value H_0: Robust RMSEA &lt;= 0.050 0.007 #&gt; P-value H_0: Robust RMSEA &gt;= 0.080 0.000 #&gt; #&gt; Standardized Root Mean Square Residual: #&gt; #&gt; SRMR 0.046 0.046 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Sandwich #&gt; Information bread Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; DEP =~ #&gt; BDI1 0.438 0.028 15.417 0.000 #&gt; BDI2 0.356 0.031 11.445 0.000 #&gt; BDI3 0.353 0.030 11.726 0.000 #&gt; BDI4 0.433 0.028 15.285 0.000 #&gt; BDI5 0.327 0.029 11.381 0.000 #&gt; BDI6 0.372 0.034 10.901 0.000 #&gt; BDI7 0.479 0.028 17.166 0.000 #&gt; BDI8 0.445 0.031 14.474 0.000 #&gt; BDI9 0.175 0.022 7.993 0.000 #&gt; BDI10 0.505 0.033 15.353 0.000 #&gt; BDI11 0.397 0.033 11.984 0.000 #&gt; BDI12 0.429 0.027 16.104 0.000 #&gt; BDI13 0.459 0.033 13.925 0.000 #&gt; BDI14 0.361 0.029 12.354 0.000 #&gt; BDI15 0.404 0.029 14.049 0.000 #&gt; BDI16 0.307 0.028 11.115 0.000 #&gt; BDI17 0.379 0.029 13.112 0.000 #&gt; BDI18 0.370 0.032 11.611 0.000 #&gt; BDI19 0.467 0.031 15.232 0.000 #&gt; BDI20 0.402 0.029 13.925 0.000 #&gt; BDI21 0.333 0.036 9.268 0.000 #&gt; Std.lv Std.all #&gt; #&gt; 0.438 0.633 #&gt; 0.356 0.559 #&gt; 0.353 0.588 #&gt; 0.433 0.597 #&gt; 0.327 0.512 #&gt; 0.372 0.500 #&gt; 0.479 0.678 #&gt; 0.445 0.568 #&gt; 0.175 0.412 #&gt; 0.505 0.562 #&gt; 0.397 0.488 #&gt; 0.429 0.582 #&gt; 0.459 0.557 #&gt; 0.361 0.544 #&gt; 0.404 0.549 #&gt; 0.307 0.374 #&gt; 0.379 0.526 #&gt; 0.370 0.418 #&gt; 0.467 0.547 #&gt; 0.402 0.526 #&gt; 0.333 0.427 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .BDI1 0.346 0.021 16.142 0.000 #&gt; .BDI2 0.295 0.020 14.938 0.000 #&gt; .BDI3 0.287 0.019 15.397 0.000 #&gt; .BDI4 0.467 0.022 20.782 0.000 #&gt; .BDI5 0.475 0.020 23.953 0.000 #&gt; .BDI6 0.329 0.023 14.281 0.000 #&gt; .BDI7 0.391 0.022 17.853 0.000 #&gt; .BDI8 0.663 0.024 27.270 0.000 #&gt; .BDI9 0.134 0.013 10.126 0.000 #&gt; .BDI10 0.496 0.028 17.804 0.000 #&gt; .BDI11 0.560 0.025 22.184 0.000 #&gt; .BDI12 0.591 0.023 25.866 0.000 #&gt; .BDI13 0.487 0.026 19.083 0.000 #&gt; .BDI14 0.278 0.021 13.494 0.000 #&gt; .BDI15 0.676 0.023 29.607 0.000 #&gt; .BDI16 0.921 0.025 36.169 0.000 #&gt; .BDI17 0.436 0.022 19.500 0.000 #&gt; .BDI18 0.788 0.027 28.680 0.000 #&gt; .BDI19 0.708 0.026 26.713 0.000 #&gt; .BDI20 0.688 0.024 29.058 0.000 #&gt; .BDI21 0.378 0.024 15.651 0.000 #&gt; DEP 0.000 #&gt; Std.lv Std.all #&gt; 0.346 0.501 #&gt; 0.295 0.463 #&gt; 0.287 0.477 #&gt; 0.467 0.644 #&gt; 0.475 0.743 #&gt; 0.329 0.443 #&gt; 0.391 0.554 #&gt; 0.663 0.846 #&gt; 0.134 0.314 #&gt; 0.496 0.552 #&gt; 0.560 0.688 #&gt; 0.591 0.802 #&gt; 0.487 0.592 #&gt; 0.278 0.418 #&gt; 0.676 0.918 #&gt; 0.921 1.122 #&gt; 0.436 0.605 #&gt; 0.788 0.889 #&gt; 0.708 0.828 #&gt; 0.688 0.901 #&gt; 0.378 0.485 #&gt; 0.000 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .BDI1 0.287 0.021 13.687 0.000 #&gt; .BDI2 0.279 0.023 11.888 0.000 #&gt; .BDI3 0.236 0.018 13.273 0.000 #&gt; .BDI4 0.339 0.024 13.925 0.000 #&gt; .BDI5 0.302 0.017 17.306 0.000 #&gt; .BDI6 0.413 0.037 11.232 0.000 #&gt; .BDI7 0.270 0.019 13.971 0.000 #&gt; .BDI8 0.417 0.023 18.342 0.000 #&gt; .BDI9 0.150 0.019 7.732 0.000 #&gt; .BDI10 0.553 0.036 15.323 0.000 #&gt; .BDI11 0.504 0.034 14.943 0.000 #&gt; .BDI12 0.359 0.023 15.813 0.000 #&gt; .BDI13 0.468 0.032 14.807 0.000 #&gt; .BDI14 0.311 0.028 10.907 0.000 #&gt; .BDI15 0.379 0.021 17.842 0.000 #&gt; .BDI16 0.580 0.025 23.143 0.000 #&gt; .BDI17 0.375 0.028 13.486 0.000 #&gt; .BDI18 0.649 0.031 20.872 0.000 #&gt; .BDI19 0.512 0.024 21.019 0.000 #&gt; .BDI20 0.421 0.024 17.796 0.000 #&gt; .BDI21 0.496 0.037 13.346 0.000 #&gt; DEP 1.000 #&gt; Std.lv Std.all #&gt; 0.287 0.599 #&gt; 0.279 0.687 #&gt; 0.236 0.654 #&gt; 0.339 0.644 #&gt; 0.302 0.738 #&gt; 0.413 0.750 #&gt; 0.270 0.541 #&gt; 0.417 0.678 #&gt; 0.150 0.831 #&gt; 0.553 0.685 #&gt; 0.504 0.762 #&gt; 0.359 0.661 #&gt; 0.468 0.690 #&gt; 0.311 0.704 #&gt; 0.379 0.699 #&gt; 0.580 0.860 #&gt; 0.375 0.723 #&gt; 0.649 0.826 #&gt; 0.512 0.701 #&gt; 0.421 0.723 #&gt; 0.496 0.818 #&gt; 1.000 1.000 # fit.measures will show the model fit indicators such as RMSSEA and CFI Alright, a lot of output. Recall that output interpretation is beyond the scope of this guide but I note two things: 1. you could argue that there is room for improvement based on multiple fit indicators. Some would argue that the CFI and TLI should preferably be above .90, perhaps even above .950. Further, The RMSEA looks “ok” but some would prefer this value below 0.05. Of course there is not really a real cut-off, only conventions. 2. The factor loadings (see “Latent Variables” in the output) are not equal. This can be taken as a violation of the tau equivalence assumption, so Cronbach’s alpha may not be that appropriate to report. Next to the above two points, notice that the current values of alpha and omega are very similar. However, sometimes it could be that notable differences emerge upon improving your model fit. How do we do that? Well a straightforward way is to can inspect the residual correlations in the CFA object and “adjust/account” for (manifest) variables show a… “notable” correlation. High residual correlations may indicate to some items sare unique variance, that is, variance that is not explained by the latent construct (extra variance beyond the latent construct so to speak). Let’s have a look at the residual correlations and note correlations of at least say .075 (arbitrary example). options(scipen=999) # Disables the scientific notation residuals(mycfa, type = &quot;cor&quot;)$cor # For visual inspection of #&gt; NULL # TIP, since I have a lot of correlations visual inspection of correlations &gt;.075 becomes cumbersome # Below a code that will prompt a window showing the variables with correlations above .075 View( data.frame( as.table(residuals(mycfa, type = &quot;cor&quot;)$cov) ) %&gt;% filter(Freq&gt;0.075) %&gt;% # with Freq being the correlations. Unfortunately this will output duplicates so I will filter them out mutate(is_duplicated = ifelse(duplicated(Freq)==TRUE , &quot;yes&quot;,&quot;no&quot; )) %&gt;% # i.e., yes to duplicate correlations filter(!is_duplicated ==&quot;yes&quot;) # get rid of duplicates ) My code tells me yhat there are 13 (unique) correlations above .075. Remember the step where we defined a model for the CFA (I called this “mymodel”)? Within this model definition, I will also add that the correlations between the 13 (manifest) factors are correlated. Then I’ll check my CFA model output again. mymodel = &#39; DEP =~ BDI1 + BDI2 + BDI3 + BDI4 + BDI5 + BDI6 + BDI7 + BDI8 + BDI9 + BDI10 + BDI11 + BDI12 + BDI13 + BDI14 + BDI15 + BDI16 + BDI17 + BDI18 + BDI19 + BDI20 + BDI21 # the ~~ stands for correlation. So I tell R to consider their correlations these (and to not set them to 0 by default) BDI10 ~~ BDI1 BDI4 ~~ BDI2 BDI5 ~~ BDI3 BDI7 ~~ BDI3 BDI9 ~~ BDI3 BDI6 ~~ BDI5 BDI8 ~~ BDI5 BDI17 ~~ BDI11 BDI19 ~~ BDI3 BDI16 ~~ BDI15 BDI20 ~~ BDI15 BDI18 ~~ BDI16 BDI20 ~~ BDI16 &#39; mycfa=cfa(mymodel, data = mydata, std.lv=TRUE, missing = &quot;direct&quot;, estimator = &quot;MLR&quot;) summary(mycfa, fit.measures=TRUE, standardized=TRUE) #&gt; lavaan 0.6.16 ended normally after 34 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 76 #&gt; #&gt; Number of observations 1040 #&gt; Number of missing patterns 1 #&gt; #&gt; Model Test User Model: #&gt; Standard Scaled #&gt; Test Statistic 551.368 340.106 #&gt; Degrees of freedom 176 176 #&gt; P-value (Chi-square) 0.000 0.000 #&gt; Scaling correction factor 1.621 #&gt; Yuan-Bentler correction (Mplus variant) #&gt; #&gt; Model Test Baseline Model: #&gt; #&gt; Test statistic 6110.882 3716.183 #&gt; Degrees of freedom 210 210 #&gt; P-value 0.000 0.000 #&gt; Scaling correction factor 1.644 #&gt; #&gt; User Model versus Baseline Model: #&gt; #&gt; Comparative Fit Index (CFI) 0.936 0.953 #&gt; Tucker-Lewis Index (TLI) 0.924 0.944 #&gt; #&gt; Robust Comparative Fit Index (CFI) 0.954 #&gt; Robust Tucker-Lewis Index (TLI) 0.945 #&gt; #&gt; Loglikelihood and Information Criteria: #&gt; #&gt; Loglikelihood user model (H0) -21252.244 -21252.244 #&gt; Scaling correction factor 1.797 #&gt; for the MLR correction #&gt; Loglikelihood unrestricted model (H1) NA NA #&gt; Scaling correction factor 1.674 #&gt; for the MLR correction #&gt; #&gt; Akaike (AIC) 42656.489 42656.489 #&gt; Bayesian (BIC) 43032.459 43032.459 #&gt; Sample-size adjusted Bayesian (SABIC) 42791.073 42791.073 #&gt; #&gt; Root Mean Square Error of Approximation: #&gt; #&gt; RMSEA 0.045 0.030 #&gt; 90 Percent confidence interval - lower 0.041 0.026 #&gt; 90 Percent confidence interval - upper 0.050 0.034 #&gt; P-value H_0: RMSEA &lt;= 0.050 0.965 1.000 #&gt; P-value H_0: RMSEA &gt;= 0.080 0.000 0.000 #&gt; #&gt; Robust RMSEA 0.038 #&gt; 90 Percent confidence interval - lower 0.032 #&gt; 90 Percent confidence interval - upper 0.044 #&gt; P-value H_0: Robust RMSEA &lt;= 0.050 1.000 #&gt; P-value H_0: Robust RMSEA &gt;= 0.080 0.000 #&gt; #&gt; Standardized Root Mean Square Residual: #&gt; #&gt; SRMR 0.036 0.036 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Sandwich #&gt; Information bread Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; DEP =~ #&gt; BDI1 0.429 0.029 15.002 0.000 #&gt; BDI2 0.354 0.031 11.324 0.000 #&gt; BDI3 0.343 0.031 11.097 0.000 #&gt; BDI4 0.431 0.028 15.126 0.000 #&gt; BDI5 0.312 0.029 10.685 0.000 #&gt; BDI6 0.368 0.034 10.800 0.000 #&gt; BDI7 0.479 0.028 17.150 0.000 #&gt; BDI8 0.445 0.031 14.244 0.000 #&gt; BDI9 0.174 0.022 8.005 0.000 #&gt; BDI10 0.492 0.033 14.712 0.000 #&gt; BDI11 0.393 0.034 11.617 0.000 #&gt; BDI12 0.436 0.027 16.044 0.000 #&gt; BDI13 0.466 0.033 13.926 0.000 #&gt; BDI14 0.365 0.029 12.384 0.000 #&gt; BDI15 0.388 0.029 13.333 0.000 #&gt; BDI16 0.284 0.028 10.096 0.000 #&gt; BDI17 0.373 0.029 12.718 0.000 #&gt; BDI18 0.365 0.032 11.255 0.000 #&gt; BDI19 0.473 0.031 15.250 0.000 #&gt; BDI20 0.383 0.029 13.411 0.000 #&gt; BDI21 0.337 0.036 9.244 0.000 #&gt; Std.lv Std.all #&gt; #&gt; 0.429 0.621 #&gt; 0.354 0.556 #&gt; 0.343 0.572 #&gt; 0.431 0.594 #&gt; 0.312 0.489 #&gt; 0.368 0.496 #&gt; 0.479 0.678 #&gt; 0.445 0.568 #&gt; 0.174 0.410 #&gt; 0.492 0.547 #&gt; 0.393 0.483 #&gt; 0.436 0.592 #&gt; 0.466 0.565 #&gt; 0.365 0.550 #&gt; 0.388 0.527 #&gt; 0.284 0.347 #&gt; 0.373 0.517 #&gt; 0.365 0.411 #&gt; 0.473 0.554 #&gt; 0.383 0.502 #&gt; 0.337 0.433 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .BDI1 ~~ #&gt; .BDI10 0.096 0.020 4.901 0.000 #&gt; .BDI2 ~~ #&gt; .BDI4 0.038 0.017 2.294 0.022 #&gt; .BDI3 ~~ #&gt; .BDI5 0.037 0.012 3.041 0.002 #&gt; .BDI7 0.041 0.015 2.763 0.006 #&gt; .BDI9 0.021 0.010 2.138 0.033 #&gt; .BDI5 ~~ #&gt; .BDI6 0.059 0.016 3.719 0.000 #&gt; .BDI8 0.046 0.014 3.294 0.001 #&gt; .BDI11 ~~ #&gt; .BDI17 0.050 0.018 2.824 0.005 #&gt; .BDI3 ~~ #&gt; .BDI19 -0.008 0.016 -0.478 0.633 #&gt; .BDI15 ~~ #&gt; .BDI16 0.065 0.017 3.916 0.000 #&gt; .BDI20 0.165 0.020 8.371 0.000 #&gt; .BDI16 ~~ #&gt; .BDI18 0.100 0.022 4.631 0.000 #&gt; .BDI20 0.091 0.018 5.144 0.000 #&gt; Std.lv Std.all #&gt; #&gt; 0.096 0.235 #&gt; #&gt; 0.038 0.124 #&gt; #&gt; 0.037 0.135 #&gt; 0.041 0.160 #&gt; 0.021 0.111 #&gt; #&gt; 0.059 0.163 #&gt; 0.046 0.129 #&gt; #&gt; 0.050 0.114 #&gt; #&gt; -0.008 -0.021 #&gt; #&gt; 0.065 0.136 #&gt; 0.165 0.399 #&gt; #&gt; 0.100 0.162 #&gt; 0.091 0.180 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .BDI1 0.346 0.021 16.142 0.000 #&gt; .BDI2 0.295 0.020 14.938 0.000 #&gt; .BDI3 0.287 0.019 15.397 0.000 #&gt; .BDI4 0.467 0.022 20.782 0.000 #&gt; .BDI5 0.475 0.020 23.953 0.000 #&gt; .BDI6 0.329 0.023 14.281 0.000 #&gt; .BDI7 0.391 0.022 17.853 0.000 #&gt; .BDI8 0.663 0.024 27.270 0.000 #&gt; .BDI9 0.134 0.013 10.126 0.000 #&gt; .BDI10 0.496 0.028 17.804 0.000 #&gt; .BDI11 0.560 0.025 22.184 0.000 #&gt; .BDI12 0.591 0.023 25.866 0.000 #&gt; .BDI13 0.487 0.026 19.083 0.000 #&gt; .BDI14 0.278 0.021 13.494 0.000 #&gt; .BDI15 0.676 0.023 29.607 0.000 #&gt; .BDI16 0.921 0.025 36.169 0.000 #&gt; .BDI17 0.436 0.022 19.500 0.000 #&gt; .BDI18 0.788 0.027 28.680 0.000 #&gt; .BDI19 0.708 0.026 26.713 0.000 #&gt; .BDI20 0.688 0.024 29.058 0.000 #&gt; .BDI21 0.378 0.024 15.651 0.000 #&gt; DEP 0.000 #&gt; Std.lv Std.all #&gt; 0.346 0.501 #&gt; 0.295 0.463 #&gt; 0.287 0.478 #&gt; 0.467 0.644 #&gt; 0.475 0.744 #&gt; 0.329 0.443 #&gt; 0.391 0.554 #&gt; 0.663 0.846 #&gt; 0.134 0.314 #&gt; 0.496 0.552 #&gt; 0.560 0.688 #&gt; 0.591 0.802 #&gt; 0.487 0.592 #&gt; 0.278 0.418 #&gt; 0.676 0.918 #&gt; 0.921 1.123 #&gt; 0.436 0.605 #&gt; 0.788 0.889 #&gt; 0.708 0.828 #&gt; 0.688 0.901 #&gt; 0.378 0.485 #&gt; 0.000 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .BDI1 0.294 0.022 13.545 0.000 #&gt; .BDI2 0.281 0.024 11.752 0.000 #&gt; .BDI3 0.242 0.019 13.038 0.000 #&gt; .BDI4 0.340 0.025 13.644 0.000 #&gt; .BDI5 0.310 0.018 17.449 0.000 #&gt; .BDI6 0.416 0.037 11.200 0.000 #&gt; .BDI7 0.270 0.020 13.588 0.000 #&gt; .BDI8 0.417 0.023 18.242 0.000 #&gt; .BDI9 0.151 0.019 7.734 0.000 #&gt; .BDI10 0.566 0.037 15.313 0.000 #&gt; .BDI11 0.507 0.034 14.969 0.000 #&gt; .BDI12 0.353 0.023 15.557 0.000 #&gt; .BDI13 0.462 0.032 14.528 0.000 #&gt; .BDI14 0.308 0.028 10.829 0.000 #&gt; .BDI15 0.392 0.022 18.068 0.000 #&gt; .BDI16 0.592 0.025 23.500 0.000 #&gt; .BDI17 0.380 0.028 13.398 0.000 #&gt; .BDI18 0.653 0.031 20.910 0.000 #&gt; .BDI19 0.506 0.025 20.483 0.000 #&gt; .BDI20 0.436 0.024 18.173 0.000 #&gt; .BDI21 0.492 0.037 13.289 0.000 #&gt; DEP 1.000 #&gt; Std.lv Std.all #&gt; 0.294 0.615 #&gt; 0.281 0.691 #&gt; 0.242 0.673 #&gt; 0.340 0.647 #&gt; 0.310 0.761 #&gt; 0.416 0.754 #&gt; 0.270 0.540 #&gt; 0.417 0.678 #&gt; 0.151 0.832 #&gt; 0.566 0.700 #&gt; 0.507 0.766 #&gt; 0.353 0.650 #&gt; 0.462 0.680 #&gt; 0.308 0.698 #&gt; 0.392 0.722 #&gt; 0.592 0.880 #&gt; 0.380 0.732 #&gt; 0.653 0.831 #&gt; 0.506 0.693 #&gt; 0.436 0.748 #&gt; 0.492 0.812 #&gt; 1.000 1.000 Ok, the model fit indicators did improve a bit. Did the omega change? reliability(mycfa) #&gt; DEP #&gt; alpha 0.8889281 #&gt; omega 0.8677875 #&gt; omega2 0.8677875 #&gt; omega3 0.8657039 #&gt; avevar 0.2784132 There is bigger difference between alpha and omega but it is not that… “notable”. Of course, what is small or large (there is a reason you often see me put “” around certain words)? From my own experience, the omega sometimes changes “a little”, sometimes more “notably”. Various factors likely play a role in this, e.g., in how far Cronbach’s alpha held assumptions are violated. 5.3 Exploratory factor analysis Until now, we assumed that we knew the number of factors and constructs that could explain our data. This is especially clear in CFA where we define our models and say how many (latent) factors there are and what loads on them (but again, a model that fit does not mean a confirmed true model). Still, what if you don’t know? You receive a couple of questionnaire items and, for whatever reason, you want to try to identify what variables/factors could explain patterns of (co)relations between your variables. For this purpose we could conduct an exploratory factor analysis (EFA). In the following parts I will use the PoliticalDemocracy dataset which is a built-in dataset from the lavaan package. Note that I renamed the last variables. mydata = PoliticalDemocracy names(mydata)[9:ncol(mydata)] = c(&quot;y9&quot;, &quot;y10&quot;, &quot;y11&quot;) Alright, now before you plan to conduct an EFA, always check your data. Several points need to be considered before conducting an EFA. For example, the sample size (sometimes debated), missing data, multicollinearity, skewness and kurtosis (whether our variables resemble a univariate/multivariate normal distribution), (univariate/multivariate) outliers, and so on. Here I want to quickly inspect the last three points: multicollinearity, the distribution of the data, and outliers. These may help us determine what methods to consider when conducting our EFA later on. 5.3.1 EFA check: multicollinearity How would you inspect multicollinearity between your variables? Did you spontaneously think about a correlation matrix? Well, you might miss out on multicollinearity as correlation matrices might miss complex interactions among multiple variables. Correlations could be “low” but that does not necessarily imply the absence of multicollinearity. As an extra indicator, we can use the variance inflation factor (VIF) using the vif() function from the car package. In short, the VIF indicates whether the variance of a regression coefficient is inflated due to inter-correlations between predictors in a regression model. A VIF around 1 suggests “no” multicollinearity, a VIF between 1-5 suggests “moderate” multicollinearity. However, a VIF of 5 and above as suggests “high” multicollinearity. For example, a VIF of 5 suggests that the variance of that predictor is 5 times the value it would be without multicollinearity. I admit that VIF is more typically used in regression analysis rather than factor analysis. Still it can provide useful if you use it before you conduct your EFA. Spoilers for the upcoming part about linear regression but to compute the VIF, I will fit a linear model in which one variable of my dataset is regressed on all remaining ones. To regress one variable on all others, I will create a text that shows the formula for the linear regression model. That text object is then restyled to not be read as a text but as a formula so it can be used in the lm() function to fit a simple linear model. mylm = lm( as.formula( # A text that will look like y1 ~ y1 + y2 + ... y11. This text will be &quot;restyled&quot; as a formula (basicially dropping the &quot;&quot; signs) paste( colnames(mydata)[1], &quot;~&quot;, paste(colnames(mydata)[2:(ncol(mydata))], collapse =&quot;+&quot;), sep = &quot;&quot; ) ) , data = mydata) Now we can put our linear model in the vif() function, as simple as that. library(car) #&gt; Loading required package: carData #&gt; #&gt; Attaching package: &#39;car&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; recode vif(mylm) #&gt; y2 y3 y4 y5 y6 y7 #&gt; 2.875379 2.005594 3.669274 2.669889 3.070501 2.984355 #&gt; y8 y9 y10 y11 #&gt; 3.619843 6.045630 6.930182 3.907849 5.3.2 EFA check: univariate/multivariate normal distribution Skewness and kurtosis affect correlations (especially Pearson correlations), and therefore EFA. If you detect “notable” skewness and/or kurtosis, you might want to consider looking into e.g., Spearman or polychoric correlations instead of Pearson ones. Important to know, both the univariate distribution of your individual variables and the multivariate distribution of your data, can play a role. Think of a multivariate distribution as multidimensional overview of combinations (cross-points of the values of multiple variables). Univariate distributions can be plotted on a (2D) histogram that only has one axis, multivariate distributions have multiple axes (one per variable) so it looks multidimensional. Here an example of a bivariate normal distribution from Wikipedia Univariate distributions can be inspected by histograms or by calculation of the skew and kurtosis. Multivariate distributions - and whether they resemble a “normal gaussian distribution” - can be checked with the mardia() function from the psych package. library(psych) #&gt; #&gt; Attaching package: &#39;psych&#39; #&gt; The following object is masked from &#39;package:car&#39;: #&gt; #&gt; logit #&gt; The following object is masked from &#39;package:MBESS&#39;: #&gt; #&gt; cor2cov #&gt; The following objects are masked from &#39;package:semTools&#39;: #&gt; #&gt; reliability, skew #&gt; The following object is masked from &#39;package:lavaan&#39;: #&gt; #&gt; cor2cov mardia(mydata) #&gt; Call: mardia(x = mydata) #&gt; #&gt; Mardia tests of multivariate skew and kurtosis #&gt; Use describe(x) the to get univariate tests #&gt; n.obs = 75 num.vars = 11 #&gt; b1p = 26.47 skew = 330.9 with probability &lt;= 0.035 #&gt; small sample skew = 346.41 with probability &lt;= 0.0083 #&gt; b2p = 134.57 kurtosis = -2.16 with probability &lt;= 0.031 The output shows the multivariate kurtosis, skewness, and corresponding p values. If you want to follow convention, then p values above .05 on kurtosis and/or skewness are deemed to suggest a multivariate normal distribution. 5.3.3 EFA check: univariate/multivariate outliers Correlations are sensitive to “notable” data values that are much “larger” or “smaller” in value, as compared to others. Consequently, EFA can be affected by outliers as well. You can spot univariate outliers (i.e., “extreme values” in one variable) using e.g., box plots. Then you have multivariate outliers which are “extremities” in the combinations of two or more variables. A simple example, suppose you measure height and weight in a human adult population. Some people are 1.98 meters tall, some people weight 53 kilograms. Now imagine a person who is both 1.98 meters tall and (only) weights 53 kilograms. This could be considered as an “outlying” combination. Multivariate outliers can be checked by the Mahalanobis distance using the mahalanobis() function. We’ll feed this function the mean per variable and the covariance between all variables. Afterwards, we can test whether a given distance score is “statistically significantly different” compared to others. Commonly, a chi-square test with k-1 degrees of freedom (here our number of variables -1) is used to test the Mahalanobis distance, with statistically significant differences noted by p values below or equal to .001. In the example below I will put these distances in a dataframe object, add p-values, and an indicator of whether these are significant (thus “outliers”). mydistances = data.frame( distance = mahalanobis(mydata, center = colMeans(mydata), cov = cov(mydata)) ) mydistances$pvalues = pchisq(q = mydistances$distance, df = ncol(mydata)-1, lower.tail = FALSE) mydistances = mydistances %&gt;% mutate(outlier = ifelse( pvalues&lt;=0.001,&quot;outlier&quot;,&quot;not outlier&quot;)) 5.3.4 EFA: how many factors? Asking the big questions: what is the number of factors that “sufficiently” explain our data? Several indications can give use an idea including the on scree plots depicting eigenvalues and parallel analysis. 5.3.4.1 EFA: scree plots with eigenvalues Scree plots are line plots showing and connecting eigenvalues. Eigenvalues represent the amount of variance explained by each factor. You can compute the eigenvalues using the eigenComputes() function from the nFactors package. To this function I will enter my dataset and specify to use the (Pearson) correlation matrix from that dataset (the function will automatically compute it for me) instead of a covariance matrix. library(nFactors) myeigenvalues = eigenComputes(mydata, cor = TRUE) In case you deem your variables not sufficiently univariate/multivariate normal distributed, you could e.g. compute the polychoric correlations using the polychoric() function from the psych package, store these correlations in a separate variable, and enter that variable in eigenComputes(). We could also use e.g., Spearman correlations which already have a built-in option within eigenComputes(): eigenComputes(mydata, cor = TRUE, method=“spearman”). Ok, let’s create the scree plot. Commonly, the number of factors to be retained is conventionally deemed to be the number of eigenvalues above the value 1. I will be use the ggplot2 package (see also the previous part) to make the plot. # First I make a dataset containing the eigenvalues and the amount of variables (to create the x- and y-axis) library(ggplot2) #&gt; #&gt; Attaching package: &#39;ggplot2&#39; #&gt; The following objects are masked from &#39;package:psych&#39;: #&gt; #&gt; %+%, alpha data.frame( variable = 1:ncol(mydata), eigenvalues = myeigenvalues) %&gt;% # From I create the scree plot ggplot(aes(x=variable, y=eigenvalues)) + geom_point(size = 2, color=&quot;red&quot;,alpha=0.8) + geom_line(color=&quot;red&quot;, alpha=0.8) + geom_hline(aes(yintercept=1), linetype=&quot;dashed&quot;) + theme_minimal() 5.3.4.2 EFA: parallel analysis Parallel analysis simultaneously simulates the eigenvalues based on both our own observed data (like we did above) and by those based on a simulated dataset that is parallel to our own observed data. To determine the number of factors to retain, we have to look at the number of factors above those of the simulated data. We can use the fa.parallel() function from the psych package to run this function with principal components analysis, common factor analysis, or both. fa.parallel(mydata, fa = &quot;pc&quot;, cor = &quot;cor&quot;, n.iter=500) #&gt; Parallel analysis suggests that the number of factors = NA and the number of components = 2 # Pearson correlations by default but this can be modified # fa both will output both principal components and principal factor analysis 5.3.4.3 EFA: Minimum Average Partial (correlations) The last technique that I want to discuss, the Minimum Average Partial, is used to select the number of factors with the smallest mean partial correlation. Partial correlations measure the relation between variables, while adjusting for the influence of other variables. These partial correlations progress to 0 with a more optimal number of factors. We can use the VSS()function from the psych package VSS(cor(mydata)) #&gt; n.obs was not specified and was arbitrarily set to 1000. This only affects the chi square values. #&gt; #&gt; Very Simple Structure #&gt; Call: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, #&gt; n.obs = n.obs, plot = plot, title = title, use = use, cor = cor) #&gt; VSS complexity 1 achieves a maximimum of 0.89 with 1 factors #&gt; VSS complexity 2 achieves a maximimum of 0.97 with 2 factors #&gt; #&gt; The Velicer MAP achieves a minimum of 0.05 with 2 factors #&gt; BIC achieves a minimum of 38.92 with 6 factors #&gt; Sample Size adjusted BIC achieves a minimum of 51.63 with 6 factors #&gt; #&gt; Statistics by number of factors #&gt; vss1 vss2 map dof chisq #&gt; 1 0.89 0.00 0.096 44 3431.3938136 #&gt; 2 0.81 0.97 0.046 34 831.5327108 #&gt; 3 0.51 0.89 0.054 25 434.9763651 #&gt; 4 0.49 0.80 0.076 17 259.3664968 #&gt; 5 0.47 0.75 0.100 10 135.9475999 #&gt; 6 0.40 0.71 0.145 4 66.5522974 #&gt; 7 0.34 0.55 0.216 -1 13.9568471 #&gt; 8 0.44 0.70 0.345 -5 0.0000057 #&gt; prob #&gt; 1 0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 #&gt; 2 0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000011 #&gt; 3 0.000000000000000000000000000000000000000000000000000000000000000000000000000205989781877151495567863925817420067687635309994220733642578125000000000000000 #&gt; 4 0.000000000000000000000000000000000000000000002537310578025230469682155254318445258832070976495742797851562500000000000000000000000000000000000000000000000 #&gt; 5 0.000000000000000000000002847326567079168220270740663480069088109303265810012817382812500000000000000000000000000000000000000000000000000000000000000000000 #&gt; 6 0.000000000000121155814533981051996439082252265961869852617383003234863281250000000000000000000000000000000000000000000000000000000000000000000000000000000 #&gt; 7 NA #&gt; 8 NA #&gt; sqresid fit RMSEA BIC SABIC complex eChisq #&gt; 1 4.93 0.89 0.28 3127 3267 1.0 2375.35970442 #&gt; 2 1.33 0.97 0.15 597 705 1.2 199.31215851 #&gt; 3 0.83 0.98 0.13 262 342 1.7 55.44789599 #&gt; 4 0.67 0.99 0.12 142 196 1.8 31.78593735 #&gt; 5 0.53 0.99 0.11 67 99 2.1 11.62641311 #&gt; 6 0.46 0.99 0.13 39 52 2.1 3.90783385 #&gt; 7 0.34 0.99 NA NA NA 2.5 0.63731046 #&gt; 8 0.30 0.99 NA NA NA 2.3 0.00000029 #&gt; SRMR eCRMS eBIC #&gt; 1 0.1469496 0.164 2071 #&gt; 2 0.0425668 0.054 -36 #&gt; 3 0.0224515 0.033 -117 #&gt; 4 0.0169989 0.031 -86 #&gt; 5 0.0102808 0.024 -57 #&gt; 6 0.0059603 0.022 -24 #&gt; 7 0.0024070 NA NA #&gt; 8 0.0000016 NA NA You will receive a plot and plenty of output. The Velicer MAP achieved a minimum (partial correlation) with 2 factors. Note that you also get the Bayesian Information Criterium (BIC) as well as the sample adjusted version (SABIC). These suggests 2 (Velicer MAP), and 6 (BIC and SABIC) factors. Of course, six factors maybe a bit too much with our dataset of 11 variables. 5.3.5 EFA example Finally, we arrive at running the EFA. Before we can run it we need to consider two additional points (almost there). On one hand, we will need to choose what factor extraction method to use. This includes principal components, maximum likelihood (which some recommend when data is “suficiently” normal distributed), principal axis factoring (which some recommend if the data is not), and more. One another hand, we need to decide what factor rotation methods to use to enhance the factor interpret ability. We have orthogonal rotation which assumes that factors are not correlated. We also have oblique factor rotation which assumes inter-factor correlations. The psych packages includes various rotation methods. Popular ones include the “oblimin” (oblique factor rotation) and “varimax” (orthogonal). Just as an example I will go with Maximumlikelihood and varimax factor rotation, and enter 2 factors. fa(mydata, nfactors=2,fm=&quot;ML&quot;, rotate=&quot;varimax&quot;) #&gt; Factor Analysis using method = ml #&gt; Call: fa(r = mydata, nfactors = 2, rotate = &quot;varimax&quot;, fm = &quot;ML&quot;) #&gt; Standardized loadings (pattern matrix) based upon correlation matrix #&gt; ML2 ML1 h2 u2 com #&gt; y1 0.83 0.15 0.71 0.286 1.1 #&gt; y2 0.77 0.07 0.59 0.407 1.0 #&gt; y3 0.68 0.17 0.49 0.512 1.1 #&gt; y4 0.81 0.28 0.74 0.264 1.2 #&gt; y5 0.71 0.39 0.66 0.342 1.6 #&gt; y6 0.77 0.19 0.62 0.378 1.1 #&gt; y7 0.78 0.24 0.67 0.334 1.2 #&gt; y8 0.79 0.29 0.71 0.292 1.3 #&gt; y9 0.26 0.89 0.85 0.147 1.2 #&gt; y10 0.22 0.94 0.94 0.058 1.1 #&gt; y11 0.17 0.86 0.76 0.236 1.1 #&gt; #&gt; ML2 ML1 #&gt; SS loadings 4.87 2.88 #&gt; Proportion Var 0.44 0.26 #&gt; Cumulative Var 0.44 0.70 #&gt; Proportion Explained 0.63 0.37 #&gt; Cumulative Proportion 0.63 1.00 #&gt; #&gt; Mean item complexity = 1.2 #&gt; Test of the hypothesis that 2 factors are sufficient. #&gt; #&gt; df null model = 55 with the objective function = 9.74 with Chi Square = 677.07 #&gt; df of the model are 34 and the objective function was 0.83 #&gt; #&gt; The root mean square of the residuals (RMSR) is 0.04 #&gt; The df corrected root mean square of the residuals is 0.05 #&gt; #&gt; The harmonic n.obs is 75 with the empirical chi square 15.11 with prob &lt; 1 #&gt; The total n.obs was 75 with Likelihood Chi Square = 56.66 with prob &lt; 0.0087 #&gt; #&gt; Tucker Lewis Index of factoring reliability = 0.94 #&gt; RMSEA index = 0.093 and the 90 % confidence intervals are 0.048 0.137 #&gt; BIC = -90.14 #&gt; Fit based upon off diagonal values = 0.99 #&gt; Measures of factor score adequacy #&gt; ML2 ML1 #&gt; Correlation of (regression) scores with factors 0.96 0.98 #&gt; Multiple R square of scores with factors 0.92 0.95 #&gt; Minimum correlation of possible factor scores 0.84 0.90 Lots of output. From to the top you see the factor loadings (PA1-PA2), communalities (h2), uniqueness values (u2), and complexity (com). Scroll down, the “Proportion var” indicates that the first factor “explains” 44% of the total variance, the second factor 26%. 5.4 Referred article (McDonald’s omega reliability index) Flora, D. B. (2020). Your coefficient alpha is probably wrong, but which coefficient omega is right? A tutorial on using R to obtain better reliability estimates. Advances in Methods and Practices in Psychological Science, 3(4), 484–501. https://doi.org/10.1177/2515245920951747 "],["regression-analysis.html", "Chapter 6 Regression analysis 6.1 Regression models and things to consider 6.2 (Simple) linear regression using a general linear model 6.3 Simple generalIZED linear models 6.4 Mixed effects model 6.5 Model assumptions 6.6 Effect sizes", " Chapter 6 Regression analysis This part continues the data analysis spirit with one of the most common analysis models, regression. Does x relate to y (or even predict given sufficient methodological rigor)? Is there indication of a linear relation? Is this relation associated with the level of another variable (moderation)? There exists a wide variety of regression techniques. 1. Starting off with a brief overview of “regression language”, how to reshape datasets from wide to long (and vice versa), and things to consider prior to fitting regression models 2. We move on to the “classic” (frequentist) linear regression. We briefly explore general linear models versus generalized linear models. Then we explore moderation and how to plot it simple slopes, heyman-nuyens 3. I then discuss other regression models: repeated measures, multivariate regression, and mixed-effects regression models. 4. I won’t ignore model assumptions and provide you some ways on how to inspect these in classic linear regression and mixed-effects regression models 5. We end off with a brief discussion of effect sizes 6.1 Regression models and things to consider 6.1.1 The language of regression models In regression models we regress the outcome or outcomes (dependent) variable(s) on one or more predictors (independent variables). Similar to dplyr and ggplot2, regression have their own preferred notations and “symbol” use. A brief overview: 1. Y ~ variable1 + variable2 will output the “main effect” 2. Y ~ variable1 : variable2 will output only the interaction/moderation estimate between them 3. Y ~ variable1 x variable2 will output both main effects and interactions 6.1.2 Long to wide format and vice versa Generally you can distinguish two types of data formats: a wide data format in which every repeated measure is put in separate column and the long format where repeated measures are put in the same column. Knowing how to transform your data between wide and long data formats is crucial, as different regression models require different data structures. For example, regression with multiple outcomes prefers your data to be in wide format, whereas mixed-effects models ask for a long format. A variety of functions can transform long formats to wide formats, and back. Personally, I prefer using the melt() and dcast() functions from the reshape2 package. For the demonstration below, I made a dataset in wide data format set.seed(123) mydata = data.frame( ID = factor(1:10), age_6_years = runif(10,100,120), age_8_years = runif(10,120,135), age_10_years = runif(10,135,150), age_12_years = runif(10,150,180) ) Now I will reshape the data from wide to long so that the ages are “glued” to each individual (4 ages per individual). With the melt() function we specify the variable that identifies individuals using “id.var” (here ID). library(reshape2) mydata_long = melt(mydata, id.var =&quot;ID&quot;, variable_name =&quot;age&quot;) head(mydata_long) #&gt; ID variable value #&gt; 1 1 age_6_years 105.7516 #&gt; 2 2 age_6_years 115.7661 #&gt; 3 3 age_6_years 108.1795 #&gt; 4 4 age_6_years 117.6603 #&gt; 5 5 age_6_years 118.8093 #&gt; 6 6 age_6_years 100.9111 I’m aware that the reshape2 package is quite old now, so I will also provide you an alternative from the more modern tidyr package. To mimic the melt() function, tidyr has its own pivot_longer() function. library(tidyr) #&gt; #&gt; Attaching package: &#39;tidyr&#39; #&gt; The following object is masked from &#39;package:reshape2&#39;: #&gt; #&gt; smiths mydata_long =pivot_longer(mydata, cols = -ID, names_to = &quot;age&quot;, values_to = &quot;value&quot;) head(mydata_long) #&gt; # A tibble: 6 × 3 #&gt; ID age value #&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 age_6_years 106. #&gt; 2 1 age_8_years 134. #&gt; 3 1 age_10_years 148. #&gt; 4 1 age_12_years 179. #&gt; 5 2 age_6_years 116. #&gt; 6 2 age_8_years 127. Now we go back from long format to wide format. data_wide = dcast(data = mydata_long , ID ~ age, value.var = &quot;value&quot;) head(data_wide) #&gt; ID age_10_years age_12_years age_6_years age_8_years #&gt; 1 1 148.3431 178.8907 105.7516 134.3525 #&gt; 2 2 145.3921 177.0690 115.7661 126.8000 #&gt; 3 3 144.6076 170.7212 108.1795 130.1636 #&gt; 4 4 149.9140 173.8640 117.6603 128.5895 #&gt; 5 5 144.8356 150.7384 118.8093 121.5439 #&gt; 6 6 145.6280 164.3339 100.9111 133.4974 If you prefer tidyr, you can instead use the the pivot_wider() function. mydata_wide = pivot_wider(mydata_long, names_from = &quot;age&quot;, values_from = &quot;value&quot;) head(data_wide) #&gt; ID age_10_years age_12_years age_6_years age_8_years #&gt; 1 1 148.3431 178.8907 105.7516 134.3525 #&gt; 2 2 145.3921 177.0690 115.7661 126.8000 #&gt; 3 3 144.6076 170.7212 108.1795 130.1636 #&gt; 4 4 149.9140 173.8640 117.6603 128.5895 #&gt; 5 5 144.8356 150.7384 118.8093 121.5439 #&gt; 6 6 145.6280 164.3339 100.9111 133.4974 This was a fairly simple example with age as main variable. What if we had multiple variables? Take for example the next dataset in long format. set.seed(123) mydata = data.frame( subject = factor( rep( rep(c(1:100), times = 2), times = 3 ) ), condition = factor( rep( rep(c(1:2), each = 100 ), times = 3 ) ), wave = factor( rep(c(1:3), each = 200) ), score = runif(600, 1, 100) ) head(mydata) #&gt; subject condition wave score #&gt; 1 1 1 1 29.470174 #&gt; 2 2 1 1 79.042208 #&gt; 3 3 1 1 41.488715 #&gt; 4 4 1 1 88.418723 #&gt; 5 5 1 1 94.106261 #&gt; 6 6 1 1 5.510093 We could transform this dataset to wide format by combining both the variable wave and condition. When combining multiple variable to wide format, we cannot use the raw score anymore (otherwise it stays long), so a function will need to be applied that will transform the score variable. Here we can simply aggregate the score variable per wave and condition. mydata_wide = dcast(mydata, subject ~ wave + condition, value.var = &quot;score&quot;, fun.aggregate = mean) names(mydata_wide)[2:ncol(mydata_wide)] = c(&quot;Wave1_Condition1&quot;, &quot;Wave2_Condition1&quot;, &quot;Wave3_Condition1&quot;, &quot;Wave1_Condition2&quot;, &quot;Wave2_Condition2&quot;, &quot;Wave3_Condition2&quot;) head(mydata_wide) #&gt; subject Wave1_Condition1 Wave2_Condition1 #&gt; 1 1 29.470174 60.39891 #&gt; 2 2 79.042208 33.94953 #&gt; 3 3 41.488715 49.37269 #&gt; 4 4 88.418723 95.49291 #&gt; 5 5 94.106261 48.80734 #&gt; 6 6 5.510093 89.14467 #&gt; Wave3_Condition1 Wave1_Condition2 Wave2_Condition2 #&gt; 1 24.63388 78.672951 98.61938 #&gt; 2 96.27353 1.933561 14.56968 #&gt; 3 60.53521 78.127522 90.62565 #&gt; 4 51.98794 73.209675 58.05388 #&gt; 5 40.85476 63.383053 40.14944 #&gt; 6 88.14441 48.610172 45.53045 #&gt; Wave3_Condition2 #&gt; 1 36.007002 #&gt; 2 37.277703 #&gt; 3 29.422913 #&gt; 4 8.917318 #&gt; 5 37.179973 #&gt; 6 18.623368 Unfortunately, we can’t go back to long format as we would not know what the original not-aggregated scores were (various combinations can lead to the same average score). 6.2 (Simple) linear regression using a general linear model We start our demonstration of fitting linear lines with the lm() function to compute a “relatively simple” general linear regression model. However, I will quickly note that the classic regression may show its age as more recommended modern alternatives are available and problems such as the specificity problem (in repeated measures context, see the next part about MANOVA) could arise. The data this time is an online free-to-use dataset about the quality of red wine based on physicochemical tests mydata = read.csv(&quot;data_files/wineQualityReds.csv&quot;) head(mydata) #&gt; X fixed.acidity volatile.acidity citric.acid #&gt; 1 1 7.4 0.70 0.00 #&gt; 2 2 7.8 0.88 0.00 #&gt; 3 3 7.8 0.76 0.04 #&gt; 4 4 11.2 0.28 0.56 #&gt; 5 5 7.4 0.70 0.00 #&gt; 6 6 7.4 0.66 0.00 #&gt; residual.sugar chlorides free.sulfur.dioxide #&gt; 1 1.9 0.076 11 #&gt; 2 2.6 0.098 25 #&gt; 3 2.3 0.092 15 #&gt; 4 1.9 0.075 17 #&gt; 5 1.9 0.076 11 #&gt; 6 1.8 0.075 13 #&gt; total.sulfur.dioxide density pH sulphates alcohol #&gt; 1 34 0.9978 3.51 0.56 9.4 #&gt; 2 67 0.9968 3.20 0.68 9.8 #&gt; 3 54 0.9970 3.26 0.65 9.8 #&gt; 4 60 0.9980 3.16 0.58 9.8 #&gt; 5 34 0.9978 3.51 0.56 9.4 #&gt; 6 40 0.9978 3.51 0.56 9.4 #&gt; quality #&gt; 1 5 #&gt; 2 5 #&gt; 3 5 #&gt; 4 6 #&gt; 5 5 #&gt; 6 5 Now before you regress X with Y, always check your data (e.g., outliers, “strange occurrences”, unwanted duplicates, etc.), run descriptive statistics, check correlations, and visualize the distribution and relations/patterns between variables. Let’s assume we did all this and that we want to compute the main effect of alcohol content on the potential of Hydrogen (pH) scale. mylm = lm(pH~ alcohol, data = mydata) summary(mylm) #&gt; #&gt; Call: #&gt; lm(formula = pH ~ alcohol, data = mydata) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.54064 -0.10223 -0.00064 0.09340 0.63701 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 3.000606 0.037171 80.725 &lt;2e-16 *** #&gt; alcohol 0.029791 0.003548 8.397 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.1511 on 1597 degrees of freedom #&gt; Multiple R-squared: 0.04228, Adjusted R-squared: 0.04169 #&gt; F-statistic: 70.51 on 1 and 1597 DF, p-value: &lt; 2.2e-16 Inspecting the summary output, it never hurts to glance over the residuals as it can have a first quick look at how the residuals are distributed. Of note, general linear models assume a normal distribution of the residuals of the model (see the upcoming parts), not the variables themselves. That being said, this is not an excuse to not check the distribution of your variables**. Moving on, we have the coefficients with a statistically significant (p value) positive main effect. We can just ask directly for these coefficients using coef(mylm) and their confidence interval using confint(mylm). Next to coefficients, we can ask for the predicted values (that make up the predicted linear line) using predict(mylm), the confidence interval around the predicted values using predict(mylm,interval=“confidence”), and the residuals using residuals(mylm). Note that the regression coefficients are not standardized. If we want standardized versions (that could ease the interpretation and more easily allow comparison) we can either standardize both the predictor and outcome or we can use the lm.beta() function from the QuantPsyc package or standardize_parameters() function from the effectsize package. # Not using the lm.beta() function. Here I standardized outcome and predictor within the lm() but feel free to compute and add standardized variables to your dataset and use these in the lm() function coef( lm( scale(mydata$pH) ~ scale(mydata$alcohol), data = mydata) )[2] #&gt; scale(mydata$alcohol) #&gt; 0.2056325 # Using the lm.beta() function from QuantPsyc package library(QuantPsyc) #&gt; Warning: package &#39;QuantPsyc&#39; was built under R version #&gt; 4.3.3 #&gt; Loading required package: boot #&gt; Loading required package: dplyr #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union #&gt; Loading required package: purrr #&gt; Loading required package: MASS #&gt; #&gt; Attaching package: &#39;MASS&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; select #&gt; #&gt; Attaching package: &#39;QuantPsyc&#39; #&gt; The following object is masked from &#39;package:base&#39;: #&gt; #&gt; norm lm.beta( lm(pH~ alcohol, data = mydata) ) #&gt; alcohol #&gt; 0.2056325 # Or using the standardize_parameters() from the effectsize package library(effectsize) standardize_parameters(mylm) #&gt; # Standardization method: refit #&gt; #&gt; Parameter | Std. Coef. | 95% CI #&gt; ---------------------------------------- #&gt; (Intercept) | 1.32e-16 | [-0.05, 0.05] #&gt; alcohol | 0.21 | [ 0.16, 0.25] In case of binary predictors, we can standardize the outcome leading to partially standardized coefficients. 6.2.1 Interaction terms, Johnson-Neyman intervals, and simple slopes 6.2.1.1 two-way interaction with continuous variable You suspect that the (linear) relation between pH and alcohol content changes by the level of chlorides (a continuous variable). In other words, we expect a moderation “effect” of chlorides. We add the main effect of chlorides and the interaction term between alcohol and chlorides to the model. mylm = lm(pH ~ alcohol * chlorides, data = mydata) summary(mylm) #&gt; #&gt; Call: #&gt; lm(formula = pH ~ alcohol * chlorides, data = mydata) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.52078 -0.09344 -0.00439 0.09103 0.59329 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 2.965316 0.081419 36.420 &lt; 2e-16 *** #&gt; alcohol 0.040910 0.008201 4.989 6.74e-07 *** #&gt; chlorides 1.607694 0.948468 1.695 0.0903 . #&gt; alcohol:chlorides -0.245655 0.098170 -2.502 0.0124 * #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.1469 on 1595 degrees of freedom #&gt; Multiple R-squared: 0.09651, Adjusted R-squared: 0.09481 #&gt; F-statistic: 56.79 on 3 and 1595 DF, p-value: &lt; 2.2e-16 In our example the interaction term is negative. What does this mean? Well, what helps is to visualize interaction effects. Before we plot anything I want to briefly stand still for a moment. Procedures to visualize interactions could potentially be misleading if you are not careful. Simple slopes are often used when plotting interaction “effects”. Simple slopes simply represent the relation of the predictor’s effect on the outcome at different levels of the moderator. The important thing with these slopes is that you need to select some values of the moderator. More specifically, the link between outcome and predictor (i.e., their slope) is plotted against these values. Often people select three values that represent “low” levels of the moderator (often one standard deviation below the mean), “moderate” levels (often the mean), and “high” levels (often one standard deviation above the mean). Question is, do these values truly represent “low”, “moderate”, and “high”? This is likely not the case if e.g. the distribution of the moderation variable is “notably” skewed. In case of an interaction with two continuous variables, I would recommend to plot Johnson-Neyman intervals using the johnson_neyman() function from the interactions package. This function computes and plots the Johnson-Neyman intervals along series of values of the moderator. In one glance, you can spot at what values of the moderator, the main effect of the predictor on outcome is statistically significant based on p value. Even better in my humble opinion, the size of the relation at each point is displayed in full view. library(interactions) #&gt; Warning: package &#39;interactions&#39; was built under R version #&gt; 4.3.3 johnson_neyman(model=mylm, pred=alcohol, modx=chlorides) #&gt; JOHNSON-NEYMAN INTERVAL #&gt; #&gt; When chlorides is OUTSIDE the interval [0.12, 0.50], the #&gt; slope of alcohol is p &lt; .05. #&gt; #&gt; Note: The range of observed values of chlorides is #&gt; [0.01, 0.61] If you dislike the plot, you can also make your own version. Luckily, all elements that I need to plot are provided by the package itself. Recall my “store as an object and click” strategy from the previous part? If I store my Johnson Neyman plot in a variable, click on the variable under (Environment), and inspect every element, then I could extract all information that I need. myplot = johnson_neyman(model=mylm, pred=alcohol, modx=chlorides) myjohnson_neyman = data.frame( ci_lower = myplot[[&quot;cbands&quot;]][[&quot;Lower&quot;]], # Lower bound of the 95% ci_upper = myplot[[&quot;cbands&quot;]][[&quot;Upper&quot;]], # Upper bound slope = myplot[[&quot;cbands&quot;]][[&quot;Slope of alcohol&quot;]], # the slope significance = myplot[[&quot;cbands&quot;]][[&quot;Significance&quot;]], # indicates when the slope is significant or not bound_start = min(mydata$chlorides), # To make the range of observed data for the chlorides variable, the lowest value bound_end = max(mydata$chlorides), # The highest value chlorides_x_axis = myplot[[&quot;cbands&quot;]][[&quot;chlorides&quot;]] # Needed for the x-axis ) Now we can load the ggplot2 package and create the figure ourselves. Below, I will approximately recreate the original figure but in grayscale. library(ggplot2) myjohnson_neyman %&gt;% # Only the full range of &quot;chlorides&quot; ggplot(aes(x=chlorides_x_axis)) + scale_fill_manual(values=c(&quot;grey50&quot;,&quot;grey30&quot;), labels=c(&quot;p &lt; .05&quot;,&quot;n.s.&quot;), name=&quot;&quot; ) + # I had to set the labels since it otherwise switches them up. Also I omitted the legend title geom_ribbon( aes(ymin = ci_lower, ymax = ci_upper, fill = significance), alpha = 0.40 ) + geom_line( aes(y = ci_upper), size=1) + # I add this line otherwise the contours of the &quot;ribbon&quot; will be transparant (since apha = 0.40) geom_line( aes(y = ci_lower), size=1) + # See above geom_hline(aes(yintercept=0), linetype = &quot;dashed&quot;) + # Horizontal line at zero (helps to look when the confidence interval crosses zero) geom_segment(aes(x = bound_start , xend = bound_end), y = 0, yend=0, size=2 ) + # To create the thick line depicting the range of chlorides scale_color_manual(values=&quot;black&quot;, labels=&quot;Range of Chlorides&quot;, name=&quot;&quot; ) + # for the additional legend geom_line(aes(y = slope), size = 1) + # The slope line xlab(&quot;Chlorides&quot;) + ylab(&quot;Slope of alcohol&quot;) + ggtitle(&quot;Johnson-Neyman plot&quot;) + theme_classic() #&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 #&gt; 3.4.0. #&gt; ℹ Please use `linewidth` instead. #&gt; This warning is displayed once every 8 hours. #&gt; Call `lifecycle::last_lifecycle_warnings()` to see where #&gt; this warning was generated. 6.2.1.2 two-way interaction with a continuous and factor variable Forget the chlorides variable for a second. Now we want to see whether the link between pH and alcohol content is moderated by quality (a categorical/factor variable). To visualize this interaction, we could still use the Johnson-Neyman technique which will note statistical significance by category. However, since the values of the categorical/factor moderator are determined, we can plot simple slopes. For the simple slopes I will use the effect() function from the [effects package(https://cran.r-project.org/web/packages/effects/index.html)**. Let’s start with fitting the regression model. For simplicity, I will only include two values of “quality”. In addition, I will transform this variable to 0 to 1 to potentially ease interpretability. mydata_simpleslopes = mydata %&gt;% filter(quality %in% c(4,8)) %&gt;% mutate(quality = factor(recode(quality,&#39;4&#39; = &#39;0&#39; , &#39;8&#39; = &#39;1&#39;) ) ) mylm = lm(pH ~ alcohol * quality, data = mydata_simpleslopes) summary(mylm) #&gt; #&gt; Call: #&gt; lm(formula = pH ~ alcohol * quality, data = mydata_simpleslopes) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.55389 -0.07285 0.00586 0.09034 0.34470 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 2.34178 0.23601 9.922 8.78e-15 *** #&gt; alcohol 0.10129 0.02290 4.423 3.66e-05 *** #&gt; quality1 -0.45912 0.44029 -1.043 0.301 #&gt; alcohol:quality1 0.01319 0.03821 0.345 0.731 #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.1544 on 67 degrees of freedom #&gt; Multiple R-squared: 0.3793, Adjusted R-squared: 0.3515 #&gt; F-statistic: 13.65 on 3 and 67 DF, p-value: 4.796e-07 To the effect, I will add the interaction part (which you can copy-paste), then I define from where to where the slope should be drawn. Specifically, to cover the full range of the continuous predictor, I could use the minimum and maximum value of this variable. The confidence interval and the slope per category. The slope is a straight line library(effects) #&gt; Loading required package: carData #&gt; lattice theme set by effectsTheme() #&gt; See ?effectsTheme for details. data_slope = as.data.frame( effect(mod=mylm, term = &quot;alcohol * quality&quot;, xlevels=list(alcohol = c(min(mydata_simpleslopes$alcohol), max(mydata_simpleslopes$alcohol)),se=TRUE, confidence.level=.95) ) ) # Create the plot library(ggplot2) data_slope %&gt;% ggplot(aes(x = alcohol, y = fit, color = quality)) + geom_ribbon(aes(ymin = lower, ymax = upper, fill = quality), alpha = 0.2) + geom_line() Always handy would be some indication of the distribution of the variables to show where most data points lie. Here I show the data points of the outcome (pH) and their distribution (as a separate histograms to be put above the simple slopes). library(ggplot2) my_simple_slopes = data_slope %&gt;% ggplot(aes(x = alcohol, y = fit, color = quality)) + geom_point(data = mydata_simpleslopes, aes(y = pH), alpha = 0.5 ) + geom_ribbon(aes(ymin = lower, ymax = upper, fill = quality), alpha = 0.2) + geom_line() + facet_wrap(~ quality) + ylab(&quot;Slope&quot;) + xlab(&quot;Alcohol content&quot;) + theme_classic() + theme( legend.position = &quot;None&quot; ) my_histograms = mydata_simpleslopes %&gt;% ggplot(aes(x = alcohol, fill = quality)) + geom_histogram(color=&quot;black&quot;, alpha = 0.75, bins = 18) + facet_wrap(~quality) + theme_minimal() library(cowplot) plot_grid(my_histograms, my_simple_slopes, ncol=1, nrow=2, align=&quot;v&quot;, rel_heights = c(0.25, 1)) #&gt; Warning: Graphs cannot be vertically aligned unless the #&gt; axis parameter is set. Placing graphs unaligned. 6.3 Simple generalIZED linear models There may exist numerous instances in which your outcomes (and the residuals of the models) are “notably” deviating from the normal distribution. Think binary outcomes, counts (Poisson), categories, and so on. Generalized linear regression models allow to modify how the distribution of the model’s residuals is handled. This allows to do (binary logistic regression) and (logistic regression). As these models can soften the assumption of a normal distribution of the model’s residual terms, they can also be considered in case of model assumption violations. Within the R environment, generalized linear models resemble the general ones but they introduce (at least) two additional parameters: the family and the link. The link can be used to, if wished for, transform the model predicted outcome (e.g., logarithms). By default, the link is “identity” (no transformation). The family determines how the distribution or residuals is handled. By default the family is gaussian (normal distribution just like general linear models) but can be modified poison, binomial, gamma, and more. Several examples below. Starting off with an example of a poisson linear regression. set.seed(123) poisson_data = data.frame( counts = c( round(runif(30,15,35),0), round(runif(30,15,28),0), round(runif(30,1,18),0) ), altitude = 1:90 ) # Note family poisson will by default take the log from the outcome poisson_fit = glm(counts ~ altitude, data = poisson_data, family = poisson(link =&quot;log&quot;)) summary(poisson_fit) #&gt; #&gt; Call: #&gt; glm(formula = counts ~ altitude, family = poisson(link = &quot;log&quot;), #&gt; data = poisson_data) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 3.5020143 0.0427726 81.88 &lt;2e-16 *** #&gt; altitude -0.0136118 0.0009657 -14.10 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1) #&gt; #&gt; Null deviance: 382.59 on 89 degrees of freedom #&gt; Residual deviance: 176.47 on 88 degrees of freedom #&gt; AIC: 598.46 #&gt; #&gt; Number of Fisher Scoring iterations: 4 poisson_data$counts_log = log(poisson_data$counts) # link identity will apply no transformation (but since we took the log ourselves...) poisson_fit = glm(counts_log ~ altitude, data = poisson_data, family = poisson(link =&quot;identity&quot;)) #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.044522 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.433987 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.135494 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.496508 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.526361 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.772589 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.258097 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.496508 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.258097 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.178054 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.526361 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.178054 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.367296 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.258097 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.833213 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.496508 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.995732 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.772589 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.091042 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.526361 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.496508 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.367296 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.332205 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.555348 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.332205 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.367296 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.258097 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.295837 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.044522 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.890372 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.332205 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.295837 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.178054 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.218876 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.708050 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.044522 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.218876 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.890372 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.944439 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.890372 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.833213 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.995732 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.995732 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.995732 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.833213 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.833213 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.890372 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.044522 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.890372 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.258097 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.772589 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.044522 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.218876 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.833213 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.091042 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.890372 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.833213 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.218876 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 3.295837 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.995732 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.484907 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 1.098612 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.079442 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 1.791759 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.708050 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.197225 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.708050 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.708050 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.708050 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.079442 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.639057 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.484907 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.564949 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.197225 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 1.609438 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 1.945910 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.397895 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 1.945910 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 1.098612 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 1.609438 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.484907 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.079442 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.639057 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 1.098612 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.079442 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.890372 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.772589 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 2.772589 #&gt; Warning in dpois(y, mu, log = TRUE): non-integer x = #&gt; 1.386294 summary(poisson_fit) #&gt; #&gt; Call: #&gt; glm(formula = counts_log ~ altitude, family = poisson(link = &quot;identity&quot;), #&gt; data = poisson_data) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 3.598829 0.376842 9.550 &lt; 2e-16 *** #&gt; altitude -0.017720 0.006705 -2.643 0.00822 ** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1) #&gt; #&gt; Null deviance: 17.422 on 89 degrees of freedom #&gt; Residual deviance: 10.737 on 88 degrees of freedom #&gt; AIC: Inf #&gt; #&gt; Number of Fisher Scoring iterations: 5 # Note, that the regression coefficients will be on the log scale, so you could compute the exponent to ease interpretability exp(coef(poisson_fit)) #&gt; (Intercept) altitude #&gt; 36.5553993 0.9824361 Next, in case of a binary outcome. # Data used set.seed(123) binary_data = data.frame( on_or_off = c( round(runif(100,0,1),0) ), distance = 1:100 ) # Note family binomial will by default take the logit (log of the odds ratio) from the outcome binary_fit = glm(on_or_off ~ distance, data = binary_data, family = binomial(link =&quot;logit&quot;)) summary(binary_fit) #&gt; #&gt; Call: #&gt; glm(formula = on_or_off ~ distance, family = binomial(link = &quot;logit&quot;), #&gt; data = binary_data) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 0.343678 0.406669 0.845 0.398 #&gt; distance -0.009227 0.007052 -1.308 0.191 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 138.27 on 99 degrees of freedom #&gt; Residual deviance: 136.53 on 98 degrees of freedom #&gt; AIC: 140.53 #&gt; #&gt; Number of Fisher Scoring iterations: 4 For the final example, we could also fit a general model using the glm() function if we use a gaussian link. mydata = read.csv(&quot;data_files/wineQualityReds.csv&quot;) summary( lm( pH ~ alcohol , data = mydata ) ) #&gt; #&gt; Call: #&gt; lm(formula = pH ~ alcohol, data = mydata) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.54064 -0.10223 -0.00064 0.09340 0.63701 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 3.000606 0.037171 80.725 &lt;2e-16 *** #&gt; alcohol 0.029791 0.003548 8.397 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.1511 on 1597 degrees of freedom #&gt; Multiple R-squared: 0.04228, Adjusted R-squared: 0.04169 #&gt; F-statistic: 70.51 on 1 and 1597 DF, p-value: &lt; 2.2e-16 summary( glm( pH ~ alcohol, data = mydata, family = gaussian(link =&quot;identity&quot;)) ) #&gt; #&gt; Call: #&gt; glm(formula = pH ~ alcohol, family = gaussian(link = &quot;identity&quot;), #&gt; data = mydata) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 3.000606 0.037171 80.725 &lt;2e-16 *** #&gt; alcohol 0.029791 0.003548 8.397 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for gaussian family taken to be 0.02284161) #&gt; #&gt; Null deviance: 38.089 on 1598 degrees of freedom #&gt; Residual deviance: 36.478 on 1597 degrees of freedom #&gt; AIC: -1501.1 #&gt; #&gt; Number of Fisher Scoring iterations: 2 6.4 Mixed effects model 6.4.1 Fixed and random effects Suppose you have repeated measurements from the same subject. You are interested in whether a linear regression slope can be fitted with your measurement and time (change across time). Before you consider to run a fancy regression model, you quickly plot how linear relation would look like. Looks like a first indication of higher levels of the measurement across time. You realize that you have thirty measures per subject. So you wonder… would every subject also show this pattern? Does this average across subject positive slope fits all? We draw a regression slope of the same relation per subject. Some subjects show an increase, others a decrease or little change. In other words, on average across subjects, we may expect a positive linear relation. Nevertheless, there is some variety, some variance in this relation across subjects. Just now I winked at two concepts, fixed and random effects than can be modelled in mixed-effects regression models. Fixed effects can be considered your typical “dependent variable gets predicted by independent variable” part. Random effects refer to the variation in regression parameters across subjects or other “clusters. For example, one can attempt to study the influence of a medicine on reaction time. Here, we could”acknowledge” that some participants are naturally faster to react and their “starting point on reaction time” is lower (thus they’re faster) irrespective of medicine intake. In other words, you can acknowledge that there may be some variance in the natural/base/trait state of the outcome per person, and this depicted by random variance in the intercept. Moving on, we’re already familiar with the occurrence of random variance in the slope. Regression slopes depicting a given relation may differ per person. I will scratch the surface in my review below, but note that various points can be considered in these kind of analyses. I will generate a dataset that has 40 repetitions of measures across the 100 participants (our “cluster”). set.seed(123) n_participants = 100 n_repetitions = 40 icc_target = 0.45 # Generate participant IDs participant_id = rep(1:n_participants, each = n_repetitions) # Variance components (solving for within-person variance) between_var = 1 # Arbitrary choice within_var = between_var * (1 - icc_target) / icc_target # Ensures ICC = 0.45 # Generate random intercepts (participant-level effects) random_intercepts = rnorm(n_participants, mean = 50, sd = sqrt(between_var)) random_intercepts_expanded = rep(random_intercepts, each = n_repetitions) # Generate within-person residuals residuals = rnorm(n_participants * n_repetitions, mean = 0, sd = sqrt(within_var)) # Compute the outcome variable outcome = random_intercepts_expanded + residuals # Create the data frame mydata = data.frame( participant_id = factor(participant_id), measurement = rep(1:n_repetitions, times = n_participants), outcome = outcome, predictor = runif(4000, 10,100) ) As before, let’s quickly check how the outcome changes across time (indicated by “simple” regression slopes from the geom_smooth function). library(plotly) #&gt; #&gt; Attaching package: &#39;plotly&#39; #&gt; The following object is masked from &#39;package:ggplot2&#39;: #&gt; #&gt; last_plot #&gt; The following object is masked from &#39;package:MASS&#39;: #&gt; #&gt; select #&gt; The following object is masked from &#39;package:stats&#39;: #&gt; #&gt; filter #&gt; The following object is masked from &#39;package:graphics&#39;: #&gt; #&gt; layout ggplotly( mydata %&gt;% ggplot(aes(y = outcome, x = measurement, color=participant_id, group=participant_id)) + geom_smooth(method=&quot;lm&quot;, se=FALSE) + geom_smooth(aes(group=0),method=&quot;lm&quot;,se=FALSE, linetype=&quot;dashed&quot;,color=&quot;black&quot;) + theme_classic() ) #&gt; `geom_smooth()` using formula = &#39;y ~ x&#39; #&gt; `geom_smooth()` using formula = &#39;y ~ x&#39; Visually, we may not expect a general (fixed) relation. However, it could likely that there is a “notable” bit of variance in this slope per participant, at least based on vision. Looking at the start point (measurement 1), some participants have a lower or higher outcome compared to one another. Based on vision, you might expect that participants’ “baseline/natural/trait state” of the outcome may be different. In more technical terms you might expect some variance in the intercepts per schools. Alright, let’s fit a mixed-effect model. There are several packages in R that allow to fit mixed-effects models. I will use the lmer() function from the lmerTest package. Let’s start with the most transformation from your everyday general linear regression to general linear mixed regression, the introduction of a random intercept. library(lmerTest) #&gt; Loading required package: lme4 #&gt; Loading required package: Matrix #&gt; #&gt; Attaching package: &#39;Matrix&#39; #&gt; The following objects are masked from &#39;package:tidyr&#39;: #&gt; #&gt; expand, pack, unpack #&gt; #&gt; Attaching package: &#39;lmerTest&#39; #&gt; The following object is masked from &#39;package:lme4&#39;: #&gt; #&gt; lmer #&gt; The following object is masked from &#39;package:stats&#39;: #&gt; #&gt; step myfit = lmer(outcome ~ measurement + (1|participant_id), REML = TRUE, data = mydata) # Note. REML = Restricted likelihood. If &quot;TRUE&quot; then MaximumLikelihood would be used summary(myfit) #&gt; Linear mixed model fit by REML. t-tests use #&gt; Satterthwaite&#39;s method [lmerModLmerTest] #&gt; Formula: outcome ~ measurement + (1 | participant_id) #&gt; Data: mydata #&gt; #&gt; REML criterion at convergence: 12478.7 #&gt; #&gt; Scaled residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.1934 -0.6513 -0.0151 0.6813 3.5145 #&gt; #&gt; Random effects: #&gt; Groups Name Variance Std.Dev. #&gt; participant_id (Intercept) 0.8271 0.9094 #&gt; Residual 1.2157 1.1026 #&gt; Number of obs: 4000, groups: participant_id, 100 #&gt; #&gt; Fixed effects: #&gt; Estimate Std. Error df t value Pr(&gt;|t|) #&gt; (Intercept) 5.007e+01 9.764e-02 1.223e+02 512.800 &lt;2e-16 #&gt; measurement 1.210e-03 1.510e-03 3.899e+03 0.801 0.423 #&gt; #&gt; (Intercept) *** #&gt; measurement #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Correlation of Fixed Effects: #&gt; (Intr) #&gt; measurement -0.317 Of note, I used Restricted likelihood (REML) which is recommended when estimating random effects variances but not when you are interested in comparing models with different fixed effects. If you are interested To compare fixed effects, could consider Maximum Likelihood instead (REML = FALSE). Glancing over the summary, you might have the impression that it looks similar to output of classic linear models. What’s new is that we have a separation of random and fixed effects. Notice that the random effects part consists of variance so it could be more appropriate to call it random effect variance. The first parameter here is the random intercept variance (variations in the intercept when participants are compared). The second, the residual random variance, is variance . Essentially,it is variance that is left and this could be referred to as within-cluster variance (here within-participant variance). The sum of these variances forms the total variance. Similar to summary output from more “classic” linear regression we can call the regression coefficients using coef(). However, note that we now get the coefficients per cluster-unit (participants here). Have a look at the coefficients for the first seven participants. coef(myfit)[[&quot;participant_id&quot;]][1:7,] #&gt; (Intercept) measurement #&gt; 1 49.22585 0.001209674 #&gt; 2 49.75307 0.001209674 #&gt; 3 51.44249 0.001209674 #&gt; 4 50.01370 0.001209674 #&gt; 5 50.42549 0.001209674 #&gt; 6 51.63878 0.001209674 #&gt; 7 50.39875 0.001209674 The intercept is different per participant, “measurement” is the same per participant, which is as expected since we have modeled both a fixed and random effect for the intercept, whereas we estimated “measurement” as a sole fixed variable without a random part. To show the fixed effect (the fixed slope) you can call fixef(), while we can call for th random effect (the variation per cluster-unit) using ranef(). Good to know, the coefficient for the random intercept per participant is the sum of the fixed effect of the intercept (shared across participants) and the random effect of the intercept (unique per participant). Have a look at participant one. coef(myfit)[[&quot;participant_id&quot;]][1,1] #&gt; [1] 49.22585 # -&gt; 49.22585 fixef(myfit)[1] + ranef(myfit)[[&quot;participant_id&quot;]][1,] #&gt; (Intercept) #&gt; 49.22585 # -&gt; 49.22585 6.4.2 Intraclass correlation/variance partition Now that we familiarized with the summary output. One parameter to compute in context of repeated measures is the intraclasscorrelation (ICC) (sometimes named variance partition coefficient) which shows the proportion of the between-cluster (between-participant) variance as compared to the total variance.To compute the ICC, fit a mixed-effects model where the outcome variable is regressed on only a fixed and random intercept (a null model). In our case: lmer(outcome ~ 1 + (1|participant_id), REML = TRUE, data = mydata) #&gt; Linear mixed model fit by REML [&#39;lmerModLmerTest&#39;] #&gt; Formula: outcome ~ 1 + (1 | participant_id) #&gt; Data: mydata #&gt; REML criterion at convergence: 12468.22 #&gt; Random effects: #&gt; Groups Name Std.Dev. #&gt; participant_id (Intercept) 0.9094 #&gt; Residual 1.1025 #&gt; Number of obs: 4000, groups: participant_id, 100 #&gt; Fixed Effects: #&gt; (Intercept) #&gt; 50.09 To obtain the ICC we could use the icc() function from the performance package. This function will output both the adjusted ICC and the unadjusted ICC. Without fancy packages we could also compute it easily ourselves. From the summary output, take the between-cluster variance (here the random intercept variance; 0.8271) and divide it by the sum of the random variances (0.8271 + 1.2156). The outcome is 0.4049053. 6.4.3 Random slopes Let’s add the “predictor” variable as both a fixed and random effect. We now introduce both random intercept variance and random slope variance to the model. Let’s look again at the summary output myfit = lmer(outcome ~ measurement + predictor + (1 + predictor |participant_id), REML = TRUE, data = mydata) #&gt; Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = #&gt; control$checkConv, : Model failed to converge with #&gt; max|grad| = 1.81782 (tol = 0.002, component 1) #&gt; Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: very large eigenvalue #&gt; - Rescale variables? summary(myfit) #&gt; Linear mixed model fit by REML. t-tests use #&gt; Satterthwaite&#39;s method [lmerModLmerTest] #&gt; Formula: #&gt; outcome ~ measurement + predictor + (1 + predictor | participant_id) #&gt; Data: mydata #&gt; #&gt; REML criterion at convergence: 12488.1 #&gt; #&gt; Scaled residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.1858 -0.6519 -0.0073 0.6809 3.5471 #&gt; #&gt; Random effects: #&gt; Groups Name Variance Std.Dev. Corr #&gt; participant_id (Intercept) 7.264e-01 0.852283 #&gt; predictor 1.012e-05 0.003182 0.13 #&gt; Residual 1.211e+00 1.100293 #&gt; Number of obs: 4000, groups: participant_id, 100 #&gt; #&gt; Fixed effects: #&gt; Estimate Std. Error df t value #&gt; (Intercept) 5.008e+01 9.981e-02 1.310e+02 501.739 #&gt; measurement 1.385e-03 1.510e-03 3.887e+03 0.918 #&gt; predictor -2.826e-04 7.573e-04 8.973e+01 -0.373 #&gt; Pr(&gt;|t|) #&gt; (Intercept) &lt;2e-16 *** #&gt; measurement 0.359 #&gt; predictor 0.710 #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Correlation of Fixed Effects: #&gt; (Intr) msrmnt #&gt; measurement -0.311 #&gt; predictor -0.299 0.002 #&gt; optimizer (nloptwrap) convergence code: 0 (OK) #&gt; Model failed to converge with max|grad| = 1.81782 (tol = 0.002, component 1) #&gt; Model is nearly unidentifiable: very large eigenvalue #&gt; - Rescale variables? In the section “Random effects” we now get a correlation of .13. This is the correlation between random intercepts and the random slopes, indicating that participants with higher intercepts (“baselines”) tend to have (slightly) higher slopes. Moving on, we also see correlations between the fixed effects. The fixed effect of “measurement” and “predictor” (the “average across participants” slopes so to speak) have a negative correlation with the fixed intercept (“average across participants”). Participants, in general, with a lower intercept (“baseline” levels of the outcome) may show higher levels of “measurement” and “predictor”. Furthermore, the fixed-effects between “measurement” and “predictor” is near-zero, which can be a good thing as a high correlation may indicate multicollinearity. 6.4.3.1 Extraction and plotting the random slopes Now that we have both a random intercept and a random slope, we could extract and plot the specific slope of a cluster-unit (here a participant). This process is relatively simple: extract the necessary ingredients from the fitted mixed-effect model (i.e., the participant, their intercept, and their slope) and glue it to the original dataset (mydata). In this example I stored the result in a separate dataset. extracted_data = data.frame( rownames ( coef(myfit)[[&quot;participant_id&quot;]] ), # coef(myfit)... does contain the participant but only as rownames so I call rownames of this object to get my participants coef(myfit)[[&quot;participant_id&quot;]] # Note. I added the [[&quot;participant_id&quot;]] part as data.frame( coef(myfit) ) would prompt an error ) names(extracted_data) = c(&quot;participant_id&quot;,&quot;intercept&quot;,&quot;slope_measurement&quot;,&quot;slope_predictor&quot;) mydata_plus_extracted = left_join(mydata,extracted_data) #&gt; Joining with `by = join_by(participant_id)` And plot it. For visual clarity, I included only participants 1 to 30. library(ggplot2) library(plotly) library(dplyr) ggplotly( mydata_plus_extracted %&gt;% filter(participant_id&lt;=30) %&gt;% ggplot(aes(y=outcome,x= predictor,color=participant_id)) + geom_point(alpha = 0.10) + # To avoid visual overload I make the points fully transparant geom_abline(aes(intercept = intercept, slope = slope_predictor, color = participant_id) ) + geom_abline(intercept = 50.08, slope = -0.0002826, color = &quot;black&quot;,linetype=&quot;dashed&quot; ) + # Based on the summary output, the fixed slope theme_classic() ) 6.5 Model assumptions By now you have seen the output of a few regression models. Have you ever wondered how the model the “sees” your data? The models receives our instruction to “forcefully” draw a (linear) line but while computing parameter estimations, the model will likely have some assumptions of its own to ease the computations. The assumptions of the model can relate to how accuracy you can interpret the output of the model, so violations of assumptions should not be overlooked. We have already seen some model assumptions when discussing exploratory factor analysis. In this part, I’ll focus on linear regression models and provide you some easy ways to inspect certain model assumptions. 6.5.1 Assumptions: Linear regression (not multilevel) Unsurprisingly, linear regression models assumes linear relations between outcomes and predictors, something you can visually inspect using scatter plots. Other assumptions include: The assumption of normally distributed residuals The assumption of homoscedasticity/homogeneity of residuals The assumption of multicollinearity (brief repetition from previous part) The assumption of no autocorrelation of residuals/independence (time-series data!) One function that does most of the above in one go 6.5.1.1 Normal distributed residuals of the regression model Various regression models expect that the residuals from the fitted regression model follow a normal (gaussian) distribution. Let me repeat: the error terms of the model are expected to resemble a normal distribution, this assumption does not say that the outcome should resemble a normal distribution. Rather unsurprising, a histogram of the model’s residuals will suffice. Let’s take again the data about the quality of red wine, fit a linear regression model, extract the model its residuals, and plot a histogram. mydata = read.csv(&quot;data_files/wineQualityReds.csv&quot;) mymodel = lm(pH ~ chlorides, data = mydata) summary(mymodel) # Recall that the summary output can give a quick at how the residuals are distributed #&gt; #&gt; Call: #&gt; lm(formula = pH ~ chlorides, data = mydata) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.46195 -0.09847 -0.00152 0.08717 0.65849 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 3.387153 0.007861 430.89 &lt;2e-16 *** #&gt; chlorides -0.869355 0.079148 -10.98 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.1489 on 1597 degrees of freedom #&gt; Multiple R-squared: 0.07024, Adjusted R-squared: 0.06966 #&gt; F-statistic: 120.6 on 1 and 1597 DF, p-value: &lt; 2.2e-16 library(ggplot2) data.frame( myresiduals=mymodel$residuals) %&gt;% ggplot(aes(x = myresiduals)) + geom_histogram(fill=&quot;white&quot;,color=&quot;black&quot;) #&gt; `stat_bin()` using `bins = 30`. Pick better value with #&gt; `binwidth`. 6.5.1.2 homoscedasticity/homogeneity of the residuals An unique sounding word, the homoscedasticity of the residuals refer to the extent to which residuals are constant along the predicted values of the models’ linear regression. If we want to check whether the residuals are homoscedastic or rather heteroskedastic, we will need to extract and plot the residuals (y-axis) along the predicted values (x-axis). Now, you could use the “raw” residuals using the residuals() function. However, you could also use standardized residuals. Here I will use studentized residuals as an example. To get studentized residuals, you can use the rstudent() function. Below, I will plot both the raw and studentized residuals. mymodel = lm(pH ~ chlorides, data = mydata) library(ggplot2) data.frame(myresiduals=residuals(mymodel), mypredictedvalues = predict(mymodel) ) %&gt;% ggplot(aes(x = mypredictedvalues, y = myresiduals)) + geom_point() + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;blue&quot;) + ylab(&quot; raw residuals&quot;) + theme_classic() data.frame(myresiduals=rstudent(mymodel), mypredictedvalues = predict(mymodel) ) %&gt;% ggplot(aes(x = mypredictedvalues, y = myresiduals)) + geom_point() + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;blue&quot;) + ylab(&quot;studentized residuals&quot;) + theme_classic() Now for a visual inspection of the extent of homoscedasticity, look at the most left side of the plot. Mind the distance between the upper and lower points. What I always do is to put my index finger on the “upper residuals” and my thumb on the “lower ones”. Now move along from left to right (if you follow my method, slide your hand from left to right). Did the distance change notably at some or multiple points? Do you see a funnel shape? If so, this may indicate that the residuals are not (that) homoscedastic. Next to visual inspection, you could also consider the Breusch-Pagan test for heteroskedasticity in which a p value below .05 suggests heteroskedasticity. For this purpose you can use e.g., the bptest() function from the lmtest package library(lmtest) #&gt; Warning: package &#39;lmtest&#39; was built under R version 4.3.3 #&gt; Loading required package: zoo #&gt; #&gt; Attaching package: &#39;zoo&#39; #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; as.Date, as.Date.numeric bptest(mymodel, studentize = TRUE) #&gt; #&gt; studentized Breusch-Pagan test #&gt; #&gt; data: mymodel #&gt; BP = 6.3505, df = 1, p-value = 0.01173 6.5.1.3 No multicollinearity As discussed in the previous part about exploratory factor analysis, we can use the vif() function from the car package. Of course, this assumption applies to regression models with more than one predictor. As I mentioned before, pay extra attention to vif scores above five, which could suggest “high” multicollinearity. mymodel = lm(pH ~ chlorides + alcohol + residual.sugar,data = mydata) library(car) #&gt; #&gt; Attaching package: &#39;car&#39; #&gt; The following object is masked from &#39;package:purrr&#39;: #&gt; #&gt; some #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; recode #&gt; The following object is masked from &#39;package:boot&#39;: #&gt; #&gt; logit vif(mymodel) #&gt; chlorides alcohol residual.sugar #&gt; 1.056105 1.054706 1.006240 6.5.1.4 No autocorrelation of residuals An assumption in case you have multiple measurements over time (e.g., time-series), autocorrelation implies that the residuals relate to one another over time. Specifically, the correlation between a variable (here the residuals) and a copy of itself at one or more previous time points (commonly referred to as lags) First, I will generate data in which I predetermine a fixed autocorrelation of .55. example_autocor = data.frame( predictor = runif(9999,1,10) ) error = arima.sim(n = 9999, list(ar = 0.55)) # AR(1) process, to create autocorrelated residuals example_autocor$outcome = 3 + 1 * example_autocor$predictor + error Autocorrelations can be generated and visualized using the acf() function from the car package. The function will correlate the inserted value with its past values (commonly called “lags”) and these autocorrelations are depicted as bars (unless you instruct not to plot the autocorrelations). A “low” autocorrelation is indicated by bars that drop immediately to (approximately) zero. Bars that instead gradually or barely decline are indicative of autocorrelation. Below I will use the acf() function but I’ll tell the function to not plot the autocorrelations, I will do that myself. mymodel = lm(outcome ~ predictor ,data = example_autocor) my_autocor = acf(residuals(mymodel), lag.max = 5, plot = FALSE) plotly::ggplotly( data.frame(autocorrelations = my_autocor$acf[-1], # The first value is the correlation with itself at the current time, hence a perfect correlation of 1 (so I will exclude this value), lagged = my_autocor$lag[-1] ) %&gt;% ggplot(aes(y=autocorrelations, x = lagged)) + coord_cartesian(ylim=c(-0.5,0.5)) + geom_bar(stat=&quot;identity&quot;, fill=&quot;green&quot;,color=&quot;black&quot;,alpha=0.75) + geom_point() + geom_line(alpha = 0.5, linetype=&quot;dashed&quot;) + geom_hline(aes(yintercept = 0), color=&quot;red&quot;) + geom_hline(aes(yintercept = 0.2), color=&quot;blue&quot;) + geom_hline(aes(yintercept = -0.2), color=&quot;blue&quot;) # Note that negative autocorrelations are also possible. ) #&gt; Warning: &#39;bar&#39; objects don&#39;t have these attributes: &#39;mode&#39; #&gt; Valid attributes include: #&gt; &#39;_deprecated&#39;, &#39;alignmentgroup&#39;, &#39;base&#39;, &#39;basesrc&#39;, &#39;cliponaxis&#39;, &#39;constraintext&#39;, &#39;customdata&#39;, &#39;customdatasrc&#39;, &#39;dx&#39;, &#39;dy&#39;, &#39;error_x&#39;, &#39;error_y&#39;, &#39;hoverinfo&#39;, &#39;hoverinfosrc&#39;, &#39;hoverlabel&#39;, &#39;hovertemplate&#39;, &#39;hovertemplatesrc&#39;, &#39;hovertext&#39;, &#39;hovertextsrc&#39;, &#39;ids&#39;, &#39;idssrc&#39;, &#39;insidetextanchor&#39;, &#39;insidetextfont&#39;, &#39;legendgroup&#39;, &#39;legendgrouptitle&#39;, &#39;legendrank&#39;, &#39;marker&#39;, &#39;meta&#39;, &#39;metasrc&#39;, &#39;name&#39;, &#39;offset&#39;, &#39;offsetgroup&#39;, &#39;offsetsrc&#39;, &#39;opacity&#39;, &#39;orientation&#39;, &#39;outsidetextfont&#39;, &#39;selected&#39;, &#39;selectedpoints&#39;, &#39;showlegend&#39;, &#39;stream&#39;, &#39;text&#39;, &#39;textangle&#39;, &#39;textfont&#39;, &#39;textposition&#39;, &#39;textpositionsrc&#39;, &#39;textsrc&#39;, &#39;texttemplate&#39;, &#39;texttemplatesrc&#39;, &#39;transforms&#39;, &#39;type&#39;, &#39;uid&#39;, &#39;uirevision&#39;, &#39;unselected&#39;, &#39;visible&#39;, &#39;width&#39;, &#39;widthsrc&#39;, &#39;x&#39;, &#39;x0&#39;, &#39;xaxis&#39;, &#39;xcalendar&#39;, &#39;xhoverformat&#39;, &#39;xperiod&#39;, &#39;xperiod0&#39;, &#39;xperiodalignment&#39;, &#39;xsrc&#39;, &#39;y&#39;, &#39;y0&#39;, &#39;yaxis&#39;, &#39;ycalendar&#39;, &#39;yhoverformat&#39;, &#39;yperiod&#39;, &#39;yperiod0&#39;, &#39;yperiodalignment&#39;, &#39;ysrc&#39;, &#39;key&#39;, &#39;set&#39;, &#39;frame&#39;, &#39;transforms&#39;, &#39;_isNestedKey&#39;, &#39;_isSimpleKey&#39;, &#39;_isGraticule&#39;, &#39;_bbox&#39; If you want to statistically test the absence of autocorrelation, the Durbin-Watson test can be of service. This tests checks the correlation between the values of a variable (including residuals) with itself at a previous time (i.e., a time lag of one), and comes with a test statistic and a p.Value (below .05 suggests autocorrelation). The test statistic ranges from zero to four with values below two suggesting positive autocorrelation, above two negative autocorrelation, and close to two no autocorrelation. Multiple functions can perform the Durbin-Watson test. Here I will use the durbinWatsonTest() function from the car() package as it implements a bootstrap resampling procedure which can be convenient, especially when you would have “smaller” samples. library(car) durbinWatsonTest(mymodel, reps=500) #&gt; lag Autocorrelation D-W Statistic p-value #&gt; 1 0.5506772 0.8983863 0 #&gt; Alternative hypothesis: rho != 0 6.5.1.5 (cheat) all-in-one to test a lot of The performance package with the check_model() function providing a visual check of the assumptions including the normal distribution of the residuals, homoscedasticity of residuals, and multicollinearity. mydata = read.csv(&quot;data_files/wineQualityReds.csv&quot;) mymodel = lm(pH ~ chlorides, data = mydata) library(performance) check_model(mymodel) 6.5.2 Assumptions: Multilevel linear regression Alright let’s take one step further and Move on to assumptions of multilevel (mixed-effects, hierarchical) linear regression models. These models rely on assumption that are similar to those we’ve seen earlier but they also their own assumptions. Like before we have the assumptions of: linear relation between predictor and outcome Normal distribution of residuals Homoscedasticity of residuals No autocorrelations of residuals No multicollinearity And the more unique ones including: 6.Normal distribution of the random effects 7. An appropriate Covariance structure The first five points can be tested like before, so nothing new. To inspect whether or not the random effects resemble a normal distribution, you can extract the random effects (i.e., the random intercepts and slopes) using the ranef() function and then use these to plot a histogram or quantile-quantile plot. Then the last one, an appropriate covariance structure. We could test whether models with a more complex covariance structure (e.g., models with random slopes) are preferred over simple ones (e.g., models with only a random intercept). We can compare simple and more complex models in an analysis of variance (conveniently the topic of the upcoming part). In the example below I compare a model with a random intercept with one including both random intercept and slope model1 = lmer(Reaction ~ Days + (1 | Subject), data = sleepstudy) model2 = lmer(Reaction ~ Days + (1 + Days | Subject), data = sleepstudy) anova(model1, model2) #&gt; refitting model(s) with ML (instead of REML) #&gt; Data: sleepstudy #&gt; Models: #&gt; model1: Reaction ~ Days + (1 | Subject) #&gt; model2: Reaction ~ Days + (1 + Days | Subject) #&gt; npar AIC BIC logLik deviance Chisq Df #&gt; model1 4 1802.1 1814.8 -897.04 1794.1 #&gt; model2 6 1763.9 1783.1 -875.97 1751.9 42.139 2 #&gt; Pr(&gt;Chisq) #&gt; model1 #&gt; model2 7.072e-10 *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 A significant test suggests that the more complex model is preferable, but I would advise to not base all decisions solely on p value significance. A more complex model could affect your statistical power and how results are interpret so consider this as well. 6.6 Effect sizes One aspect where p values fail is that these do not quantify the strength of relations. Therefore, it is handy to also report effect sizes that do give indications about the strength of associations. Here I will give a brief overview of effect sizes in the context of linear regression with continuous predictors. In the next part concerning the analysis of variance, I will briefly cover effect sizes appropriate in that context (e.g., eta-squared). 6.6.1 Standardized beta coefficients and R² Alright, we actually already discussed one type of effect size, the standardized beta coefficient. These coefficients allow to compare your predictors if these were measured on different scales. Moreover, they can help in interpreting your model outcomes. Suppose you have a standardized beta coefficient of 0.5 - this would mean that if your predictor increases by 1 standard deviation, the outcome would increase by 0.5 of a standard deviation. Remember that you can obtain these coefficients by including standardized variables in your regression models or by using functions such as the lm.beta() from the QuantPsyc package or **standardize_parameters() effectsize package. Just a quick reminder: # Load dataset mydata = mtcars # Fit a regression model mylm = lm(wt ~ hp + mpg, data = mydata) library(effectsize) standardize_parameters(mylm) #&gt; # Standardization method: refit #&gt; #&gt; Parameter | Std. Coef. | 95% CI #&gt; ----------------------------------------- #&gt; (Intercept) | 8.24e-17 | [-0.19, 0.19] #&gt; hp | -0.04 | [-0.34, 0.26] #&gt; mpg | -0.90 | [-1.20, -0.60] There is actually an effect size that we did not discuss yet in our summary output, the R². The R² reflects the proportion of variance in your outcome/dependent variable explained by the model (i.e., all the predictors in your model). As it turns out, it equates or approximates the square of the correlation between the predicted outcome (through linear regression) and the raw observed outcome from your dataset, hence the name. You can find R² in your summary output. summary(mylm) #&gt; #&gt; Call: #&gt; lm(formula = wt ~ hp + mpg, data = mydata) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.6097 -0.3261 -0.1417 0.3081 1.3873 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 6.2182863 0.7455994 8.340 3.42e-09 *** #&gt; hp -0.0005277 0.0020872 -0.253 0.802 #&gt; mpg -0.1455218 0.0237443 -6.129 1.12e-06 *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.5024 on 29 degrees of freedom #&gt; Multiple R-squared: 0.7534, Adjusted R-squared: 0.7364 #&gt; F-statistic: 44.29 on 2 and 29 DF, p-value: 1.529e-09 # Or call it directly from the output summary(mylm)$r.squared #&gt; [1] 0.7533765 # Which equates/resembles the square of the correlation between the predicted outcome and the raw outcome cor(predict(mylm), mydata$wt, use = &quot;complete.obs&quot;)^2 #&gt; [1] 0.7533765 However, the basic R² may be biased if you decide to include many predictors for whatever reason (i.e., it could overestimate the explained variance). In such case, you might want to shift your attention more towards the adjusted R² which can be also retrieved from the summary output. summary(mylm)$adj.r.squared # Adjusted #&gt; [1] 0.736368 the R² and its adjusted version concerns the joint contribution of each predictor. Now, what if we want to focus on the contribution of each individual predictor? For this purpose we can use squared semi-partial correlations (sr²) which reflects the proportion of variance in your outcome/dependent variable explained by a specific predictor. In other words, sr² shows the percentage of unique variance in the outcome by the predictor. library(effectsize) r2_semipartial(mylm,alternative = &quot;two.sided&quot;) #&gt; Term | sr2 | 95% CI #&gt; ------------------------------ #&gt; hp | 5.44e-04 | [0.00, 0.01] #&gt; mpg | 0.32 | [0.10, 0.54] 6.6.2 In case of mixed-effects models What about mixed-effects models (multilevel)? Here I will use the built-in sleepstudy dataset (from the lme4 package). library(lmerTest) set.seed(531) mydata = sleepstudy mydata$Predictor = rnorm(180, 100,25) # I also added an extra predictor mymixed = lmer(Reaction ~ Days + Predictor + (1 | Subject), data = mydata) Remember that in mixed-effects models we have a fixed and random part. Intercepts and slopes can be specified to vary for each cluster unit. Given this situation, effect sizes of fixed effects should also control or adjust for the random effects. Like before, we could opt for (partially) standardized beta coefficients to compare predictors. One detail here, your variables will be standardized using the standard deviation from the sample of each variable. Therefore, standard deviations can differ to some extent between variables. What about R²? Its computation is less straightforward given there are two residual terms. However, the r2() function from the performance package allows to compute it easily. library(performance) r2(mymixed) #&gt; # R2 for Mixed Models #&gt; #&gt; Conditional R2: 0.703 #&gt; Marginal R2: 0.279 We receive the conditional R² and the marginal R². The conditional R² reflects the explained variance from both the fixed and random effects. The marginal R² reflects how much variance of the conditional R² is attributed to the fixed effects. Moving on from the joint-contribution from all of our predictors to individual contribution. Semi-partial R² can be provided by the partR2() function from the partR2 package. Note that this function also computes bootstrapped confidence intervals. For demonstration purpose I ask for 5 bootstraps but in practice consider to set this much higher (e.g., over 500). library(partR2) #&gt; Warning: package &#39;partR2&#39; was built under R version 4.3.3 options(scipen=999) # part_r_squared = partR2(mymixed, partvars = c(&quot;Days&quot;,&quot;Predictor&quot;), nboot = 5 # Just for demonstration purpose, set this to a higher value in practice. ) summary(part_r_squared) #&gt; #&gt; #&gt; R2 (marginal) and 95% CI for the full model: #&gt; R2 CI_lower CI_upper ndf #&gt; 0.2795 0.2726 0.3418 3 #&gt; #&gt; ---------- #&gt; #&gt; Part (semi-partial) R2: #&gt; Predictor(s) R2 CI_lower CI_upper ndf #&gt; Model 0.2795 0.2726 0.3418 3 #&gt; Days 0.2792 0.2723 0.3413 2 #&gt; Predictor 0.0000 0.0016 0.0331 2 #&gt; Days+Predictor 0.2795 0.2726 0.3418 1 #&gt; #&gt; ---------- #&gt; #&gt; Inclusive R2 (SC^2 * R2): #&gt; Predictor IR2 CI_lower CI_upper #&gt; Days 0.2795 0.2724 0.3411 #&gt; Predictor 0.0002 0.0002 0.0019 #&gt; #&gt; ---------- #&gt; #&gt; Structure coefficients r(Yhat,x): #&gt; Predictor SC CI_lower CI_upper #&gt; Days 1.0000 0.9956 1.0000 #&gt; Predictor 0.0275 -0.0698 0.0556 #&gt; #&gt; ---------- #&gt; #&gt; Beta weights (standardised estimates) #&gt; Predictor BW CI_lower CI_upper #&gt; Days 0.5351 0.5287 0.5894 #&gt; Predictor 0.0052 -0.0487 0.0223 #&gt; #&gt; ---------- #&gt; #&gt; Parametric bootstrapping resulted in warnings or messages: #&gt; Check r2obj$boot_warnings and r2obj$boot_messages. A lot of output. Let’s start At the top, first we have the marginal R² with the 95% and the number of predictors plus the intercept. Then we have our semi-partial R². In our example, the unique variance of the days variable almost equates the full model. Third, we have the inclusive R² which shows the strength between the predictors and the model’s predicted values. If this value approximates that of the semi-partial R², then this indicates that it has mainly unique variance. If it is notably higher, the predictor shares some variance with other predictors. Fourth we have the structure coefficients. You may have predictors that show a “low” regression coefficient or semi-partial R², but these could be (statistically) relevant. A predictor showing a “high” structure coefficient with “low” semi-partial R², likely contributes indirectly through shared variance. In case the structure coefficient is “low” as well, then the predictor is likely not (strongly) related to the outcome. As a summary of the above. If the semi-partial R², inclusive semi-partial R², and structure coefficients are “high”, you have a predictor that shows a unique (direct) contribution to the total variance. If all but the semi-partial R² are “high”, the predictor shows little unique contribution but does nonetheless explains variance through shared correlations with others. If only the semi-partial R² is “high”, you have a predictor that uniquely contributes but is not correlated with others. Finally the last part of the output shows the standardized beta coefficients. "],["anova.html", "Chapter 7 Anova 7.1 Sum and Treatment Contrasts 7.2 One-way ANOVA 7.3 Two-way ANOVA 7.4 MANOVA 7.5 Referred article", " Chapter 7 Anova In linear regression the goal is to “forcefully” draw a straight line (the slope) by using the full range of your continuous predictor (your whole x-axis). But what if our predictor is a categorical variable, a variable that offers only a few discrete values, two, three, or a few categories? Instead of estimating a slope, the focus shifts to comparing group means, corresponding to the levels of your categorical predictors. This part will tackle a special case of linear regression, the analysis of variance, or ANOVA for short. ANOVA tests if the group means per categorical variable (e.g., the experimental vs control condition) differ statistically from one another. ANOVA does this by comparing the variance between each group to the variance within groups, hence the name. you can consider using ANOVA when all of your predictors are categorical and you want to see whether there exist statistically significant differences (based on p value) in the mean of the outcome per category. This part will all be about making contrasts, comparing groups to one another. We start with how contrasts are created to allow comparing groups. Two common ones, sum and treatment contrasts, will be discussed. Along the lines, I will tell you the story of dummy-coding Then we finally conduct our first ANOVA with one factor and multiple factors. From the omnibus test I will show some post-hoc group comparisons techniques, how to create your own contrasts using the linearhypothesis() function, and how to calculate some effect sizes. Finally, I will discuss multivariate ANOVA (MANOVA). You will learn how to create your own outcome contrast matrix to test specific “effects” and hypotheses. 7.1 Sum and Treatment Contrasts As I mentioned earlier, ANOVA is all about comparing groups by e.g., making contrasts between them. Knowledge about how R contrasts groups is important if we want to properly interpret the output of ANOVA and if we want to flexibly create our own contrasts and restrictions (as part of hypothesis testing). Before conducting a series of ANOVAs (and special types), I will first discuss two common types of contrasts: treatment contrasts and sum contrasts. When treatment contrasts are applied to your factor or categorical variables, the default in R, all of your categories will be compared to a reference category (i.e., shown as the intercept of your model). Say I have a dataset containing a factor variable with three categories The mean scores of these factors are 20,60, and 10, respectively. When you fit a regression model with the factor as the predictor, and you do not change the default treatment contrasts, R will take a reference category to which all other categories are contrasted to. set.seed(999) mydata = data.frame( score = c( rnorm(20,20,0.001), rnorm(20,60,0.001), rnorm(20,10,0.001) ), factor_variable = factor( rep(c(&quot; A&quot;,&quot; B&quot;, &quot; C&quot; ), each = 20 ) ) ) mymodel = lm(score ~ factor_variable, data=mydata) What category will be the reference? By default, R will select the reference category alphabetically or based on the lowest value if you have unordered factors, so it will choose category “A” here. If you want another reference category like “B”, you can use the relevel() function. mymodel = lm(score ~ relevel(factor_variable, ref=&quot; B&quot;), data = mydata) # Disclaimer. I will NOT run the above code. The reference category will still be &quot;A&quot; in the parts below Now, with treatment contrasts, category “B” (with an average score of 60) will be contrasted with the reference category “A” (20). Similarly, category “C” (10) will be contrasted with factor “A”. If I were to call for the coefficients, I can expect the values 20 (the reference), 40 (since factor B is, on average, 40 units higher than the reference), and - 10 (same reasoning). Let’s check. round( coef(mymodel), 0) #&gt; (Intercept) factor_variable B factor_variable C #&gt; 20 40 -10 You can also check how treatment contrasts handles our factor variable by running the contrasts() function. This can come in handy when I’ll later reveal how you can create your own contrasts and restrictions to test specific group comparisons. For clarity purposes, I will discuss the output of the contrasts() function in the section: Making your own specific hypotheses and contrasts. contrasts(mydata$factor_variable) #&gt; B C #&gt; A 0 0 #&gt; B 1 0 #&gt; C 0 1 Moving on to sum contrasts, these will compare (contrast) each category to the overall mean. If you use these type of contrasts then the sum of estimated coefficients will equal zero. As I discuss later on, this type of contrasts is required when conducting type III ANOVA. Recall that the means of our factors are 20,60, and 10, therefore the overall mean is 30. Now, the values of our coefficients will be different compared to the (default) threatment contrasts. For starters, the first coefficient will not be the mean of factor “A” but the overall mean (i.e., 30). The second coefficient will be difference between the first category “A” and the overall mean (i.e., 20-30 = -10). The value of the third and last coefficient will be the difference between the second category “B” and the overall mean (60-30 = 30). What about the coefficient of category “C”? It is hidden but we can easily uncover it. According to sum contrasts, the coefficient for the remaining category (“C”) is the value that makes the sum of the estimated coefficients (i.e., -10 and 30), zero. In other words you get the following equation: -10 + 30 + coefficient for factor “C” = 0. Just to be clear, the value of coefficient “C” is -20. All is good and well but how do we set sum contrasts? We can change contrasts both “locally” (per individual variable) and “globally” (for all variables at once). Note that you set contrasts before you fit your regression models. # &quot;Locally&quot; setting contrasts per variable contrasts(mydata$factor_variable) = contr.sum(3) # 3 since we this variable has three levels/categories # &quot;Globally&quot; setting contrasts that will apply to all variables options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.sum&quot;)) # the first &quot;contr.sum&quot; applies it to categorical variables # the second &quot;contr.sum&quot; applies it to ordered factor variables (ordered() instead of factor() ) Now you may be wondering, what contrasts to use? Without making your own custom contrasts (which is also a possibility), in my experience, the default treatment contrasts are fine and intuitive to interpret. However, as I will bring up later, if you want to test interaction effects, you might want to consider to set sum contrasts (when thinking about using a type III ANOVA). 7.1.1 Dummy-coding One small thing, you may have heard about dummy coding or that you need to dummy-code your variables. There might a bit of confusing surrounding this topic, so let me clarify. Dummy coding is often deemed synonymous with the aforementioned treatment contrasts. Technically, however, dummy coding can refer to any contrasts that use non-negative binary values. Other contrasts such as sum contrasts (with negative binary values) are not dummy coded. Personally, to avoid confusing, I prefer to use the terms treatment, sum, or “custom” coded and I would say something like “you should contrast code your variables” (then you still can decide how to). However, when you fit regression models using lm(), lmer(), or aov() (I will demonstrate this function later on), R will automatically create contrasts (i.e., contrast code your factors). In regression models and ANOVA, this will be the treatment contrasts by default. So within R you typically do not need to add contrast coded variables to your dataset for use in your ANOVAs. However, it still doesn’t hurt to do so if you want to share analysis scripts across software (Stata, SPSS, Mplus, and so on) and if you want to create your own special contrasts. I will quickly show you how to add contrast coded variables to your datasets. First, contrast coding identical to treatment contrasts (or “dummy-coding” as many people would call it). library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union set.seed(999) mydata = data.frame( score = c( rnorm(20,20,0.001), rnorm(20,60,0.001), rnorm(20,10,0.001) ), factor_variable = factor( rep(c(&quot; A&quot;,&quot; B&quot;, &quot; C&quot; ), each = 20 ) ) ) %&gt;% mutate( treatment_B = ifelse(factor_variable==&quot; B&quot;,1,0), treatment_C = ifelse(factor_variable==&quot; C&quot;,1,0) ) mymodel = lm(score ~ factor_variable, data = mydata) # To show that R automatically (by default) use treatment contrast coding mymodel_dummy = lm(score ~ treatment_B + treatment_C, data = mydata) # treatment_A is the reference so I can leave that out round( coef(mymodel), 0) #&gt; (Intercept) factor_variable1 factor_variable2 #&gt; 30 -10 30 round( coef(mymodel_dummy), 0) #&gt; (Intercept) treatment_B treatment_C #&gt; 20 40 -10 Sum coding. set.seed(999) library(dplyr) mydata = data.frame( score = c(rnorm(20,20,0.001), rnorm(20,60,0.001), rnorm(20,10,0.001)), factor_variable = factor(rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each = 20)) ) %&gt;% mutate( sum_A = ifelse(factor_variable == &quot;A&quot;, 1, ifelse(factor_variable == &quot;C&quot;, -1, 0)), sum_B = ifelse(factor_variable == &quot;B&quot;, 1, ifelse(factor_variable == &quot;C&quot;, -1, 0)) ) contrasts(mydata$factor_variable) = contr.sum(3) mymodel = lm(score ~ factor_variable, data = mydata) mymodel_contrast = lm(score ~ sum_A + sum_B, data = mydata) round(coef(mymodel), 0) #&gt; (Intercept) factor_variable1 factor_variable2 #&gt; 30 -10 30 round(coef(mymodel_contrast), 0) #&gt; (Intercept) sum_A sum_B #&gt; 30 -10 30 7.2 One-way ANOVA For now let’s leave the contrasts for a bit. They will come back later if I demonstrate how you can flexible compare groups. Moving on to actually conducting an ANOVA, starting off with the classic example of one categorical variable in a simple linear regression. I will use the build in iris dataset to test whether the average sepal length differs per species, per group so to speak. Before we conduct the ANOVA it never hurts to plot the outcome per group. For this purpose, I personally like to create violin plots library(pacman) p_load(dplyr, ggplot2) mydata = iris mydata %&gt;% ggplot(aes(x = Species, y = Sepal.Length, fill = Species)) + geom_violin(alpha=0.6) + geom_boxplot(alpha = 0.8) + stat_summary(fun = &quot;mean&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) To conduct the ANOVA, we can use the aov() function and enter a regression formula just like we’ve done before to compute regression models. Here you can choose whether you want to directly conduct an ANOVA or make a linear regression model first. One important detail, make sure that your predictor variables are factors as the aov() function requires this. # Feeding the regression formula directly to aov() myanova = aov(Sepal.Length ~ Species, data = mydata) summary(myanova) #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; Species 2 63.21 31.606 119.3 &lt;2e-16 *** #&gt; Residuals 147 38.96 0.265 #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Or compute a separate regression model first mymodel = lm(Sepal.Length ~ Species, data = mydata) myanova = aov(mymodel) summary(myanova) #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; Species 2 63.21 31.606 119.3 &lt;2e-16 *** #&gt; Residuals 147 38.96 0.265 #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Time for summary output. We receive the degrees of freedom, the sum of squares (i.e., total variance between the overall mean and the means per group), the mean of the sum of squares (i.e., the sum of squares divided by the degrees of freedom of each parameter, except for the residuals), F- and p values. The p value suggests that the mean in outcome of at least one group is statistically significantly different from the others. Indeed, the aov() function has conducted the omnibus test to test whether at least one group-comparison is different. Nice, but what groups are statistically significant different from each other? 7.2.1 Post-hoc group-comparisons There exist a couple of group-comparison methods to determine which pair of groups are statistically significantly different, after you have done the omnibus test (post-hoc). Let’s start with a popular one, the Tukey Honest Significant Differences test for multiple pairwise-comparisons of our groups (I know, a mouthful). This method is called through the TukeyHSD() function. This test can be recommended if you have equal sample size per group (and if e.g., the groups are sufficiently normal distributed). TukeyHSD(myanova) #&gt; Tukey multiple comparisons of means #&gt; 95% family-wise confidence level #&gt; #&gt; Fit: aov(formula = mymodel) #&gt; #&gt; $Species #&gt; diff lwr upr p adj #&gt; versicolor-setosa 0.930 0.6862273 1.1737727 0 #&gt; virginica-setosa 1.582 1.3382273 1.8257727 0 #&gt; virginica-versicolor 0.652 0.4082273 0.8957727 0 Conveniently, we receive the difference in sepal length, the p value per group comparison, and the corresponding 95% confidence interval. Note that the p values are adjusted for multiple comparisons. Other methods in context of one-way ANOVA include the Bonferroni correction(a classic), which could be ok if you have a small number of group comparisons (e.g., three) but but may be conservative otherwise. A less conservative post-hoc comparison test is the Holm correction and the Benjamini-Hochberg correction, which is even less conservative to the Bonferroni and Holm (which could be recommended if you have large sample and a lot of group comparisons). To do the above tests you use pairwise.t.test() function and specify the p value adjustment method. options(scipen=999) # prevents the scientific notation pairwise.t.test(mydata$Sepal.Length, mydata$Species, p.adj = &quot;bonf&quot;) # Outcome ; predictor ; adjustment method pairwise.t.test(mydata$Sepal.Length, mydata$Species, p.adj = &quot;holm&quot;) pairwise.t.test(mydata$Sepal.Length, mydata$Species, p.adj = &quot;BH&quot;) 7.2.2 Making your own specific hypotheses and contrasts. All of the above methods create their own group-comparisons by using contrasts (which I explained in detail at the start of this part). However, if desired, you can flexibly manipulate the contrasts, and make your very own group-comparisons for whatever differences you want to test with any restrictions or “special rule” you can imagine. In this part we will create combinations of specific group-comparisons that we we want to test separately or simultaneously. This will also yield knowledge relevant for special types of ANOVA (see later on). 1. Create a “contrast matrix” that will be made up with “contrast/restriction vectors”. These contrast matrices will reflect the specific group-comparisons that we want 2. Conduct the specific (or combinations of) group-comparison(s) using the linearhypothesis() function from the car package Before you create your own contrast matrix, it could be handy to have a look at the coefficients of your regression model. Indeed, we will work with the coefficients of our factors and specifically the order of coefficients. Let’s keep the model from before with the three species of flowers. mydata = iris mymodel = lm(Sepal.Length ~ Species, data = mydata) coef(mymodel) #&gt; (Intercept) Species1 Species2 #&gt; 5.84333333 -0.83733333 0.09266667 Note that the regression model used treatment contrasts as by default. I got three coefficients/categories: Setosa (the intercept, hence the “reference category” when using treatment contrasts), versicolor, and virginica. These categories/groups can be represented as a vector, a “contrast/restriction vector” so to speak, that looks like: c(category 1, category 2,categoy 3). Importantly, I can easily manipulate the categories within the matrix/restriction vector. You can “activate” and “deactivate” a category by asigning a “0” or “1” to it. For example, the vector (0,1,0) will “activate” “category 2” (i.e., the coefficient will be multiplied by 1) while “category 3” will be “deactivated” (since this coefficient will be multiplied with 0). The linearHypothesis() function will take this contrast/restriction matrix made up from contrast/restriction vectors (in our example binded by the rbind function) and read it as: test whether the coefficient/ the average of “category 2” (versicolor) is different from zero. Remember, we work with coefficients (differences with the reference category) so different from zero actually means different from the average of the reference category (setosa flowers). restriction = rbind( c(0,1,0) ) library(car) #&gt; Loading required package: carData #&gt; #&gt; Attaching package: &#39;car&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; recode linearHypothesis(mymodel, hypothesis.matrix = restriction, test=&quot;F&quot;) #&gt; Linear hypothesis test #&gt; #&gt; Hypothesis: #&gt; Species1 = 0 #&gt; #&gt; Model 1: restricted model #&gt; Model 2: Sepal.Length ~ Species #&gt; #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 148 91.541 #&gt; 2 147 38.956 1 52.585 198.43 &lt; 2.2e-16 *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can also combine multiple “contrast/restriction” vectors to a create a restriction matrix that tests multiple group-comparisons simultaneously. For example, we can recreate the omnibus test based on a treatment contrast style . restriction = rbind( c(0,1,0), c(0,0,1) ) library(car) linearHypothesis(mymodel, hypothesis.matrix = restriction, test=&quot;F&quot;) #&gt; Linear hypothesis test #&gt; #&gt; Hypothesis: #&gt; Species1 = 0 #&gt; Species2 = 0 #&gt; #&gt; Model 1: restricted model #&gt; Model 2: Sepal.Length ~ Species #&gt; #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 149 102.168 #&gt; 2 147 38.956 2 63.212 119.26 &lt; 2.2e-16 *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Another thing that we can do is to make contrasts between groups (similar to sum contrasts). Have a look at this vector: (0,1,-1). We “activate category 2” but now we asign a negative value to “category 3”. This can be interpret as a contrast between “category 2” and “category 3, so does the average of versicolor differ from virginica? Let’s find out: restriction = rbind( c(0,1,-1) ) linearHypothesis(mymodel, hypothesis.matrix = restriction, test=&quot;F&quot;) #&gt; Linear hypothesis test #&gt; #&gt; Hypothesis: #&gt; Species1 - Species2 = 0 #&gt; #&gt; Model 1: restricted model #&gt; Model 2: Sepal.Length ~ Species #&gt; #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 148 60.579 #&gt; 2 147 38.956 1 21.622 81.592 8.77e-16 *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.2.2.1 Let us get creative with hypothesis testing Alright, I promised that we can get creative with hypothesis testing did I not? Well, to give a couple of examples, suppose, for some reason, that we want to test whether the category “versicolor” is five times larger than the reference category “setosa”. We already know that by default R uses treatment contrasts and that setosa will be the reference category (since “s” comes first in the alphabet). Now for the restriction/contrast vector, did you thought of using something like: restriction = rbind( c(0,5,0) ) Well, I get why you would think so but this would test whether the coefficient (i.e., the difference in mean between versicolor and setosa) of the versicolor group times five is zero. This is not the same as testing whether the mean for versicolor is five times that of the reference setosa Instead, to get the “a specific group (and not a difference score) is five time larger than…” part, we can use the rhs argument. If we set a prespecified value to rhs, it will test whether a linear combination of the coefficients equals your prespecified value. Therefore, if we “activate” versicolor, “deactivate” virginica, and set to rhs to be the coefficient for versicolor times five, we will test whether versicolor is five times larger than the reference. restriction = rbind( c(0,1,0) ) library(car) linearHypothesis(mymodel, hypothesis.matrix = restriction, rhs = coef(mymodel)[1]*5, test=&quot;F&quot;) # with coef(mymodel)[1] being the versicolor category #&gt; Linear hypothesis test #&gt; #&gt; Hypothesis: #&gt; Species1 = 29.2166666666667 #&gt; #&gt; Model 1: restricted model #&gt; Model 2: Sepal.Length ~ Species #&gt; #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 148 67782 #&gt; 2 147 39 1 67743 255627 &lt; 2.2e-16 *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Another example, suppose I want to test whether the sum of the coefficients of “versicolor” and “virginica” is 10. In this case we need to use both these categories so we will need to activate both of them. Remember that the rhs argument allows to test whether the linear combination equals a value. Therefore, if we activate “versicolor”, activate “virginica”, and set rhs to 10, we will test whether the sum of these coefficients is 10. restriction = rbind( c(0,1,1) ) library(car) linearHypothesis(mymodel, hypothesis.matrix = restriction, test=&quot;F&quot;, rhs = 10) #&gt; Linear hypothesis test #&gt; #&gt; Hypothesis: #&gt; Species1 + Species2 = 10 #&gt; #&gt; Model 1: restricted model #&gt; Model 2: Sepal.Length ~ Species #&gt; #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 148 8697.5 #&gt; 2 147 39.0 1 8658.6 32673 &lt; 2.2e-16 *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.2.3 Effect sizes Continuing the spirit from the last part, it is useful to include effect sizes next to the common F statistics obtained from your output. Several effect sizes are appropriate to use in the context of ANOVA. Let’s start with perhaps the most known effect size Cohen’s d which is the difference between the group means divided by the pooled standard deviations of the groups. We can use the cohens_d() function from our old acquaintance, the effectsize package, to compute this effect size. Before you use this function, make sure you (temporarily) remove categories until you have two groups as an error is outputted otherwise. library(dplyr) library(effectsize) mydata = iris mydata_temp = mydata %&gt;% filter(!Species==&quot;virginica&quot;) # the cohens_d functions asks for exactly two groups, hence I temporarily filter out &quot;virginica&quot; as an example cohens_d(Sepal.Length ~ Species, data = mydata_temp) #&gt; Cohen&#39;s d | 95% CI #&gt; -------------------------- #&gt; -2.10 | [-2.59, -1.61] #&gt; #&gt; - Estimated using pooled SD. Moving on. Similar to the discussed semi-partial R² in linear regressions (with continuous predictors), in the ANOVA context, we have the (partial) eta-squared and the omega-squared measure. The eta-squared measures shows the proportion of total variance explained by your (one) factor variable whereas the partial eta-squared shows the proportion of variance explained by a specific factor after controlling for others. However, as (partial) eta-squared is biased and with this bias decreasing the more the size of your groups increases, you could consider the omega-square measure instead (especially in “small” sample sizes). To compute these effect sizes, you can consider The eta_squared() and omega_squared() function from the effectsize package. library(effectsize) myanova = aov(Sepal.Length ~ Species, data = iris) eta_squared(myanova) #&gt; # Effect Size for ANOVA #&gt; #&gt; Parameter | Eta2 | 95% CI #&gt; ------------------------------- #&gt; Species | 0.62 | [0.54, 1.00] #&gt; #&gt; - One-sided CIs: upper bound fixed at [1.00]. omega_squared(myanova) #&gt; # Effect Size for ANOVA #&gt; #&gt; Parameter | Omega2 | 95% CI #&gt; --------------------------------- #&gt; Species | 0.61 | [0.53, 1.00] #&gt; #&gt; - One-sided CIs: upper bound fixed at [1.00]. One thing I want to notice. Recently, the use eta-squared in ANOVA from a mixed-effects model has been criticized (click here). As discussed in the previous part, effect sizes in mixed-effects models should acknowledge that there is a fixed and random effects part. If you want to compute the (partial) eta-squared in mixed-effects models that include a 2 x 2 design (i.e., two categorical predictors with two levels), It is recommended to use the sum contrast coding and to use the cluster-mean centered outcome variables to obtain eta-squared within. Full details are provided in the linked article, the reference to this article is shown at the end of this part. 7.3 Two-way ANOVA Let’s add an extra categorical variable to the equation. Suppose we also included the location of the flower (location “a”,“b”,…“e”). Now we can test the main effect of species and location on sepal length as well the interaction effect between these two predictors. mydata = iris mydata$Location = factor(rep(c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;), each = 10)) mydata %&gt;% ggplot(aes(x = Species, y = Sepal.Length, fill = Species)) + geom_violin(alpha=0.6) + geom_boxplot(alpha = 0.8) + stat_summary(fun = &quot;mean&quot;) + facet_wrap(~Location) + theme_classic() + theme(legend.position = &quot;none&quot;) mymodel = lm(Sepal.Length ~ Species * Location, data = mydata) coef(mymodel) #&gt; (Intercept) Species1 Species2 #&gt; 5.843333e+00 -8.373333e-01 9.266667e-02 #&gt; Location1 Location2 Location3 #&gt; 4.658198e-17 2.666667e-02 1.233333e-01 #&gt; Location4 Species1:Location1 Species2:Location1 #&gt; 3.666667e-02 -1.460000e-01 1.640000e-01 #&gt; Species1:Location2 Species2:Location2 Species1:Location3 #&gt; 1.773333e-01 -1.126667e-01 -1.193333e-01 #&gt; Species2:Location3 Species1:Location4 Species2:Location4 #&gt; 2.006667e-01 2.733333e-02 -1.426667e-01 Since we used the default treatment contrasts, note that our “reference category” consists of two variables: the setosa flower measured on location “a”. The Coefficient versicolor shows the difference between itself and the reference flower setosa at the reference location “a”. Similarly, the coefficient “c” shows the difference between itself and the reference location “a” in the reference flower setosa. If we look at the coefficients for interaction terms, say virginica and location d, then it shows how the difference between virginica and setosa, differs at location “c” compared to location “a”. Since we have more factors and an interaction effect, a novel decision will pop up. What type of ANOVA do we want to use? 7.3.1 Four different types of ANOVA Since we introduced an interaction effect to our regression models and ANOVA, it may be a good time to acknowledge the existence of different types of ANOVA. The aov() function we used so far computes the type I ANOVA which tests each factor in the order you specified in the model (handy if the order of factors is crucial). Next to type I, you also have type II, III, and IV. The type II ANOVA tests each main effect while adjusting for other main effects (however it ignores all interactions so not suitable when interactions are included). Type III tests each of yours factors accounting for all others, including interactions. If you consider to go for type III (since you want to include interactions), use sum contrasts instead of the default treatment contrasts. Finally, type IV, could be considered when you have complex models with categories or category combinations that are empty (no data). To access type II to IV, you can use the Anova() function from the car package. # Type II ANOVA mymodel = lm(Sepal.Length ~ Species * Location, data = mydata) Anova(mymodel, type=&quot;II&quot;) # the type ignoring the interaction #&gt; Anova Table (Type II tests) #&gt; #&gt; Response: Sepal.Length #&gt; Sum Sq Df F value Pr(&gt;F) #&gt; Species 63.212 2 120.9107 &lt;2e-16 *** #&gt; Location 1.563 4 1.4952 0.2071 #&gt; Species:Location 2.104 8 1.0061 0.4345 #&gt; Residuals 35.289 135 #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Type III ANOVA options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.sum&quot;)) Anova(mymodel, type=&quot;III&quot;) # the type you should use with sum contrasts, see above #&gt; Anova Table (Type III tests) #&gt; #&gt; Response: Sepal.Length #&gt; Sum Sq Df F value Pr(&gt;F) #&gt; (Intercept) 5121.7 1 19593.2734 &lt;2e-16 *** #&gt; Species 63.2 2 120.9107 &lt;2e-16 *** #&gt; Location 1.6 4 1.4952 0.2071 #&gt; Species:Location 2.1 8 1.0061 0.4345 #&gt; Residuals 35.3 135 #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The difference between type II and type III ANOVA will depend on how “strong” the interaction effect in your models will be. For the remainder of the examples I will use type III as it is a flexible approach (i.e., it has no strong assumptions or limitations). Irrespective of your choice of ANOVA type, the model will conduct an omnibus test. Therefore, once again, we need to dive deeper into individual post-hoc group-comparisons. 7.3.2 Two-way ANOVA: Post-hoc comparisons Instead of the pairwise t tests that we did with one-way ANOVA, I prefer to use the emmeans() function from the emmeans package. This function allows to do comparisons in the main effects: comparing the petal length of species with one another per location and comparing location with one another per species, and interaction effects: compare the difference in petal length of species across different locations. Similar to the pairwise.t.test() function, we can choose how we want to adjust the p value (Bonferroni, Benjamini-Hochberg, etc.). Now you could approach the group-comparisons in different ways. For example, you could check whether the interaction effect is statistically significant in your regression model or ANOVA (or if you deem it yourself sufficiently significant). If so, you can zoom in how the mean of sepal length differs per species but within each location, and how it differs per location within each species. That way, when checking the main effects, you involve the other variable. In addition, you could look more to the interaction effect to check whether combinations between species and locations differ between all other combinations mydata = iris mydata$Location = factor(rep(c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;), each = 10)) mymodel = lm(Sepal.Length ~ Species * Location, data = mydata) library(emmeans) emmeans(mymodel, pairwise ~ Location | Species, adjust = &quot;Tukey&quot;) # Main effect: differences between locations WITHIN species #&gt; $emmeans #&gt; Species = setosa: #&gt; Location emmean SE df lower.CL upper.CL #&gt; a 4.86 0.162 135 4.54 5.18 #&gt; b 5.21 0.162 135 4.89 5.53 #&gt; c 5.01 0.162 135 4.69 5.33 #&gt; d 5.07 0.162 135 4.75 5.39 #&gt; e 4.88 0.162 135 4.56 5.20 #&gt; #&gt; Species = versicolor: #&gt; Location emmean SE df lower.CL upper.CL #&gt; a 6.10 0.162 135 5.78 6.42 #&gt; b 5.85 0.162 135 5.53 6.17 #&gt; c 6.26 0.162 135 5.94 6.58 #&gt; d 5.83 0.162 135 5.51 6.15 #&gt; e 5.64 0.162 135 5.32 5.96 #&gt; #&gt; Species = virginica: #&gt; Location emmean SE df lower.CL upper.CL #&gt; a 6.57 0.162 135 6.25 6.89 #&gt; b 6.55 0.162 135 6.23 6.87 #&gt; c 6.63 0.162 135 6.31 6.95 #&gt; d 6.74 0.162 135 6.42 7.06 #&gt; e 6.45 0.162 135 6.13 6.77 #&gt; #&gt; Confidence level used: 0.95 #&gt; #&gt; $contrasts #&gt; Species = setosa: #&gt; contrast estimate SE df t.ratio p.value #&gt; a - b -0.35 0.229 135 -1.531 0.5444 #&gt; a - c -0.15 0.229 135 -0.656 0.9652 #&gt; a - d -0.21 0.229 135 -0.918 0.8894 #&gt; a - e -0.02 0.229 135 -0.087 1.0000 #&gt; b - c 0.20 0.229 135 0.875 0.9057 #&gt; b - d 0.14 0.229 135 0.612 0.9729 #&gt; b - e 0.33 0.229 135 1.443 0.6009 #&gt; c - d -0.06 0.229 135 -0.262 0.9989 #&gt; c - e 0.13 0.229 135 0.569 0.9794 #&gt; d - e 0.19 0.229 135 0.831 0.9206 #&gt; #&gt; Species = versicolor: #&gt; contrast estimate SE df t.ratio p.value #&gt; a - b 0.25 0.229 135 1.093 0.8097 #&gt; a - c -0.16 0.229 135 -0.700 0.9562 #&gt; a - d 0.27 0.229 135 1.181 0.7623 #&gt; a - e 0.46 0.229 135 2.012 0.2660 #&gt; b - c -0.41 0.229 135 -1.793 0.3818 #&gt; b - d 0.02 0.229 135 0.087 1.0000 #&gt; b - e 0.21 0.229 135 0.918 0.8894 #&gt; c - d 0.43 0.229 135 1.881 0.3328 #&gt; c - e 0.62 0.229 135 2.712 0.0574 #&gt; d - e 0.19 0.229 135 0.831 0.9206 #&gt; #&gt; Species = virginica: #&gt; contrast estimate SE df t.ratio p.value #&gt; a - b 0.02 0.229 135 0.087 1.0000 #&gt; a - c -0.06 0.229 135 -0.262 0.9989 #&gt; a - d -0.17 0.229 135 -0.744 0.9458 #&gt; a - e 0.12 0.229 135 0.525 0.9847 #&gt; b - c -0.08 0.229 135 -0.350 0.9968 #&gt; b - d -0.19 0.229 135 -0.831 0.9206 #&gt; b - e 0.10 0.229 135 0.437 0.9923 #&gt; c - d -0.11 0.229 135 -0.481 0.9890 #&gt; c - e 0.18 0.229 135 0.787 0.9339 #&gt; d - e 0.29 0.229 135 1.268 0.7110 #&gt; #&gt; P value adjustment: tukey method for comparing a family of 5 estimates emmeans(mymodel, pairwise ~ Species | Location, adjust = &quot;Bonferroni&quot;) #&gt; $emmeans #&gt; Location = a: #&gt; Species emmean SE df lower.CL upper.CL #&gt; setosa 4.86 0.162 135 4.54 5.18 #&gt; versicolor 6.10 0.162 135 5.78 6.42 #&gt; virginica 6.57 0.162 135 6.25 6.89 #&gt; #&gt; Location = b: #&gt; Species emmean SE df lower.CL upper.CL #&gt; setosa 5.21 0.162 135 4.89 5.53 #&gt; versicolor 5.85 0.162 135 5.53 6.17 #&gt; virginica 6.55 0.162 135 6.23 6.87 #&gt; #&gt; Location = c: #&gt; Species emmean SE df lower.CL upper.CL #&gt; setosa 5.01 0.162 135 4.69 5.33 #&gt; versicolor 6.26 0.162 135 5.94 6.58 #&gt; virginica 6.63 0.162 135 6.31 6.95 #&gt; #&gt; Location = d: #&gt; Species emmean SE df lower.CL upper.CL #&gt; setosa 5.07 0.162 135 4.75 5.39 #&gt; versicolor 5.83 0.162 135 5.51 6.15 #&gt; virginica 6.74 0.162 135 6.42 7.06 #&gt; #&gt; Location = e: #&gt; Species emmean SE df lower.CL upper.CL #&gt; setosa 4.88 0.162 135 4.56 5.20 #&gt; versicolor 5.64 0.162 135 5.32 5.96 #&gt; virginica 6.45 0.162 135 6.13 6.77 #&gt; #&gt; Confidence level used: 0.95 #&gt; #&gt; $contrasts #&gt; Location = a: #&gt; contrast estimate SE df t.ratio p.value #&gt; setosa - versicolor -1.24 0.229 135 -5.423 &lt;.0001 #&gt; setosa - virginica -1.71 0.229 135 -7.479 &lt;.0001 #&gt; versicolor - virginica -0.47 0.229 135 -2.056 0.1253 #&gt; #&gt; Location = b: #&gt; contrast estimate SE df t.ratio p.value #&gt; setosa - versicolor -0.64 0.229 135 -2.799 0.0176 #&gt; setosa - virginica -1.34 0.229 135 -5.861 &lt;.0001 #&gt; versicolor - virginica -0.70 0.229 135 -3.061 0.0080 #&gt; #&gt; Location = c: #&gt; contrast estimate SE df t.ratio p.value #&gt; setosa - versicolor -1.25 0.229 135 -5.467 &lt;.0001 #&gt; setosa - virginica -1.62 0.229 135 -7.085 &lt;.0001 #&gt; versicolor - virginica -0.37 0.229 135 -1.618 0.3239 #&gt; #&gt; Location = d: #&gt; contrast estimate SE df t.ratio p.value #&gt; setosa - versicolor -0.76 0.229 135 -3.324 0.0034 #&gt; setosa - virginica -1.67 0.229 135 -7.304 &lt;.0001 #&gt; versicolor - virginica -0.91 0.229 135 -3.980 0.0003 #&gt; #&gt; Location = e: #&gt; contrast estimate SE df t.ratio p.value #&gt; setosa - versicolor -0.76 0.229 135 -3.324 0.0034 #&gt; setosa - virginica -1.57 0.229 135 -6.866 &lt;.0001 #&gt; versicolor - virginica -0.81 0.229 135 -3.543 0.0016 #&gt; #&gt; P value adjustment: bonferroni method for 3 tests emmeans(mymodel, pairwise ~ Species * Location, adjust = &quot;BH&quot;) # interaction effect: comparing combinations to all other combinations #&gt; $emmeans #&gt; Species Location emmean SE df lower.CL upper.CL #&gt; setosa a 4.86 0.162 135 4.54 5.18 #&gt; versicolor a 6.10 0.162 135 5.78 6.42 #&gt; virginica a 6.57 0.162 135 6.25 6.89 #&gt; setosa b 5.21 0.162 135 4.89 5.53 #&gt; versicolor b 5.85 0.162 135 5.53 6.17 #&gt; virginica b 6.55 0.162 135 6.23 6.87 #&gt; setosa c 5.01 0.162 135 4.69 5.33 #&gt; versicolor c 6.26 0.162 135 5.94 6.58 #&gt; virginica c 6.63 0.162 135 6.31 6.95 #&gt; setosa d 5.07 0.162 135 4.75 5.39 #&gt; versicolor d 5.83 0.162 135 5.51 6.15 #&gt; virginica d 6.74 0.162 135 6.42 7.06 #&gt; setosa e 4.88 0.162 135 4.56 5.20 #&gt; versicolor e 5.64 0.162 135 5.32 5.96 #&gt; virginica e 6.45 0.162 135 6.13 6.77 #&gt; #&gt; Confidence level used: 0.95 #&gt; #&gt; $contrasts #&gt; contrast estimate SE df t.ratio #&gt; setosa a - versicolor a -1.24 0.229 135 -5.423 #&gt; setosa a - virginica a -1.71 0.229 135 -7.479 #&gt; setosa a - setosa b -0.35 0.229 135 -1.531 #&gt; setosa a - versicolor b -0.99 0.229 135 -4.330 #&gt; setosa a - virginica b -1.69 0.229 135 -7.391 #&gt; setosa a - setosa c -0.15 0.229 135 -0.656 #&gt; setosa a - versicolor c -1.40 0.229 135 -6.123 #&gt; setosa a - virginica c -1.77 0.229 135 -7.741 #&gt; setosa a - setosa d -0.21 0.229 135 -0.918 #&gt; setosa a - versicolor d -0.97 0.229 135 -4.242 #&gt; setosa a - virginica d -1.88 0.229 135 -8.222 #&gt; setosa a - setosa e -0.02 0.229 135 -0.087 #&gt; setosa a - versicolor e -0.78 0.229 135 -3.411 #&gt; setosa a - virginica e -1.59 0.229 135 -6.954 #&gt; versicolor a - virginica a -0.47 0.229 135 -2.056 #&gt; versicolor a - setosa b 0.89 0.229 135 3.892 #&gt; versicolor a - versicolor b 0.25 0.229 135 1.093 #&gt; versicolor a - virginica b -0.45 0.229 135 -1.968 #&gt; versicolor a - setosa c 1.09 0.229 135 4.767 #&gt; versicolor a - versicolor c -0.16 0.229 135 -0.700 #&gt; versicolor a - virginica c -0.53 0.229 135 -2.318 #&gt; versicolor a - setosa d 1.03 0.229 135 4.505 #&gt; versicolor a - versicolor d 0.27 0.229 135 1.181 #&gt; versicolor a - virginica d -0.64 0.229 135 -2.799 #&gt; versicolor a - setosa e 1.22 0.229 135 5.336 #&gt; versicolor a - versicolor e 0.46 0.229 135 2.012 #&gt; versicolor a - virginica e -0.35 0.229 135 -1.531 #&gt; virginica a - setosa b 1.36 0.229 135 5.948 #&gt; virginica a - versicolor b 0.72 0.229 135 3.149 #&gt; virginica a - virginica b 0.02 0.229 135 0.087 #&gt; virginica a - setosa c 1.56 0.229 135 6.823 #&gt; virginica a - versicolor c 0.31 0.229 135 1.356 #&gt; virginica a - virginica c -0.06 0.229 135 -0.262 #&gt; virginica a - setosa d 1.50 0.229 135 6.560 #&gt; virginica a - versicolor d 0.74 0.229 135 3.236 #&gt; virginica a - virginica d -0.17 0.229 135 -0.744 #&gt; virginica a - setosa e 1.69 0.229 135 7.391 #&gt; virginica a - versicolor e 0.93 0.229 135 4.067 #&gt; virginica a - virginica e 0.12 0.229 135 0.525 #&gt; setosa b - versicolor b -0.64 0.229 135 -2.799 #&gt; setosa b - virginica b -1.34 0.229 135 -5.861 #&gt; setosa b - setosa c 0.20 0.229 135 0.875 #&gt; setosa b - versicolor c -1.05 0.229 135 -4.592 #&gt; setosa b - virginica c -1.42 0.229 135 -6.210 #&gt; setosa b - setosa d 0.14 0.229 135 0.612 #&gt; setosa b - versicolor d -0.62 0.229 135 -2.712 #&gt; setosa b - virginica d -1.53 0.229 135 -6.692 #&gt; setosa b - setosa e 0.33 0.229 135 1.443 #&gt; setosa b - versicolor e -0.43 0.229 135 -1.881 #&gt; setosa b - virginica e -1.24 0.229 135 -5.423 #&gt; versicolor b - virginica b -0.70 0.229 135 -3.061 #&gt; versicolor b - setosa c 0.84 0.229 135 3.674 #&gt; versicolor b - versicolor c -0.41 0.229 135 -1.793 #&gt; versicolor b - virginica c -0.78 0.229 135 -3.411 #&gt; versicolor b - setosa d 0.78 0.229 135 3.411 #&gt; versicolor b - versicolor d 0.02 0.229 135 0.087 #&gt; versicolor b - virginica d -0.89 0.229 135 -3.892 #&gt; versicolor b - setosa e 0.97 0.229 135 4.242 #&gt; versicolor b - versicolor e 0.21 0.229 135 0.918 #&gt; versicolor b - virginica e -0.60 0.229 135 -2.624 #&gt; virginica b - setosa c 1.54 0.229 135 6.735 #&gt; virginica b - versicolor c 0.29 0.229 135 1.268 #&gt; virginica b - virginica c -0.08 0.229 135 -0.350 #&gt; virginica b - setosa d 1.48 0.229 135 6.473 #&gt; virginica b - versicolor d 0.72 0.229 135 3.149 #&gt; virginica b - virginica d -0.19 0.229 135 -0.831 #&gt; virginica b - setosa e 1.67 0.229 135 7.304 #&gt; virginica b - versicolor e 0.91 0.229 135 3.980 #&gt; virginica b - virginica e 0.10 0.229 135 0.437 #&gt; setosa c - versicolor c -1.25 0.229 135 -5.467 #&gt; setosa c - virginica c -1.62 0.229 135 -7.085 #&gt; setosa c - setosa d -0.06 0.229 135 -0.262 #&gt; setosa c - versicolor d -0.82 0.229 135 -3.586 #&gt; setosa c - virginica d -1.73 0.229 135 -7.566 #&gt; setosa c - setosa e 0.13 0.229 135 0.569 #&gt; setosa c - versicolor e -0.63 0.229 135 -2.755 #&gt; setosa c - virginica e -1.44 0.229 135 -6.298 #&gt; versicolor c - virginica c -0.37 0.229 135 -1.618 #&gt; versicolor c - setosa d 1.19 0.229 135 5.205 #&gt; versicolor c - versicolor d 0.43 0.229 135 1.881 #&gt; versicolor c - virginica d -0.48 0.229 135 -2.099 #&gt; versicolor c - setosa e 1.38 0.229 135 6.035 #&gt; versicolor c - versicolor e 0.62 0.229 135 2.712 #&gt; versicolor c - virginica e -0.19 0.229 135 -0.831 #&gt; virginica c - setosa d 1.56 0.229 135 6.823 #&gt; virginica c - versicolor d 0.80 0.229 135 3.499 #&gt; virginica c - virginica d -0.11 0.229 135 -0.481 #&gt; virginica c - setosa e 1.75 0.229 135 7.654 #&gt; virginica c - versicolor e 0.99 0.229 135 4.330 #&gt; virginica c - virginica e 0.18 0.229 135 0.787 #&gt; setosa d - versicolor d -0.76 0.229 135 -3.324 #&gt; setosa d - virginica d -1.67 0.229 135 -7.304 #&gt; setosa d - setosa e 0.19 0.229 135 0.831 #&gt; setosa d - versicolor e -0.57 0.229 135 -2.493 #&gt; setosa d - virginica e -1.38 0.229 135 -6.035 #&gt; versicolor d - virginica d -0.91 0.229 135 -3.980 #&gt; versicolor d - setosa e 0.95 0.229 135 4.155 #&gt; versicolor d - versicolor e 0.19 0.229 135 0.831 #&gt; versicolor d - virginica e -0.62 0.229 135 -2.712 #&gt; virginica d - setosa e 1.86 0.229 135 8.135 #&gt; virginica d - versicolor e 1.10 0.229 135 4.811 #&gt; virginica d - virginica e 0.29 0.229 135 1.268 #&gt; setosa e - versicolor e -0.76 0.229 135 -3.324 #&gt; setosa e - virginica e -1.57 0.229 135 -6.866 #&gt; versicolor e - virginica e -0.81 0.229 135 -3.543 #&gt; p.value #&gt; &lt;.0001 #&gt; &lt;.0001 #&gt; 0.1748 #&gt; 0.0001 #&gt; &lt;.0001 #&gt; 0.5729 #&gt; &lt;.0001 #&gt; &lt;.0001 #&gt; 0.4447 #&gt; 0.0001 #&gt; &lt;.0001 #&gt; 0.9304 #&gt; 0.0017 #&gt; &lt;.0001 #&gt; 0.0635 #&gt; 0.0004 #&gt; 0.3494 #&gt; 0.0756 #&gt; &lt;.0001 #&gt; 0.5479 #&gt; 0.0344 #&gt; &lt;.0001 #&gt; 0.3070 #&gt; 0.0103 #&gt; &lt;.0001 #&gt; 0.0693 #&gt; 0.1748 #&gt; &lt;.0001 #&gt; 0.0037 #&gt; 0.9304 #&gt; &lt;.0001 #&gt; 0.2358 #&gt; 0.8167 #&gt; &lt;.0001 #&gt; 0.0029 #&gt; 0.5233 #&gt; &lt;.0001 #&gt; 0.0002 #&gt; 0.6501 #&gt; 0.0103 #&gt; &lt;.0001 #&gt; 0.4680 #&gt; &lt;.0001 #&gt; &lt;.0001 #&gt; 0.5984 #&gt; 0.0124 #&gt; &lt;.0001 #&gt; 0.2036 #&gt; 0.0894 #&gt; &lt;.0001 #&gt; 0.0048 #&gt; 0.0008 #&gt; 0.1067 #&gt; 0.0017 #&gt; 0.0017 #&gt; 0.9304 #&gt; 0.0004 #&gt; 0.0001 #&gt; 0.4447 #&gt; 0.0156 #&gt; &lt;.0001 #&gt; 0.2682 #&gt; 0.7633 #&gt; &lt;.0001 #&gt; 0.0037 #&gt; 0.4754 #&gt; &lt;.0001 #&gt; 0.0003 #&gt; 0.7027 #&gt; &lt;.0001 #&gt; &lt;.0001 #&gt; 0.8167 #&gt; 0.0010 #&gt; &lt;.0001 #&gt; 0.6241 #&gt; 0.0115 #&gt; &lt;.0001 #&gt; 0.1511 #&gt; &lt;.0001 #&gt; 0.0894 #&gt; 0.0581 #&gt; &lt;.0001 #&gt; 0.0124 #&gt; 0.4754 #&gt; &lt;.0001 #&gt; 0.0014 #&gt; 0.6763 #&gt; &lt;.0001 #&gt; 0.0001 #&gt; 0.4991 #&gt; 0.0022 #&gt; &lt;.0001 #&gt; 0.4754 #&gt; 0.0221 #&gt; &lt;.0001 #&gt; 0.0003 #&gt; 0.0002 #&gt; 0.4754 #&gt; 0.0124 #&gt; &lt;.0001 #&gt; &lt;.0001 #&gt; 0.2682 #&gt; 0.0022 #&gt; &lt;.0001 #&gt; 0.0012 #&gt; #&gt; P value adjustment: BH method for 105 tests Instead, if you deem the interaction effect “not significant”, you could also decide to skip the interaction effect and to keep the main effects separate of the other variable that is, you don’t look at species within locations or locations within different species but across locations or species. emmeans(mymodel, pairwise ~ Species, adjust = &quot;Holm&quot;) #&gt; $emmeans #&gt; Species emmean SE df lower.CL upper.CL #&gt; setosa 5.01 0.0723 135 4.86 5.15 #&gt; versicolor 5.94 0.0723 135 5.79 6.08 #&gt; virginica 6.59 0.0723 135 6.45 6.73 #&gt; #&gt; Results are averaged over the levels of: Location #&gt; Confidence level used: 0.95 #&gt; #&gt; $contrasts #&gt; contrast estimate SE df t.ratio p.value #&gt; setosa - versicolor -0.930 0.102 135 -9.095 &lt;.0001 #&gt; setosa - virginica -1.582 0.102 135 -15.471 &lt;.0001 #&gt; versicolor - virginica -0.652 0.102 135 -6.376 &lt;.0001 #&gt; #&gt; Results are averaged over the levels of: Location #&gt; P value adjustment: holm method for 3 tests emmeans(mymodel, pairwise ~ Location, adjust = &quot;Holm&quot;) #&gt; $emmeans #&gt; Location emmean SE df lower.CL upper.CL #&gt; a 5.84 0.0933 135 5.66 6.03 #&gt; b 5.87 0.0933 135 5.69 6.05 #&gt; c 5.97 0.0933 135 5.78 6.15 #&gt; d 5.88 0.0933 135 5.70 6.06 #&gt; e 5.66 0.0933 135 5.47 5.84 #&gt; #&gt; Results are averaged over the levels of: Species #&gt; Confidence level used: 0.95 #&gt; #&gt; $contrasts #&gt; contrast estimate SE df t.ratio p.value #&gt; a - b -0.0267 0.132 135 -0.202 1.0000 #&gt; a - c -0.1233 0.132 135 -0.934 1.0000 #&gt; a - d -0.0367 0.132 135 -0.278 1.0000 #&gt; a - e 0.1867 0.132 135 1.414 1.0000 #&gt; b - c -0.0967 0.132 135 -0.732 1.0000 #&gt; b - d -0.0100 0.132 135 -0.076 1.0000 #&gt; b - e 0.2133 0.132 135 1.616 0.8674 #&gt; c - d 0.0867 0.132 135 0.657 1.0000 #&gt; c - e 0.3100 0.132 135 2.348 0.2031 #&gt; d - e 0.2233 0.132 135 1.692 0.8369 #&gt; #&gt; Results are averaged over the levels of: Species #&gt; P value adjustment: holm method for 10 tests So far we’ve dealth with groups manifesting in our predictors. Depending on our data, what we can also can do is to simultaneously compare different combinations/groups in our outcome as well. A special type of ANOVA can be found within the multivariate (i.e., multiple outcomes/dependent variables) regression models. One that is relatively robust against one not to be ignored issue. 7.4 MANOVA When you consider to use a classic univariate repeated measures design with factors that have over two levels, you may encounter a violation of the assumption of sphericity. Violation of sphericity could confer a risk for unreliable p values. As an attempt to detect violations herein, you could consider to use e.g., Mauchly’s test for sphericity. Unfortunately, like with many other statistical tests, the reliability of these depend on different factors including your sample size, the degree of departure from a normal distribution, and so on. In short, it is a tricky situation. In context of e.g., experiments with a within-subject design (the same subject goes through all or multiple experimental conditions) and where you would consider to aggregate your data across conditions, the method of multivariate analysis of variance (MANOVA). In a MANOVA multiple outcomes/dependent variables are analyzed simultaneously. The MANOVA typically yields the following steps. 1. Transform the data in wide format with the outcome - commonly a repeated measure from the same subject - aggregated across conditions 2. Create contrast matrices for the outcomes 3. Compute multivariate linear regression models and include the contrast matrices from step two. For the data I created myself. As the first step I will transform, or rather reshape, the data to wide format with the outcome “score” aggregated and spread across the the variable “condition_numbers”. As you may recall from the previous part, I prefer to use the dcast() function from the reshape2 package() to reshape from long to wide. set.seed(987) mydata = data.frame( participant = rep(c(1:100), times = 30), score = round( c(rnorm(100,10,3), rnorm(100,5,1.5), rnorm(100, 20,5) ), 2 ), condition_numbers = factor( rep( c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;), each = 100) ) ) # Reshape to wide format with the outcome aggregated and spread across condition library(reshape2) mydata = dcast(participant ~ condition_numbers, data=mydata, value.var=&quot;score&quot;, mean) names(mydata)[2:ncol(mydata)] = paste0(&quot;Condition_&quot; , colnames(mydata)[2:ncol(mydata)] ) # to rename &quot;1&quot; to &quot;Condition_1&quot;, etc. To create a contrast matrix for the outcomes, we can think in terms of vectors as we have done at the beginning of this part. Just like before we will work with coefficients. Different this time that we will have to model a multivariate regression model since our outcome “score” is spread across the multiple levels of “condition_number”. A multivariate regression model takes the form of a classic linear regression but with multiple outcomes which I will bind together using the cbind() function. mymodel = lm(cbind(Condition_1, Condition_2, Condition_3) ~ 1, data = mydata) coef(mymodel) #&gt; Condition_1 Condition_2 Condition_3 #&gt; (Intercept) 10.0528 5.2094 19.8799 The coefficients tells us that there are three of them. Now,we can visualize the outcome contrast matrix like this. Our job will be to replace the x to activate or deactivate a given or multiple condition(s). In other words, you will decide what part of the outcome you want to include and what parts you want to compare (all will become clear later on). However, the rules here are a bit different compared to the predictor contrast vectors and matrices. There is no treatment or sum contrast coding with a reference, at least not on the outcomes. Also, the sum of the values in the vector must be zero in most cases. Say I want to compare Condition_1 with Condition_2. We can asign the value “1” to Condition_1 and a “-1” to Condition_2, or vice versa. We are not interested in Condition_3 so multiplying it with zero will deactivate it. We get the vector c(1,-1,0) in which condition_1 will be multiplied with 1 (it remains alive), condition_2 will be multiplied with -1 (it also remains alive but it becomes negative, a contrast), and condition_3 is temporarily erased from existence as it multiplied with zero. The end product is the difference between Condition_1 and Condition_2 We feed the above vector to the (outcome) contrast matrix which I will call M. Within a classic linear regression model we multiply our outcome spread across Condition with our freshly created contrast matrix. Since we only created one vector, we will end up with a univariate regression, hence ANOVA. # Contrast &quot;Condition_1&quot; vs &quot;Condition_2&quot; diff = c(1,-1,0) M = cbind(diff) mymodel = lm(cbind(Condition_1, Condition_2, Condition_3) %*% M ~ 1, data = mydata) anova(mymodel) #&gt; Analysis of Variance Table #&gt; #&gt; Response: cbind(Condition_1, Condition_2, Condition_3) %*% M #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; Residuals 99 1050.4 10.611 Let’s briefly check what the (outcome) contrast matrix did to our outcome within the regression. cbind(mydata$Condition_1, mydata$Condition_2, mydata$Condition_3) %*% M #&gt; diff #&gt; [1,] 2.09 #&gt; [2,] 4.89 #&gt; [3,] 9.21 #&gt; [4,] 3.01 #&gt; [5,] 6.91 #&gt; [6,] 4.27 #&gt; [7,] 2.49 #&gt; [8,] -0.46 #&gt; [9,] 1.61 #&gt; [10,] -0.56 #&gt; [11,] 6.49 #&gt; [12,] 5.87 #&gt; [13,] 11.65 #&gt; [14,] 4.79 #&gt; [15,] 2.19 #&gt; [16,] 5.87 #&gt; [17,] 4.50 #&gt; [18,] 2.23 #&gt; [19,] 2.66 #&gt; [20,] 7.87 #&gt; [21,] 6.80 #&gt; [22,] 1.73 #&gt; [23,] -0.75 #&gt; [24,] 7.66 #&gt; [25,] 3.34 #&gt; [26,] 0.42 #&gt; [27,] 6.81 #&gt; [28,] 2.71 #&gt; [29,] -1.54 #&gt; [30,] 5.53 #&gt; [31,] 4.74 #&gt; [32,] 6.16 #&gt; [33,] 8.93 #&gt; [34,] 10.51 #&gt; [35,] 7.14 #&gt; [36,] 5.77 #&gt; [37,] 4.87 #&gt; [38,] 8.87 #&gt; [39,] 8.78 #&gt; [40,] 0.44 #&gt; [41,] 4.03 #&gt; [42,] 9.32 #&gt; [43,] 0.11 #&gt; [44,] 1.43 #&gt; [45,] 10.19 #&gt; [46,] 7.97 #&gt; [47,] 4.75 #&gt; [48,] 6.75 #&gt; [49,] 1.87 #&gt; [50,] 9.17 #&gt; [51,] 9.55 #&gt; [52,] 4.14 #&gt; [53,] -1.70 #&gt; [54,] 0.85 #&gt; [55,] 4.70 #&gt; [56,] 8.24 #&gt; [57,] 6.97 #&gt; [58,] 9.54 #&gt; [59,] 7.95 #&gt; [60,] 6.85 #&gt; [61,] 12.92 #&gt; [62,] 6.88 #&gt; [63,] 3.12 #&gt; [64,] -1.61 #&gt; [65,] 3.10 #&gt; [66,] 5.08 #&gt; [67,] 6.64 #&gt; [68,] 6.63 #&gt; [69,] 5.16 #&gt; [70,] -0.41 #&gt; [71,] 1.27 #&gt; [72,] 0.80 #&gt; [73,] 6.27 #&gt; [74,] 9.37 #&gt; [75,] 1.12 #&gt; [76,] 6.27 #&gt; [77,] 0.33 #&gt; [78,] 5.29 #&gt; [79,] 4.01 #&gt; [80,] 6.60 #&gt; [81,] 3.61 #&gt; [82,] 8.23 #&gt; [83,] 5.25 #&gt; [84,] 4.47 #&gt; [85,] 5.21 #&gt; [86,] 7.58 #&gt; [87,] 2.91 #&gt; [88,] 5.98 #&gt; [89,] 0.66 #&gt; [90,] 11.40 #&gt; [91,] 6.34 #&gt; [92,] 3.30 #&gt; [93,] 3.35 #&gt; [94,] 5.69 #&gt; [95,] 2.83 #&gt; [96,] 5.30 #&gt; [97,] -0.79 #&gt; [98,] 4.31 #&gt; [99,] 5.41 #&gt; [100,] 5.28 Indeed, our vector (1,-1,0) created the difference between Condition_1 and Condition_2 and this difference was then used within the regression. Alright, next example. Let’s say we to compare Condition_3 with the average of Condition_1 and Condition_2. In this case we will need to “activate” all of our conditions so nothing will be multiplied by zero. There are different ways to compute an average. We could multiply Condition_3 with two and contrast it with Condition_1 and Condition_2. Doing so makes the outcome within Condition_2 twice as high in value. Alternatively, we can half the values of Condition_1 and Condition_2 by multiplying them by 0.5. Either way, it creates an equal fight since Condition_3 will be up against both (i.e., the sum of) Condition_1 and Condition_2 and it respects the rule that the sum of values inside the vector is zero. # Contrast &quot;Condition_3&quot; vs the average between &quot;Condition_1&quot; and &quot;Condition_2&quot; diff = c(-0.5,-0.5,1) M = cbind(diff) mymodel = lm(cbind(Condition_1, Condition_2, Condition_3) %*% M ~ 1, data = mydata) anova(mymodel) #&gt; Analysis of Variance Table #&gt; #&gt; Response: cbind(Condition_1, Condition_2, Condition_3) %*% M #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; Residuals 99 2616.2 26.426 Last example with one outcome variable, Up till now, we have only included one vector inside the (outcome) contrast matrix, and therefore we did not end up with a multivariate regression. Say you want to compare some conditions to one another. # Contrast all diff1 = c(1,-1,0) diff2 = c(1,0,-1) M = cbind(diff1, diff2) mymodel = lm(cbind(Condition_1, Condition_2, Condition_3) %*% M ~ 1, data = mydata) anova(mymodel, test=&quot;Wilks&quot;) #&gt; Analysis of Variance Table #&gt; #&gt; Df Wilks approx F num Df den Df Pr(&gt;F) #&gt; (Intercept) 1 0.093849 473.11 2 98 &lt; 2.2e-16 #&gt; Residuals 99 #&gt; #&gt; (Intercept) *** #&gt; Residuals #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We keep more than one outcome (i.e., two differences). One thing to note, the Anova() function will not anymore accept the classic F test statistic (or chi-squared test statistic). In the context of MANOVA, four common test statistics are used: the Pillai-Bartlett trace (coded as “pillai” within anova), Wilk’s likelihood ration (“Wilks”), Hoteling-Lawley trace (“Hoteling-Lawley”), and Roy’s Largest Root (“Roy”). Like with many tests, the power of these tests can depend on different factors including the sample size, (multivariate) normal distribution of outcomes, and so on. Therefore, at the moment of writing this guide, I cannot claim the superiority of one test above another. 7.4.1 Practical example Manova: two outcome variables Let’s introduce the case of two outcomes variables. Suppose we receive data from a (quasi) experimental study. Participants went through two conditions, condition_X and condition_Y with two and three levels, respectively. In addition, participants’ gender was to be used as a predictor of the outcome. Like always we transform to wide format and aggregate our outcome across condition_X and condition_Y. set.seed(112358) options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.sum&quot;)) n_ID = 120 # Increase number of IDs n_rep = 60 # Repetitions per ID tot = n_ID * n_rep # Total number of rows mydata = data.frame( ID = rep(1:n_ID, each = n_rep), outcome = runif(tot, 500, 1000), condition_X = rep(c(&quot;absent&quot;, &quot;present&quot;), each = n_rep/2, times = n_ID), condition_Y = rep(rep(c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;), each = n_rep/6), times = 2 * n_ID), gender = rep(c(&quot;males&quot;, &quot;female&quot;) , each = 3600) ) # Long to wide library(reshape2) mydata = dcast(data = mydata, ID + gender ~ condition_X + condition_Y, value.var = &quot;outcome&quot;, fun.aggregate = mean) names(mydata)[3:ncol(mydata)] = c(&quot;abs_l&quot;, &quot;abs_m&quot;, &quot;abs_h&quot;, &quot;pres_l&quot;,&quot;pres_m&quot;,&quot;pres_h&quot;) Given the outcome (spreaded over two variables) and predictor we can test several things: 1. Main effects in the outcomes 2. Interaction effects in the outcomes 3. Main effect in the predictor 4. Interaction effects between outcomes and predictor In the remainder of this part, I will show you how to test each of these objectives and a way that does all in one go (without any outcome contrast matrix). First things first, I will fit a multivariate linear regression without a contrast matrix to extract the coefficients so that I know what to place where. mymodel = lm( cbind( abs_l, abs_m, abs_h, pres_l, pres_m, pres_h ) ~ gender , data = mydata ) coef(mymodel) #&gt; abs_l abs_m abs_h pres_l #&gt; (Intercept) 749.1389449 752.36649 748.6051946 746.4309883 #&gt; gender1 -0.0984652 2.10392 0.4240033 0.6321018 #&gt; pres_m pres_h #&gt; (Intercept) 749.1050034 751.80796 #&gt; gender1 0.8787361 7.01653 Alright, notice that condition_X’s “absent” level is spread across condition_Y’s low, medium, and high levels. The same goes for condition_X’s “present” level. Now, if we want to test the main effect of condition_X on the outcome we will need to look at the intercept. Specifically, we will need to test whether the difference between condition_X’s “absent” level is statistically significantly different from its “present” level. We could use the following outcome contrast matrix. diff = c(1, 1, 1, -1, -1, -1) M = cbind(diff) mymodel = lm( cbind( abs_l, abs_m, abs_h, pres_l, pres_m, pres_h ) %*% M ~ gender , data = mydata ) Anova(mymodel, type=&quot;III&quot;, test=&quot;F&quot;) #&gt; Anova Table (Type III tests) #&gt; #&gt; Response: cbind(abs_l, abs_m, abs_h, pres_l, pres_m, pres_h) %*% M #&gt; Sum Sq Df F value Pr(&gt;F) #&gt; (Intercept) 919 1 0.0860 0.7698 #&gt; gender 4462 1 0.4178 0.5193 #&gt; Residuals 1260382 118 As told before, the intercept will show you the main effect of condition_X but what about gender? Well this can be considered the interaction effect between gender and condition_X on the outcome, we have estimated a main effect and an interaction effect. Good, onto condition_Y which has three levels “low”, “medium”, and “high” spread across the two levels of condition_X. The main effect here will be difference between these levels but there is a nuance You need only two out of three potential contrasts. If you know that low levels are not statistically significantly different from medium levels and that low levels are not statistically significantly different from high levels, then medium levels will not be statistically significantly different from high levels. # Main effect diff1 = c( 1, -1, 0, 1, -1, 0 ) # low vs medium diff2 = c( 1, 0, -1, 1, 0, -1 ) # low vs high M = cbind(diff1, diff2) mymodel = lm( cbind( abs_l, abs_m, abs_h, pres_l, pres_m, pres_h ) %*% M ~ gender , data = mydata ) Anova(mymodel, type=&quot;III&quot;, test=&quot;Wilks&quot;) # Just for demonstration purposes I switch the test statistic #&gt; #&gt; Type III MANOVA Tests: Wilks test statistic #&gt; Df test stat approx F num Df den Df Pr(&gt;F) #&gt; (Intercept) 1 0.99582 0.24555 2 117 0.7827 #&gt; gender 1 0.99511 0.28725 2 117 0.7509 Again, the intercept shows the main effect and gender will show the interaction between itself and condition_Y. We can also have a look at the interaction between the outcome variables. In other words, is the difference condition_X’s “absent” and “present” levels, different over the levels of condition_y? To make a outcome contrast matrix for the interactions within outcome variables, you typically mirror the values (the “1” and “-1”) as shown below. # Interaction and three-way interaction diff1 = c( 1, -1, 0, 1, -1, 0 ) # high vs low diff2 = c( 0, 1, -1, 0, -1, 1 ) # high vs medium M = cbind(diff1, diff2) mymodel = lm( cbind( abs_l, abs_m, abs_h, pres_l, pres_m, pres_h ) %*% M ~ gender , data = mydata ) Anova(mymodel, type=&quot;III&quot;, test=&quot;Hotelling-Lawley&quot;) # Just for demonstration purposes I switch the test statistic #&gt; #&gt; Type III MANOVA Tests: Hotelling-Lawley test statistic #&gt; Df test stat approx F num Df den Df Pr(&gt;F) #&gt; (Intercept) 1 0.0108108 0.63243 2 117 0.5331 #&gt; gender 1 0.0094164 0.55086 2 117 0.5779 The intercepts shows the interaction effect, and gender now shows the three-way interaction between itself and outcome_X, and outcome_Y. Now, what if we want to look at the main effect of gender (i.e., separate from other variables)? This is will be the exception in which the sum of the values within the outcome contrast matrix is not zero. Specifically, I will set all values to 1/6 which will average the prediction of gender over each of our outcome variables. diff = c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6 ) # high vs low M = diff mymodel = lm( cbind( abs_l, abs_m, abs_h, pres_l, pres_m, pres_h ) %*% M ~ gender , data = mydata ) aov(mymodel, test=&quot;F&quot;) #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;test&#39; will be disregarded #&gt; Call: #&gt; aov(formula = mymodel, test = &quot;F&quot;) #&gt; #&gt; Terms: #&gt; gender Residuals #&gt; Sum of Squares 400.17 49919.47 #&gt; Deg. of Freedom 1 118 #&gt; #&gt; Residual standard error: 20.56809 #&gt; Estimated effects are balanced I end this part with a neat trick. What if you could do all of the above without a single (outcome) contrast matrix? What is needed is a so called idata and idesign. The idata is a simple data frame object in which you specify your the variables that are contained within your conjoined outcome (here condition_X and condition_Y). In essence you recreate the following as a data set: condition_X = factor( rep(c(&quot;absent&quot;,&quot;present&quot;), each = 3), levels = c(&quot;absent&quot;,&quot;present&quot;) ) condition_Y = factor( rep(c(&quot;low&quot;,&quot;medium&quot;, &quot;high&quot;), times = 2), levels = c(&quot;low&quot;,&quot;medium&quot;, &quot;high&quot;) ) idata = data.frame( condition_X, condition_Y ) mymodel = lm(cbind( abs_l, abs_m, abs_h, pres_l, pres_m, pres_h ) ~ gender, data=mydata ) Anova(mymodel, type=&quot;III&quot;, idata=idata, idesign=~condition_X*condition_Y, test=&quot;Wilks&quot;) #&gt; #&gt; Type III Repeated Measures MANOVA Tests: Wilks test statistic #&gt; Df test stat approx F num Df #&gt; (Intercept) 1 0.00074 159377 1 #&gt; gender 1 0.99205 1 1 #&gt; condition_X 1 0.99927 0 1 #&gt; gender:condition_X 1 0.99647 0 1 #&gt; condition_Y 1 0.99582 0 2 #&gt; gender:condition_Y 1 0.99511 0 2 #&gt; condition_X:condition_Y 1 0.99340 0 2 #&gt; gender:condition_X:condition_Y 1 0.99146 1 2 #&gt; den Df Pr(&gt;F) #&gt; (Intercept) 118 &lt;2e-16 *** #&gt; gender 118 0.3327 #&gt; condition_X 118 0.7698 #&gt; gender:condition_X 118 0.5193 #&gt; condition_Y 117 0.7827 #&gt; gender:condition_Y 117 0.7509 #&gt; condition_X:condition_Y 117 0.6790 #&gt; gender:condition_X:condition_Y 117 0.6056 #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.5 Referred article (see the section concerning effect sizes): Brysbaert, M., &amp; Debeer, D. (2025). How to Run Linear Mixed Effects Analysis for Pairwise Comparisons? A Tutorial and a Proposal for the Calculation of Standardized Effect Sizes. Journal of cognition, 8(1), 5. https://doi.org/10.5334/joc.409 "],["structural-equation-modelling-and-mediation.html", "Chapter 8 Structural Equation Modelling and mediation 8.1 Basic mediation 8.2 Basic example 8.3 Moderated mediation", " Chapter 8 Structural Equation Modelling and mediation Structural Equation Modelling (SEM) can be used to test hypotheses about relations between latent and observed (or at least measured in some way) variables. We have already seen SEM in action when discussing confirmatory factor analysis but now I want to end this guide briefly talking about mediation. In this part I will discuss The basic mediation using the lavaan package How to visualize results from a mediation using the semPlot package The moderated mediation with categorical and continuous moderators. 8.1 Basic mediation Have you heard about those spurious correlations that arise when you correlate everything with everything? A classic textbook example is the positive correlation between ice cream consumption and drowning. Of course, correlation is not causation. So what we have here is a typical “direct” relation between outcome and predictor, which can be represented by a correlation coefficient or a regression coefficient. But why does ice cream consumption (predictor) relates to drowning (outcome)? We could reason that this relation was likely brought by other indirect relations. Ice cream consumption could be related to another variable, the mediator, and this mediator could relate to drowning. Through these “indirect pathways”, ice cream consumption may appear to be associated with drowning. The common explanation is that you tend to eat more ice cream when it is hot. Hot weather can increase susceptibiliy to heatstroke symptoms (the mediator). If you experience heatstroke symptoms when you go for a swim… unfortunate outcomes may occur Mediation analysis allows us to test to what extent a (direct) relation between variables can be explained by the (indirect) relations with other variables. Do not to confuse mediation with moderation, which examines whether the strength of an association depends on other variables. When you learn about mediation, you typically encounter “the triangle” (or trapezium, whatever you prefer): Have you noticed that I talk a lot about the “direct” and “indirect” relations? The direct relation (henceforth referred to as direct effect) is the relation between predictor(s) and outcome after adjusting for the mediator (which I will denote using beta coefficients). Indirect effects is the relation between predictor and outcome through the mediator and consists of two parts. First is the association between predictor and the mediator. Second is the association between the mediatior and outcome. Our goal is to determine the extent the mediator explains the direct relation between predictor and outcome. Before we continue, I want to address a common misconception. At the time of writing this guide, mediation analysis itself cannot confirm causal relations. Causality is a methodological issue. My advice would be to look into properly(!) randomized and properly(!) controlled experimental designs for causal inference. 8.2 Basic example A mediation analysis typically involves the two steps. First, like we did with confirmatory factor analysis, we specify our variables and their relations in a model structure (the SEM model structure).Here, we will need to specify the indirect, direct, and optionally the total effects, as I discuss later on.Second, we estimate the model based on our SEM model structure. I have made a simple dataset and took the liberty to load the lavaan package. library(pacman) p_load(lavaan) set.seed(54321) n = 250 # Sample size cor_Y_X = 0.5 # correlation between outcome Y and predictor X cor_Y_M = 0.6 # correlation between outcome Y and mediator M cor_X_M = 0.7 # correlation between predictor X and mediator M predictor = rnorm(n, mean = 20, sd = 5) # Create the predictor mediator = rnorm(n, mean = 15, sd = 3) + (predictor * cor_X_M) # Create a mediator that is associated with the above predictor mydata = data.frame( predictor = predictor, mediator = mediator, outcome = rnorm(n, mean = 25, sd = 7) + (mediator * cor_Y_M) + (predictor * cor_Y_X) # Outcome that is associated with both the predictor and mediator ) Alright let’s make the SEM model structure. Let’s have a look again at the mediation. In essence, we need to model the regressions and define the direct effect and indirect effect. Notice that we have three regressions: predictor -&gt; outcome, predictor -&gt; mediator, and mediator -&gt; outcome. The direct “effect” reflects the relation between predictor -&gt; outcome. So here our regression coefficient β(PO). For convention with many other work, I will call this regression coefficient “c”. The indirect effect is the product between two regression coefficients: β(PM) from the predictor -&gt; mediator regression (hencefort called “a”), and β(MO) from the mediator -&gt; outcome regression (“b”). If we want, we can also define the total “effect” which is the sum of the direct and indirect effect. Note that within my SEM model structure, I use the above letters a,b, and c, to represent my three regression coefficients. I use these letters to give the regression coefficients a name so that I can use them to compute the (in)direct and total effects (e.g., c*predictor will name the regression coefficient β(MO) as “c”). mystructure = &#39; # Regression outcome - predictor outcome ~ c*predictor # with c being the regression coefficient between the two, beta(PM) in my figure # Indirect relation: mediator ~ a*predictor # with a being the regression coefficient, beta(PM) outcome ~ b*mediator # # Alternatively you can redo the above like (without the # of course) # outcome ~ b*mediator + c*predictor # mediator ~ a*predictor # The above is needed to compute indirect indirect := a*b direct := c total := (a*b) + c &#39; Now we use feed the above to the sem() function from lavaan. I will also ask for bootstrap simulations as these could be important for obtaining reliable confidence intervals in mediation analysis. Note that I set the number of bootstrap simulations to 5 just for demonstration purpose. However, consider to have more of them (e.g., 500 or more). mymodel=sem(mystructure,data=mydata, se = &quot;bootstrap&quot;, iseed=1234, bootstrap = 5) # I take 5 bootstrap simulation but consider 500+ when doing it for real #&gt; Warning in lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats, : lavaan WARNING: #&gt; The variance-covariance matrix of the estimated parameters (vcov) #&gt; does not appear to be positive definite! The smallest eigenvalue #&gt; (= 8.463702e-17) is close to zero. This may be a symptom that the #&gt; model is not identified. summary(mymodel,standardized=TRUE) #&gt; lavaan 0.6.16 ended normally after 1 iteration #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 5 #&gt; #&gt; Number of observations 250 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 0.000 #&gt; Degrees of freedom 0 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Bootstrap #&gt; Number of requested bootstrap draws 5 #&gt; Number of successful bootstrap draws 5 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; outcome ~ #&gt; predictor (c) 0.230 0.122 1.885 0.059 #&gt; mediator ~ #&gt; predictor (a) 0.644 0.033 19.351 0.000 #&gt; outcome ~ #&gt; mediator (b) 0.760 0.148 5.121 0.000 #&gt; Std.lv Std.all #&gt; #&gt; 0.230 0.140 #&gt; #&gt; 0.644 0.713 #&gt; #&gt; 0.760 0.417 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .outcome 47.380 5.939 7.977 0.000 #&gt; .mediator 9.689 0.684 14.163 0.000 #&gt; Std.lv Std.all #&gt; 47.380 0.723 #&gt; 9.689 0.491 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; indirect 0.490 0.113 4.334 0.000 #&gt; direct 0.230 0.136 1.686 0.092 #&gt; total 0.720 0.033 22.118 0.000 #&gt; Std.lv Std.all #&gt; 0.490 0.297 #&gt; 0.230 0.140 #&gt; 0.720 0.437 Summary output time. I will focus on our three defined parameters. Indirect, as mentioned before, reflects the model’s estimated “effect” of our predictor on our outcome through our mediator. In other words, our mediation effect. direct, shows the relation between predictor and outcome but after accounting for the mediator. total is formula-wise the sum of our indirect and direct estimated “effects” so the total impact of the predictor on our outcome. In our example, we could report that the mediation (indirect effect) is statistically significant (based on p value) whereas the estimated direct “effect” is not. In other words, the predictor statistically significantly relates to the outcome through the mediator. Some people would describe our case as a “full” mediation while they would label it as “partial” mediation in case our direct effect was also statistically significant. Let’s return back to R code, I can also obtain the (standardized) estimated parameters using the parameterEstimates() function. parameterEstimates(mymodel, standardized = TRUE) #&gt; lhs op rhs label est se z #&gt; 1 outcome ~ predictor c 0.230 0.122 1.885 #&gt; 2 mediator ~ predictor a 0.644 0.033 19.351 #&gt; 3 outcome ~ mediator b 0.760 0.148 5.121 #&gt; 4 outcome ~~ outcome 47.380 5.939 7.977 #&gt; 5 mediator ~~ mediator 9.689 0.684 14.163 #&gt; 6 predictor ~~ predictor 24.160 0.000 NA #&gt; 7 indirect := a*b indirect 0.490 0.113 4.334 #&gt; 8 direct := c direct 0.230 0.136 1.686 #&gt; 9 total := (a*b)+c total 0.720 0.033 22.118 #&gt; pvalue ci.lower ci.upper std.lv std.all std.nox #&gt; 1 0.059 -0.006 0.365 0.230 0.140 0.028 #&gt; 2 0.000 0.600 0.697 0.644 0.713 0.145 #&gt; 3 0.000 0.560 0.986 0.760 0.417 0.417 #&gt; 4 0.000 36.165 53.789 47.380 0.723 0.723 #&gt; 5 0.000 8.815 10.709 9.689 0.491 0.491 #&gt; 6 NA 24.160 24.160 24.160 1.000 24.160 #&gt; 7 0.000 0.369 0.669 0.490 0.297 0.060 #&gt; 8 0.092 -0.006 0.365 0.230 0.140 0.028 #&gt; 9 0.000 0.663 0.734 0.720 0.437 0.089 However, the confidence intervals may not use the bootstrapped solutions so to have more accurate (percentile) confidence intervals, we can use the standardized_Solution_boot_ci() function from the semhelpinghands package. library(semhelpinghands) standardizedSolution_boot_ci(mymodel) #&gt; lhs op rhs label est.std se z #&gt; 1 outcome ~ predictor c 0.140 0.079 1.769 #&gt; 2 mediator ~ predictor a 0.713 0.022 31.727 #&gt; 3 outcome ~ mediator b 0.417 0.074 5.611 #&gt; 4 outcome ~~ outcome 0.723 0.033 21.637 #&gt; 5 mediator ~~ mediator 0.491 0.032 15.333 #&gt; 6 predictor ~~ predictor 1.000 0.000 NA #&gt; 7 indirect := a*b indirect 0.297 0.051 5.791 #&gt; 8 direct := c direct 0.140 0.079 1.769 #&gt; 9 total := (a*b)+c total 0.437 0.038 11.444 #&gt; pvalue ci.lower ci.upper boot.ci.lower boot.ci.upper #&gt; 1 0.077 -0.015 0.294 -0.004 0.248 #&gt; 2 0.000 0.669 0.757 0.669 0.730 #&gt; 3 0.000 0.271 0.563 0.347 0.555 #&gt; 4 0.000 0.658 0.789 0.693 0.787 #&gt; 5 0.000 0.429 0.554 0.467 0.552 #&gt; 6 NA 1.000 1.000 NA NA #&gt; 7 0.000 0.197 0.398 0.251 0.399 #&gt; 8 0.077 -0.015 0.294 -0.004 0.248 #&gt; 9 0.000 0.362 0.512 0.394 0.499 #&gt; boot.se #&gt; 1 0.091 #&gt; 2 0.026 #&gt; 3 0.085 #&gt; 4 0.040 #&gt; 5 0.036 #&gt; 6 NA #&gt; 7 0.059 #&gt; 8 0.091 #&gt; 9 0.045 8.2.1 Visualize your : path diagrams Using the summary() output, we could use the estimated parameters and make our own plots, in the context of mediation, our own path diagrams. However, you could also consider to use ^packages like semPlot to do the work for you. Basically, we will recreate “the triangle” but now we add the estimates of β(PM), β(MO), and β(PM). In my experience, it does take a bit of knowledge to make your path diagrams look aesthetically pleasing. Allow me to guide you through. A first concern is where the figure will put the labels for your variables. So where “predictor”, “mediator”, and “outcome” will be placed. I prefer that the predictor is placed on the left, the outcome on the right, and the mediator in the middle on top. To tell semPlot to use this layout, we will have to create a matrix that contains the position of these variables (see below in the R code chunk). Then we can use the semPaths() function to make our path diagrams. library(semPlot) # To determine the location of the labels: my_layout &lt;- matrix(c( 1, 0, # To put the variable &quot;predictor&quot; to the RIGHT side and at the BOTTOM 0.5, 1, # Put the &quot;mediator&quot; in the MIDDLE at the TOP 0, 0 # Put the &quot;outcome&quot; at the left side and at the BOTTOM ), byrow = TRUE, ncol = 2) # Create the path diagram semPaths(mymodel, what = &quot;std&quot;, # I want to use the standardized estimates layout = my_layout, # Use our above custom layout edge.label.cex = 1.3, # Set the font size for the estimates label.cex = 1.2, # Set the font size for the labels (&quot;predictor&quot;, &quot;mediator&quot;, etc.) sizeMan = 15, # Set the sizes of the boxes fade = FALSE, # TRUE will create make the larger estimates also thicker residuals = FALSE, # Hide residual variances of the variables edge.color = &quot;black&quot;, # Determine the color for the arrows (&quot;edges&quot;) nCharNodes = 0 # If Set to zero, it will display the full variable name instead of shorten it down ) Now, if you want, you can also add notifications of significance (asterisks) to our (standardized) estimates. To do so, we will need to extract the p values from our estimates. Since I want to plot standardized estimates, I will need to extract those of the standardized estimates. This can be done using the aforementioned parameterEstimates() function. Using these p values we now have to (a) transform them into significance notifications and (b) glue them to our original estimates. To do the first step “from value to asterisk” I will make a function that checks the p value and determines whether to asign no, one, two, or three asterisk Then I will actually apply this function to the p values and glue the asterixes (or lack thereoff) to my standardized estimates. This is how it looks like. # Alright we need to acquire the estimates and corresponding p value parameter_estimates = parameterEstimates(mymodel, standardized = TRUE) # For our plot we only need the parameters for the a,b, and c-path so here I make sure to exclusive include these three parameter_estimates = parameter_estimates[parameter_estimates$op == &quot;~&quot;, ] # Now we can translate these p values into significance labels, the famous asteriks (*, **, ***) # To do so, I will create a function to add asterisks based on p value add_stars = function(p) { # This function will take the p-value and determines what sign it should attribute to it (i.e., the number of asterix) # Functions are beyond of the scope of this guide # However, functions apply anything contained within the function to whatever argument you feed it (here the argument called &quot;p&quot;) # So my function checks the value of the p value, and determines whether to transform it into a given number of asterisks or nothing if (is.na(p)) return(&quot;&quot;) # If the p value is NA, empty, return an empty string else if (p &lt; 0.001) return(&quot;***&quot;) # If the p-value is below .001 then return *** else if (p &lt; 0.01) return(&quot;**&quot;) else if (p &lt; 0.05) return(&quot;*&quot;) else return(&quot;&quot;) # If the p-value is 0.05 or larger then do not add an asterisk } # Alright I want to put the label and the asterisk on my path diagram (e.g., &quot;0.71***&quot;) so I will need to glue them together parameter_estimates$label = paste0( round(parameter_estimates$std.all, 2), # round the estimates to two decimals sapply(parameter_estimates$pvalue, add_stars) # apply the &quot;p value to asterisks&quot; function ) # Finally, our path diagram with significance asterisks semPaths(mymodel, what = &quot;std&quot;, # I want to use the standardized estimates layout = my_layout, # Use our above custom layout edge.label.cex = 1.3, # Set the font size for the estimates label.cex = 1.2, # Set the font size for the labels (&quot;predictor&quot;, &quot;mediator&quot;, etc.) sizeMan = 15, # Set the sizes of the boxes fade = FALSE, residuals = FALSE, # Hide residual variances of the variables edge.color = &quot;black&quot;, # Determine the color for the arrows (&quot;edges&quot;) nCharNodes = 0, # If Set to zero, it will display the full variable name instead of shorten it down edgeLabels = parameter_estimates$label) # Apply the significance labels (estimate + asterixes) 8.3 Moderated mediation What if the mediation relation (the extent of mediation, the variance explained by the mediator) depends on the level of yet another variable “the moderator”? Recall that we have three relations reflected by our three beta’s. The moderator can moderate any of these three relations. Let’s start with the most simple situation where only one relation (say the a-path) is moderated by a categorical variable. Crucially, we will need to compute and include the interaction in our dataset ourselves (which is just the predictor multiplied by the moderator) as lavaan will not accept something like “predictor:moderator” unlike in linear regression models. set.seed(54321) n = 250 # Sample size cor_Y_X = 0.5 # correlation between outcome Y and predictor X cor_Y_M = 0.6 # correlation between outcome Y and mediator M cor_X_M = 0.7 # correlation between predictor X and mediator M predictor = rnorm(n, mean = 20, sd = 5) # Create the predictor mediator = rnorm(n, mean = 15, sd = 3) + (predictor * cor_X_M) # Create a mediator that is associated with the above predictor moderator = rep(c(0,1), times=125) mydata = data.frame( predictor = predictor, mediator = mediator, outcome = rnorm(n, mean = 25, sd = 7) + (mediator * cor_Y_M) + (predictor * cor_Y_X), # Outcome that is associated with both the predictor and mediator moderator = moderator, interaction_predictor_moderator = moderator * predictor ) We repeat the steps from before but now we need to remember that we need to add the moderator to the relation between predictor and mediator. Specifically, the mediator is now analytically predicted by predictor (main effect), the interaction term between the moderator and predictor, and as customary when you include interactions, the moderator (main effect) itself. So now we have three beta’s (two main effects, one interaction term) on that link to acknowledge. Our original “a-path” has now become a1 (say main effect predictor), a2 (say main effect moderator), and a3(say the interaction term). We acknowledge this when we define our regressions within our SEM structure. But now, the inclusion of our moderator has two repercussions on how we need to compute the indirect effect. First, the original “a * b” formula will not suffice anymore as we have multiple a’s. The “a” in the original formula changes to a1 + a3 or the main effect of the predictor and the interaction term between predictor and moderator. But this is incomplete, as the indirect effect is now dependent on the values of the moderator, which brings us to the second point. So what we will do is to compute the indirect effect when the value of our moderator is something, and a indirect effect when the value is something else. Since our moderator is a binary factor, we can just compute the indirect effect if the moderator is zero and also when the moderator is one. The final formula becomes a1 + (a3 * 0) * b when the moderator is zero and a1 + (a3 * 1) * b, when the moderator is one. Finally, if you want to include the total effect, it becomes also different across the values of the moderator. So we have a total effect when the moderator is zero, and a total effect when the moderator is one. Let us implement the above in our SEM structure object. mystructure = &#39; # Direct relation outcome ~ c*predictor # Indirect relation: mediator ~ a1*predictor + a2*moderator + a3*interaction_predictor_moderator # Again, you need to compute and include the interaction in your dataset and use that outcome ~ b*mediator # The indirect indirect_0 := (a1 + (a3*0)) * b # when the moderator is zero indirect_1 := (a1 + (a3*1)) * b # when the moderator is one direct := c total_0 := indirect_0 + c # when the moderator is zero (also &quot;c&quot; can be changed to &quot;direct&quot; if you want) total_1 := indirect_1 + c # when the moderator is one &#39; mymodel=sem(mystructure,data=mydata, se = &quot;bootstrap&quot;, iseed=1234, bootstrap = 5) # Again I take 5 but consider 500+ when doing it for real summary(mymodel,standardized=TRUE) #&gt; lavaan 0.6.16 ended normally after 1 iteration #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 7 #&gt; #&gt; Number of observations 250 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 0.787 #&gt; Degrees of freedom 2 #&gt; P-value (Chi-square) 0.675 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Bootstrap #&gt; Number of requested bootstrap draws 5 #&gt; Number of successful bootstrap draws 5 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; outcome ~ #&gt; predictor (c) 0.230 0.122 1.885 0.059 #&gt; mediator ~ #&gt; predictor (a1) 0.696 0.048 14.510 0.000 #&gt; moderator (a2) 1.546 1.589 0.973 0.331 #&gt; intrctn__ (a3) -0.093 0.068 -1.359 0.174 #&gt; outcome ~ #&gt; mediator (b) 0.760 0.148 5.121 0.000 #&gt; Std.lv Std.all #&gt; #&gt; 0.230 0.140 #&gt; #&gt; 0.696 0.770 #&gt; 1.546 0.174 #&gt; -0.093 -0.226 #&gt; #&gt; 0.760 0.417 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .outcome 47.380 5.939 7.977 0.000 #&gt; .mediator 9.618 0.716 13.425 0.000 #&gt; Std.lv Std.all #&gt; 47.380 0.723 #&gt; 9.618 0.488 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; indirect_0 0.529 0.122 4.329 0.000 #&gt; indirect_1 0.458 0.106 4.316 0.000 #&gt; direct 0.230 0.136 1.686 0.092 #&gt; total_0 0.759 0.042 18.263 0.000 #&gt; total_1 0.688 0.050 13.744 0.000 #&gt; Std.lv Std.all #&gt; 0.529 0.321 #&gt; 0.458 0.227 #&gt; 0.230 0.140 #&gt; 0.759 0.461 #&gt; 0.688 0.367 Like before we can make a path diagram using the semPaths() function from the semPlot package. Remember that we made a layout ourselves to determine where are our labels will be placed? Now we have additional labels so we can determine their position if we want. Other than that nothing notably changes from before. library(semPlot) my_layout = matrix(c( 3, 0, # Predictor (bottom right) 1.5, 3, # Mediator (middle top) 0, 0, # Outcome (bottom left) 0.1, 3, # Moderator (top right) 1.25, -0.75 # Interaction (middle right) ), byrow = TRUE, ncol = 2) # Create the path diagram semPaths(mymodel, what = &quot;std&quot;, # Standardized estimates layout = my_layout, # Updated layout edge.label.cex = 1.3, # Font size for estimates label.cex = 1.2, # Font size for variable labels sizeMan = 15, # Size of the boxes fade = FALSE, residuals = FALSE, # Hide residual variances edge.color = &quot;black&quot;, # Arrow color nCharNodes = 0, # Show full variable names intercepts = FALSE) # Hide intercepts for clarity 8.3.1 On all pathways What if you decide on theory or other that a moderation is plausible on all pathways? This case is not that different from before, we just need to adjust all of our linear regressions. Note however, that you do not need both a2 * moderator and b2 * moderator, only one of two (as they are the same). The indirect effect remains the same but now we need to adjust the formula. The (only) change lies in the direct effect which now has two forms: when the moderator has value 0 and when it has value 1. Remember to compute and include an interaction term for every path way in your dataset and use that within your SEM structure object. set.seed(54321) n = 250 # Sample size cor_Y_X = 0.5 # correlation between outcome Y and predictor X cor_Y_M = 0.6 # correlation between outcome Y and mediator M cor_X_M = 0.7 # correlation between predictor X and mediator M predictor = rnorm(n, mean = 20, sd = 5) # Create the predictor mediator = rnorm(n, mean = 15, sd = 3) + (predictor * cor_X_M) moderator = rep(c(0,1), times=125) mydata = data.frame( predictor = predictor, mediator = mediator, outcome = rnorm(n, mean = 25, sd = 7) + (mediator * cor_Y_M) + (predictor * cor_Y_X), # Outcome that is associated with both the predictor and mediator moderator = moderator, interaction_predictor_moderator = moderator * predictor, interaction_mediator_outcome = mediator * moderator, interaction_predictor_outcome = predictor * moderator ) mystructure = &#39; # Direct relation outcome ~ c1*predictor + c2*moderator + c3*interaction_predictor_outcome # Indirect relation: mediator ~ a1*predictor + a2*moderator + a3*interaction_predictor_moderator outcome ~ b1*mediator + b3*interaction_mediator_outcome # Note that there is no need for a b2*moderator term as this is captued in a2*moderator # indirect, direct, and total effects indirect_0 := (a1 + (a3*0)) * (b1 + (b3*0)) # when the moderator is zero indirect_1 := (a1 + (a3*1)) * (b1 + (b3*1)) # when the moderator is one direct_0 := c1 + (c3*0) direct_1 := c1 + (c3*1) total_0 := indirect_0 + direct_0 total_1 := indirect_1 + direct_1 &#39; mymodel=sem(mystructure,data=mydata, se = &quot;bootstrap&quot;, iseed=1234, bootstrap = 5) # I take 5 bootstrap simulation but consider 500+ when doing it for real summary(mymodel,standardized=TRUE) #&gt; lavaan 0.6.16 did not run (perhaps do.fit = FALSE)? #&gt; ** WARNING ** Estimates below are simply the starting values #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 10 #&gt; #&gt; Number of observations 250 #&gt; #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Bootstrap #&gt; Number of requested bootstrap draws 5 #&gt; Number of successful bootstrap draws 0 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; outcome ~ #&gt; predictor (c1) 0.339 NA #&gt; moderator (c2) -0.251 NA #&gt; intrctn__ (c3) -0.181 NA #&gt; mediator ~ #&gt; predictor (a1) 0.696 NA #&gt; moderator (a2) 1.546 NA #&gt; intrctn__ (a3) -0.093 NA #&gt; outcome ~ #&gt; mediator (b1) 0.694 NA #&gt; intrctn__ (b3) 0.113 NA #&gt; Std.lv Std.all #&gt; #&gt; 0.339 0.207 #&gt; -0.251 -0.016 #&gt; -0.181 -0.244 #&gt; #&gt; 0.696 0.770 #&gt; 1.546 0.174 #&gt; -0.093 -0.226 #&gt; #&gt; 0.694 0.383 #&gt; 0.113 0.209 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .outcome 47.201 NA #&gt; .mediator 9.618 NA #&gt; Std.lv Std.all #&gt; 47.201 0.729 #&gt; 9.618 0.488 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; indirect_0 0.483 #&gt; indirect_1 0.487 #&gt; direct_0 0.339 #&gt; direct_1 0.158 #&gt; total_0 0.822 #&gt; total_1 0.645 #&gt; Std.lv Std.all #&gt; 0.483 0.295 #&gt; 0.487 0.322 #&gt; 0.339 0.207 #&gt; 0.158 -0.037 #&gt; 0.822 0.502 #&gt; 0.645 0.285 8.3.2 Using a continuous moderator For my final example on a moderated mediation, we have a continuous moderator that moderates path “c” and path “b”. We will not encounter something that we have not seen before. However, remember that we defined multiple (in)direct effects depending on the values of the moderator? Now that we have a continuous moderator, we we will need to decide what values we want to include and how many. Remind yourself that every included moderator value is a pair of extra (in)direct effects to be estimated, hence taxing on the statistical power. In this example I will include three values of the moderator: “low” (i.e., a standard deviation below its mean); “moderate” (its mean), and “high” (i.e., a standard deviation above its mean). To include these values within my SEM model structure, I could add them to my dataset beforehand or I could compute them while inside the SEM structure. I will demonstrate the latter. To define and use the mean of the moderator, I add an additional regression in which the moderator is predicted by the intercept without any other predictors (hence the intercept will be the mean). I now simply have to give this intercept a name so that I can use it later (like with my letter a,b, and c) to define the (in)direct effects. To get the variance of the moderator, remember from confirmatory factor analysis that the notation ~~ defines a covariance within the SEM model structure? Well, if I let the moderator co-vary with itself, I get the variance and if give a name to this variance, I can use it later on to define the (in)direct effect. With both the mean and variance defined, I can now define a standard deviation below or above the mean. Note that I will need to take the square root of the variance, which can be easily done within the SEM model structure. set.seed(54321) n = 250 # Sample size cor_Y_X = 0.5 # correlation between outcome Y and predictor X cor_Y_M = 0.6 # correlation between outcome Y and mediator M cor_X_M = 0.7 # correlation between predictor X and mediator M predictor = rnorm(n, mean = 20, sd = 5) # Create the predictor mediator = rnorm(n, mean = 15, sd = 3) + (predictor * cor_X_M) # Create a mediator that is associated with the above predictor moderator = rnorm(n, mean = 17, sd = 4 ) outcome = rnorm(n, mean = 25, sd = 7) + (mediator * cor_Y_M) + (predictor * cor_Y_X) mydata = data.frame( predictor = predictor, mediator = mediator, outcome = outcome, # Outcome that is associated with both the predictor and mediator moderator = moderator, interaction_mediator_outcome = moderator * mediator, interaction_predictor_outcome = predictor * moderator ) mystructure = &#39; # Mean and SD to be used to compute indirect and direct effect moderator ~ moderator_mean*1 # to define and use the intercept of the moderator/the mean moderator ~~ moderator_var*moderator # to define use the variance of the moderator # Direct relation outcome ~ c1*predictor + c2*moderator + c3*interaction_predictor_outcome # Indirect relation: mediator ~ a*predictor outcome ~ b1*mediator + b3*interaction_mediator_outcome # Note that there is no need for a b2*moderator term as this is captued in a2*moderator # indirect, direct, and total effects indirect_low := a * (b1 + b3*(moderator_mean - sqrt(moderator_var))) # when the moderator is zero indirect_moderate := a * (b1 + (b3*moderator_mean)) indirect_high := a * (b1 + b3*(moderator_mean + sqrt(moderator_var))) direct_low := c1 + c3*(moderator_mean - sqrt(moderator_var)) direct_moderate := c1 + c3*moderator_mean direct_high := c1 + c3*(moderator_mean + sqrt(moderator_var)) total_low := indirect_low + direct_low total_moderate := indirect_moderate + direct_moderate total_high := indirect_high + direct_high &#39; mymodel=sem(mystructure,data=mydata, se = &quot;bootstrap&quot;, iseed=1234, bootstrap = 5) # I take 5 bootstrap simulation but consider 500+ when doing it for real summary(mymodel,standardized=TRUE) #&gt; lavaan 0.6.16 ended normally after 13 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 12 #&gt; #&gt; Number of observations 250 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 1502.466 #&gt; Degrees of freedom 6 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Bootstrap #&gt; Number of requested bootstrap draws 5 #&gt; Number of successful bootstrap draws 5 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; outcome ~ #&gt; predictor (c1) 1.114 0.547 2.039 0.041 #&gt; moderator (c2) -0.994 0.753 -1.321 0.186 #&gt; intrctn__ (c3) -0.044 0.034 -1.288 0.198 #&gt; mediator ~ #&gt; predictor (a) 0.644 0.033 19.350 0.000 #&gt; outcome ~ #&gt; mediator (b1) -0.360 0.766 -0.470 0.639 #&gt; intrctn__ (b3) 0.058 0.042 1.379 0.168 #&gt; Std.lv Std.all #&gt; #&gt; 1.114 0.551 #&gt; -0.994 -0.398 #&gt; -0.044 -0.470 #&gt; #&gt; 0.644 0.713 #&gt; #&gt; -0.360 -0.161 #&gt; 0.058 0.783 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; modertr (mdr_) 16.663 0.211 79.066 0.000 #&gt; .outcome 43.243 14.415 3.000 0.003 #&gt; .mediatr 15.996 0.847 18.877 0.000 #&gt; Std.lv Std.all #&gt; 16.663 4.196 #&gt; 43.243 4.354 #&gt; 15.996 3.603 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; modertr (mdr_) 15.772 2.083 7.570 0.000 #&gt; .outcome 49.139 2.438 20.154 0.000 #&gt; .mediatr 9.689 0.684 14.163 0.000 #&gt; Std.lv Std.all #&gt; 15.772 1.000 #&gt; 49.139 0.498 #&gt; 9.689 0.491 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; indirect_low 0.238 0.205 1.163 0.245 #&gt; indirect_modrt 0.386 0.098 3.916 0.000 #&gt; indirect_high 0.533 0.133 4.011 0.000 #&gt; direct_low 0.560 0.167 3.349 0.001 #&gt; direct_moderat 0.387 0.087 4.469 0.000 #&gt; direct_high 0.214 0.202 1.059 0.289 #&gt; total_low 0.799 0.080 10.037 0.000 #&gt; total_moderate 0.772 0.032 23.942 0.000 #&gt; total_high 0.746 0.095 7.827 0.000 #&gt; Std.lv Std.all #&gt; 0.238 1.669 #&gt; 0.386 2.228 #&gt; 0.533 2.786 #&gt; 0.560 -0.952 #&gt; 0.387 -1.422 #&gt; 0.214 -1.893 #&gt; 0.799 0.717 #&gt; 0.772 0.805 #&gt; 0.746 0.893 "],["missing-data-and-multiple-imputation.html", "Chapter 9 Missing data and multiple imputation 9.1 Missing data 9.2 Multiple imputation", " Chapter 9 Missing data and multiple imputation Most of our datasets so far were a blessing to have. Sure, we sometimes had to clean, transform, and process our data but we were relatively spared of one common inconvenience, missing data. While you could simply ignore that some of your values are missing, this ignorance could potentially introduce some bias in your estimates and lead to reduced statistical power. In this part. I briefly discuss what we could consider when confronted with missing data. I will shortly show how to inspect missing values. Specifically, how much data is missing, what variables may relate to missing values in another variable (plotting these patterns using the VIM package), and the patterns of missingness: Missing Completely at Random, Missing at Random, and Missing Not at Random. Finally, I introduce the multiple imputation procedure and use and demonstrate the mice package, step-by-step. 9.1 Missing data 9.1.1 Listwise deletion and single (mean) imputation Let it be known, the best way to handle missing data is to put efforts into avoiding it in the first place. That being said, perhaps the easiest option to deal with missing values, is to exclude incomplete cases (listwise deletion). For example. mydata = data.frame(x = c(1,2,NA,3,4,5), y = c(NA,2,3,NA,4,5) ) mydata_no_NA = mydata[complete.cases(mydata), ] # To apply listwise deletion head(mydata_no_NA) #&gt; x y #&gt; 2 2 2 #&gt; 5 4 4 #&gt; 6 5 5 However, depending on the number of excluded cases, the statistical power may drop notably. Moreover, it could create additional inconveniences such as unbalanced designs (e.g., notably more participants in one group compared to others) and bias in your estimated outcomes including inflation of standard errors. In short, listwise deletion could be ill-advised sometimes. Alternatively we can also start thinking about filling in the empty values but here we are confronted with two questions. First, if we start filling in empty values, what would be the amount of missing data where, if we would consider to do so, becomes unacceptable? This is a question of ongoing debate, hence my suggestion would be to look up simulation studies or related work. Second, if we impute our data we need to decide with what we fill in our missing values. One intuitive candidate would be the variable’s mean. However, if we simply use the mean, we fill in the gaps with something uninformative. In addition, remember that to compute the variance, we divide differences between values and the mean by the amount of complete cases. Therefore, we end up underestimating to a certain extent our standard errors. To deal with this issue, we can fill in the empty values with multiple plausible values instead of one and the same. This is where multiple imputation comes into view. 9.2 Multiple imputation In Multiple imputation, missing values will be imputed (i.e., filled in) across multiple simulated and complete datasets. From there we can pool the results and inspect the extent of overlap in each simulated dataset. This in itself sounds straightforward but various aspects should be considered. I will guide you through step-by-step. Let’s assume we have the following data with missing values in two predictors and an outcome. Off note, missing values in predictor variables will be imputed and these imputed values will be used in later analyses (see later on) by the mice() function. Missing values in the outcome will also be imputed by the mice() function but these will not be used in later analyses. Note that I also created a variable named auxiliary, its relevance will become clear later on. library(dplyr) set.seed(902) # Create the dataset n = 500 predictor1 = rnorm(n) predictor2 = rnorm(n) auxiliary = rnorm(n) outcome = 0.5 * predictor1 + 0.3 * predictor2 + rnorm(n) mydata = data.frame(outcome, predictor1, predictor2, auxiliary) # Now, introduce missing values based on the variable &quot;auxiliary&quot; (so that the absence of values correlates to this &quot;auxiliary&quot; variable, as explained later on) # Below I applied my own rule, predictors and outcomes are set NA based on the value of &quot;auxiliary&quot; (based on the quantile of its values) mydata = mydata %&gt;% mutate( across(c(&quot;predictor1&quot;,&quot;predictor2&quot;), ~ ifelse(auxiliary &lt; quantile(auxiliary, 0.25), NA, .) ), outcome = ifelse(auxiliary &lt; quantile(auxiliary, 0.10), NA, outcome) ) head(mydata) #&gt; outcome predictor1 predictor2 auxiliary #&gt; 1 -0.4294810 0.305674 -0.2005052 0.68156926 #&gt; 2 0.5090547 NA NA -0.91705860 #&gt; 3 -0.3250196 NA NA -1.04904178 #&gt; 4 -1.1898698 -1.394627 0.1331301 0.05551047 #&gt; 5 0.8797796 NA NA -0.90291118 #&gt; 6 2.7468572 1.362628 0.7923936 0.22223424 9.2.1 before mice: Check missing values Alright, before we begin with the mice function(), it is always a good idea to check how much missing values we have. Here we can ask questions like how many missing values we have per variable of interest and whether missing values in one variable are related to other variables. For my missing value checking needs, I like to use the aggr() function from the VIM package. library(VIM) aggr(mydata, col = c(&#39;red&#39;, &#39;green&#39;), # Colored this way so that red will represent the missing values and green the present ones in the right side of the plot numbers = TRUE, sortVars = TRUE, labels = names(mydata), cex.axis = 0.7, gap = 3, ylab = c(&quot;Proportion of missing values&quot;, &quot;Missingness Pattern&quot;) ) #&gt; #&gt; Variables sorted by number of missings: #&gt; Variable Count #&gt; predictor1 0.25 #&gt; predictor2 0.25 #&gt; outcome 0.10 #&gt; auxiliary 0.00 On the left side of the plot you see the proportion of missing data but I imagine the right side requires more explanation. The vertical axis shows the frequency of each missing pattern. So we can distinguish three patterns in our case. The most common one (75%) is when all values are present (they are all colored green). The next pattern (15%) is that there is missing values in both predictor1 and predictor2 (they are colored red) but not in the outcome and auxiliary variable. Finally, in 10% of the cases, there are missing values in all but the auxiliary variable. Altogether, the patterns suggests that the occurrence of missing values clusters around the auxiliary variable (it is always present). If missing values are related to measured (“observed”) variables, just like in our case, we can take it as an indication of Missing at Random (MAR). 9.2.1.1 before mice: Missing (Not) (Completely) at Random The MAR pattern we encounter in our plot is one of the three forms of missingness and multiple imputations assumes that the data are missing either completely at random (MCAR) or at least at random (MAR). However, if the missing pattern would be Missing Not at Random, we may run into trouble. We describe the pattern of missingness as MNAR if missing values are notably related to a variable that we did not measure (unobserved, outside our dataset). If we run mice with MNAR, we may end up with biased outcomes. So you may be wondering, how do we know what type of missingness we have? Let’s start with MCAR, one of the most common ways to test for this type of missingness is to use Little’s MCAR test. This can be done using the mcar_test() function from the naniar package. If the test is not significant based on the provided p value, it would suggest that the data could be MCAR. library(naniar) mcar_test(mydata) #&gt; # A tibble: 1 × 4 #&gt; statistic df p.value missing.patterns #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 294. 3 0 3 Since the test is significant, the test suggests either MAR or MNAR. However, regardless of the outcome, I would advise some caution when interpreting this test. MCAR is unlikely in most cases because missingness is typically related to variables. Also, like many other tests, Little’s MCAR test can be sensitive to sample size as in larger samples, p values are more likely to drop below 0.05. Therefore, in most cases, it comes down to either MNAR and MAR. To decide whether the pattern of missingess is MAR or MNAR, we can visualize and inspect patterns in missing values like what we before using the VIM package. In addition, You could consider to fit logistic regression models in which missing values in a given variable are predicted by another variable. For example, say we want to test whether the occurrence of missing values (yes or no) in predictor1 is predicted by the predictor2. # For demonstration purpose I temporarily make a binomial variable that indicates whether the value of predictor1 is missing or not temp_mydata = mydata %&gt;% mutate(missing_predictor1 = ifelse(is.na(predictor1),1,0)) # Fit the logistic regression options(scipen=999) summary( glm(missing_predictor1 ~ predictor2, family = &quot;binomial&quot;, data=temp_mydata) ) #&gt; Warning: glm.fit: algorithm did not converge #&gt; #&gt; Call: #&gt; glm(formula = missing_predictor1 ~ predictor2, family = &quot;binomial&quot;, #&gt; data = temp_mydata) #&gt; #&gt; Coefficients: #&gt; Estimate #&gt; (Intercept) -26.566068523538149293 #&gt; predictor2 -0.000000000000004675 #&gt; Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 18390.786054817810509121 -0.001 0.999 #&gt; predictor2 17583.793427717431768542 0.000 1.000 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 0.0000000000000 on 374 degrees of freedom #&gt; Residual deviance: 0.0000000021756 on 373 degrees of freedom #&gt; (125 observations deleted due to missingness) #&gt; AIC: 4 #&gt; #&gt; Number of Fisher Scoring iterations: 25 However, note that you test for a linear corelation, so you might miss out on other forms of association. 9.2.2 mice decissions: joint modeling versus fully conditional specification We determined that the pattern of missingness is (likely) MAR and that multiple imputation would be deemed an adequate procedure given the number of missing values. A next decision is to decide which imputation method to take. In multiple imputation, we have joint modeling which assumes a multivariate distribution of all variables to be used to sample the missing values. In practice, missing values are most commonly sampled from a multivariate normal distribution. However, since categorical variable do not follow a multivariate normal distribution, you might consider not to go with joint modeling when your data is a mix of categorical and continuous variables. Instead, if you do have a mix, the second option of fully conditional specification (FCS) might be preferred. In short, FCS specifies a regression model in which missing values are predicted by taking the other variables from your dataset as predictors. This missing value procedure is repeated until convergence. The mice package uses FCS and you have to resort to other packages such as Amelia (which assumes multivariate normality) if you want to perform joint modelling. The focus of this part is on the mice package given its common use and flexibility, and hence I will go with FCS. 9.2.3 mice decisions: number of imputations A final decision before I put everything into practice: the number of imputed datasets. To my knowledge, there are no strict rules regarding this number. What sometimes is considered to take the percentage of missingness in your dataset as the number of imputation. For example, if you would have across all your variables of interest a total of 35% missing data, you could consider to use 35 imputations. However, this does not guarantee that this number will lead to stable results. My advice is that you can consider this rule to decide the number. After you ran the mice, you could then repeat and check whether the obtained results are similar if you would increase the number of imputations. I will consider this in the practical example. 9.2.4 Practical example The mice() function will ask for a couple of ingredients. First we will need to define a predictor matrix. In the predictor matrix you want to specify the variables you want to include as predictors for the missing values. With our dataset provided at the start of section, we could use every variable. To set a predictor matrix, we can use the conveniently named make.predictorMatrix() function from the mice package library(mice) predictor_matrix = make.predictorMatrix(mydata) Of note, if you would have a variable that is not at all useful as a predictor of missing values, we could have done something like: predictor_matrix[, “the variable you do not want to use as a predictor”] = 0 Next, mice will ask what imputation method to use per variable in your dataset. This will follow the same order as the variables in the dataset so in our case: outcome, predictor1, predictor2, and auxiliary. As imputation method we can choose between various options including “pmm” (predictive mean matching for numerical variables), “logreg” (logistic regression; for binary variables), “polreg” (polytomous regression for unordered factor variables having three levels or more), “polyr” (“proportional odds models for ordered factors having three levels or more). In addition, you can use ““ for variables that have no missing values. A full overview is provided here (click here). All variables are numerical in our example, and therefore I could use the “pmm” method for each variable imputation_method = c(&quot;pmm&quot;, #outcome &quot;pmm&quot;, # predictor1 &quot;pmm&quot;, # predictor2 &quot;&quot; # auxiliary which does not have missing values ) Next, we have to decide on the number of imputed datasets. As I said before there are no strict rules. We could start with 25 imputation as we have 25% of missing values in total (25% in both predictor1 and predictor2, 10% in the outcome, and 0% in the auxiliary variable). Later on, I will inspect whether results change notably when setting a higher number of imputations. n_imputations = 25 Alright, let’s run the mice() function using the above ingredients. Note that you could also set a seed. mice_data = mice( data = mydata, m = n_imputations, seed = 97531, predictorMatrix = predictor_matrix ) Our original as well as 25 imputed complete datasets, are now stored in a special type of container so to speaks, a mids object. Before we conduct further analysis with our imputed datasets, we should check the convergence of the imputation algorithm. Convergence can be visualized in the following way: for each iteration (i.e., times the imputation algorithm created an imputed dataset), you note down the mean and the standard deviation of the imputed variables. For this purpose, we could use something simple like the plot() function plot(mice_data) To conclude whether or not the imputation algorithm converged, we can focus on the line overlap and whether they show a similar extent of variability. Additionally, I will also look whether the range of the mean and standard deviation of the imputed variables resembles that of the mean/standard deviation of the observed variables. Let’s quickly check the observed means and standard deviations. library(psych) describe(mydata[,c(&quot;outcome&quot;, &quot;predictor1&quot;, &quot;predictor2&quot;)])[c(&quot;mean&quot;,&quot;sd&quot;)] #&gt; mean sd #&gt; outcome -0.08 1.12 #&gt; predictor1 -0.01 1.06 #&gt; predictor2 0.01 1.05 Ok, let us also inspect our plot. the lines show some overlap, the variability (the range in mean and standard deviation) seems not to be “large”. The lines “zigzag” as expected and do not appear to follow a pattern (e.g., a strong decrease or increase with increasing number of iteration), which is good. Comparing the observed means/standard deviations with that of imputed values, all look ok except for the mean of the outcome which is -0.08 while the range in the mean of the imputed values are -0.5 to 0.4. This is not necessarily a “bad” thing as it could be that the imputations reduced some bias that we would otherwise have in the observed mean (since listwise deletion was applied to get its average). For now I don’t see notable indications for non-convergence. Later on I will repeat the mice procedure with different seeds to see if results are similar. Next to the mean and standard deviations, we should also zoom in on the distribution of the imputed values compared to the observed ones. Here we could the densityplot() function and. Since I want to three distributions (outcome, predictor1, predictor2), I will use the plot_grid() function from the cowplot package to combine the density plots into one. library(cowplot) cowplot::plot_grid( densityplot(mice_data, ~outcome), densityplot(mice_data, ~predictor1), densityplot(mice_data, ~predictor2), ncol=2, nrow=2) Overall, I will deem it “acceptable”. The outcome variable shows the least overlap between the observed and imputed distribution, as expected based on mean, but it does not seem problematic based on visual inspection. Again, there can be some differences between the imputed and observed variables without providing any problem. 9.2.4.1 Pooling the results We are finally ready to run our analysis model. At least, in our case as we intend to run a general linear regression model. However, if you would use other types of regression models such as logistic regression, you would need to remove any NA in the outcome as the model cannot handle NA’s directly. If you would intend to run a model that cannot handle missing values in the outcome, you need to “open” your mice mids object to reveal the dataset containing the original and imputed data. Then you would need to remove instances were the outcome is missing. Finally you would need to transform the dataset back to a mids object. For the purpose of “opening” the original mids object, you can use the complete() function from the mice package. For illustration purposes it would look something like this. Note that I will not run the code below as I intend to use general linear regression. library(mice) # Open the mids object mice_data_long = complete(mice_data, action = &quot;long&quot;, include = TRUE) # Remove NA in the OUTCOME variable (here &quot;outcome&quot;) library(dplyr) mice_data_long = mice_data_long %&gt;% filter(!is.na(outcome)) # Transform back to a mids object mice_data = as.mids(mice_data_long) Back to our case where we do not need to remove the missing values in the outcomes. We now have to specify our imputation model and this should be as similar to the analysis model that you had in mind. If you want to add quadratic terms in your analysis model then you also have to include them in your imputation model. Here, I want to fit the following model: lm(outcome ~ predictor1 * predictor2, data = mydata) Therefore,our imputation should have at least the main effects of predictor1 and predictor2, and their interaction effect, on the same outcome. Important to note, the imputation model is allowed to have extra variables and this is the moment where I finally spoil the purpose of my auxiliary variable. Auxiliary variables are usually not of interest for your analysis per se but these variables may relate to missingness and missing variables. Therefore they may aid to approximate more the assumption of MAR. You could especially consider to add auxiliary variables when your pattern of missingness resembles more the missing not at random state. Of course, later on, you could always run mice with and without auxiliary variables to check whether results remain similar. In our example I will add the auxiliary variable to my imputation model. my_mice_model = with(mice_data, lm( outcome ~ predictor1 * predictor2 + auxiliary )) # we have to use &quot;with&quot; since mice_data is a mids object summary(my_mice_model) #&gt; # A tibble: 125 × 6 #&gt; term estimate std.error statistic p.value nobs #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 (Intercept) -0.110 0.0434 -2.53 1.17e- 2 500 #&gt; 2 predictor1 0.404 0.0412 9.82 6.54e-21 500 #&gt; 3 predictor2 0.296 0.0416 7.12 3.89e-12 500 #&gt; 4 auxiliary 0.0397 0.0420 0.945 3.45e- 1 500 #&gt; 5 predictor1:p… 0.0194 0.0422 0.460 6.46e- 1 500 #&gt; 6 (Intercept) -0.0353 0.0428 -0.824 4.10e- 1 500 #&gt; 7 predictor1 0.446 0.0395 11.3 1.86e-26 500 #&gt; 8 predictor2 0.295 0.0398 7.41 5.69e-13 500 #&gt; 9 auxiliary -0.0905 0.0413 -2.19 2.90e- 2 500 #&gt; 10 predictor1:p… -0.00667 0.0363 -0.184 8.54e- 1 500 #&gt; # ℹ 115 more rows Now we can pool together the results pooled_estimates = pool(my_mice_model) To deliver the finishing touches, I will put the above results in a separate dataset and compute the lower and upper bounds of the 95% confidence intervals of the estimated pooled coefficients. pooled_results = data.frame( summary(pooled_estimates) ) %&gt;% mutate(CI_lower = estimate - 1.96*(sqrt(pooled_estimates$pooled$ubar)), CI_upper = estimate + 1.96*(sqrt(pooled_estimates$pooled$ubar))) Now we can check whether the above pooled results resemble those we would obtain without multiple imputation. mymodel_no_mice = lm( outcome ~ predictor1 * predictor2 + auxiliary ) # The model with automatically drop missing values so we do not need to do it ourselves data.frame( estimate = mymodel_no_mice$coefficients, std.error = summary(mymodel_no_mice)$coefficients[, &quot;Std. Error&quot;], statistic = summary(mymodel_no_mice)$coefficients[, &quot;t value&quot;], p.value = summary(mymodel_no_mice)$coefficients[, &quot;Pr(&gt;|t|)&quot;], CI_lower = confint(mymodel_no_mice)[,1], CI_upper = confint(mymodel_no_mice)[,2] ) #&gt; estimate std.error statistic #&gt; (Intercept) -0.06612735 0.04308921 -1.5346612 #&gt; predictor1 0.46189195 0.04055634 11.3888965 #&gt; predictor2 0.29761672 0.04110791 7.2398900 #&gt; auxiliary -0.02627134 0.04159248 -0.6316367 #&gt; predictor1:predictor2 -0.01285578 0.04171811 -0.3081581 #&gt; p.value #&gt; (Intercept) 0.12550594727660110971889650954836 #&gt; predictor1 0.00000000000000000000000000754036 #&gt; predictor2 0.00000000000172646655648272597731 #&gt; auxiliary 0.52791587685890761783014113461832 #&gt; predictor1:predictor2 0.75809165072632311854761155700544 #&gt; CI_lower CI_upper #&gt; (Intercept) -0.15078765 0.01853296 #&gt; predictor1 0.38220815 0.54157575 #&gt; predictor2 0.21684922 0.37838422 #&gt; auxiliary -0.10799091 0.05544823 #&gt; predictor1:predictor2 -0.09482219 0.06911064 Overall, they look notably similar. If this was not the case, we could have to rethink every decision made up till this point. And there you have it, our outcome, obtained with our specific seed of 97531. Now it is best to check whether our outcomes remain relatively robust across different runs of mice 9.2.4.2 Checking the robustness of the results Like me, you may start wondering about at least two questions. What is the extend of similarity in our results if we used different seeds? Also, would we get different results if we would increase the number of iterations (here above 25)? Essentially, it falls down to rerunning our mice procedure a given number of times. Concerning the extent of similarity in our results, we could use a set of different seeds or we could remove the seeds altogether. In the example below, I will remove the seed and (for) loop through 100 runs of the mice procedure (in practice, try a higher value such as 500 or more), and store the results (as a data frame object) in a so called list variable. # To store the results of the imputed datasets and to determine the number of runs container_results=list() n_runs = 100 # In practice, try larger values (e.g., above 500) for(i in 1:n_runs){ # Run the mice but without a seed mice_data = mice( data = mydata, m = n_imputations, predictorMatrix = predictor_matrix, ) # Fit the imputation models (Again, depending on your model, remove NA in the outcome first!) my_mice_model = with(mice_data, lm( outcome ~ predictor1 * predictor2 + auxiliary ) ) pooled_estimates = pool(my_mice_model) # Here instead in &quot;pooled_results&quot; like I did before, I will store the each dataset object to my outcome_container container_results[[i]] = data.frame( summary(pooled_estimates) ) %&gt;% mutate(CI_lower = estimate - 1.96*(sqrt(pooled_estimates$pooled$ubar)), CI_upper = estimate + 1.96*(sqrt(pooled_estimates$pooled$ubar))) } Good, now I can retrieve each estimate per dataset, per run, that is stored in my list, and simply plot them. # Combine all data frames in the list into one data frame pooled_estimates_across_runs = bind_rows(container_results, .id = &quot;run&quot;) # Plot the estimates across datasets library(ggplot2) library(plotly) ggplotly( pooled_estimates_across_runs %&gt;% ggplot(aes(y=estimate, x = term, color=term)) + geom_point() + xlab(&quot;&quot;) + theme(axis.text.x = element_blank() ) ) There is a bit of variation in the estimations but all could be deemed robust. Of course, feel free to check other aspects such as the similarity in confidence intervals, and so on. Regarding the second question, about the number of iterations, we started with 25 iterations as as there was on average 25% of missing values in our dataset (a common but not very strict rule). We could increase the amount of iterations. In the example below, I set the seed back to the original value of 97531 and I will loop through six different numbers of iterations (i.e., the original 25, 30, 35, 40, 45, and 50). As before I will save the results to a list object and plot the estimates across datasets. The code is mostly similar to the one above but note that I now loop across the the numbers of iterations. Additionally, I made a variable mycount which will be used within the (for) loop to store each imputed “outcome dataset”. # To store the results of the imputed datasets and to determine the number of runs container_results=list() n_imputations = c(25, 30, 35, 40, 45, 50) mycount = 1 # since the for loop does not loop anymore through the values 1,2,3,... this will be used to store the &quot;outcome datasets&quot; at the end of the loop for(i in seq_along(n_imputations)){ # Put the seed back to the original (if you want to report results based on this specific seed) mice_data = mice( data = mydata, m = n_imputations[i], # On the first run this will be 25, on the second, 30, and so on. seed = 97531, predictorMatrix = predictor_matrix, ) # Fit the imputation models (Again, depending on your model, remove NA in the outcome first!) my_mice_model = with(mice_data, lm( outcome ~ predictor1 * predictor2 + auxiliary ) ) pooled_estimates = pool(my_mice_model) container_results[[mycount]] = data.frame( summary(pooled_estimates) ) %&gt;% mutate(CI_lower = estimate - 1.96*(sqrt(pooled_estimates$pooled$ubar)), CI_upper = estimate + 1.96*(sqrt(pooled_estimates$pooled$ubar))) mycount = mycount+1 # To update it so that in the next run, a new outcome dataset is stored to the list } Plot it like before. # Plotting the estimates library(ggplot2) library(plotly) ggplotly( pooled_estimates_across_runs %&gt;% ggplot(aes(y=estimate, x = term, color=term)) + geom_point() + xlab(&quot;&quot;) + theme(axis.text.x = element_blank() ) ) And there you have it. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
